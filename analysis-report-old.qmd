# Inferential analyses

## Generalized Linear Mixed Models

### Rationale

We are going to fit Generalized Linear Mixed Models (GLMM) on the RT data. As opposed to Linear Mixed Models (LMM), GLMMs are able to accommodate the shape of the skewed RT distribution, thus removing the need to transform the dependent variable. This will allow us to work with raw RT data: GLMMs allow assumptions regarding the relationship between the predictors and the dependent variable (i.e. RT here) to be tested independently of assumptions regarding the distribution of dependent variable (i.e. skewed in our case). In LMMs, the two are confounded because the relationship between the predictors and the dependent variable is dictated by the transformation selected to normalize the distribution of the dependent variable (e.g. log transformation, Box-Cox transformation, etc.). By contrast, GLMMs allows the form of the link function to be determined by the theoretical issues under consideration (Lo & Andrews, 2015).

### Model fitting

The predictors of the RT outcomes in the models are the **Group**, the **Congruence** condition and the **Color** condition. The random factors here are participants. The models could account for the varying RT means of each participant with a *random intercept per participant*, the varying effect of **Congruence** for each participant with a *slope on Congruence per participant*, and the varying effect of **Color** for each participant with a *slope on Color per participant*. These random effects will be added if they contribute significantly to the quality of the models. To accomodate the RTs, several models and distributions will be tested and compared using model quality indices: Gamma distribution with an identity link function.

Alternatively, we could also transform our data using a Box-Cox transformation to bring the RTs closer to normality and fit LMMs. Using the `tidymodels` workflows, we can easily fit all of these models and compare their quality.

Only two-way interactions between the three factors have been kept, as three-way interactions were not improving the quality of the models, and were sometimes even preventing the models from converging at all.

```{r fitting_glmms}
#| output: false
 
# ═══ Fitting Generalized Linear Mixed Models in parallel ══════════════════════

# ─── Preparing variable roles ─────────────────────────────────────────────────
model_recipe_glmm_ex <- 
  df_explicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

model_recipe_glmm_im <- 
  df_implicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

# ─── Specifying the distributions for the models ──────────────────────────────

# GLMM, Gamma distribution, identity link
glmm_gamma_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = Gamma(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, identity link
glmm_inverse_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, log link
glmm_inverse_log <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "log")
  )

# LMM = GLMM with a Gaussian distribution and identity link
glmm_gaussian <-
  linear_reg() |> 
  set_engine("lmer")

# Listing these models
# GLMMs
model_specs_glmm <- list(
  glmm_gamma_id    = glmm_gamma_id,
  glmm_inverse_id  = glmm_inverse_id,
  glmm_inverse_log = glmm_inverse_log,
  glmm_gaussian    = glmm_gaussian
)

# ─── Writing down the formulas of our models ──────────────────────────────────
# null model
formula_0 <- rt ~ (1|subjectid)
# intercept by-participant only
formula_1 <- rt ~ (aphantasia + congruence + color)^2 + (1|subjectid)
# intercept and slope on congruence by-participant
formula_2 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid)
# intercept and slope on color by-participant
formula_3 <- rt ~ (aphantasia + congruence + color)^2 + (color|subjectid)
# intercept and slope on congruence and color by-participant
formula_4 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid) + (color|subjectid)

# Listing these formulas
model_formulas <- list(
  formula_0 = formula_0,
  formula_1 = formula_1,
  formula_2 = formula_2,
  formula_3 = formula_3,
  formula_4 = formula_4
  )

# ─── Table to combine everything in workflows and fit the models ──────────────
# (
model_all_workflows_fitted <- 
  tribble(       ~recipe,     ~ task,           ~model,       ~formula,
    model_recipe_glmm_ex, "explicit", model_specs_glmm, model_formulas,
    model_recipe_glmm_im, "implicit", model_specs_glmm, model_formulas
  ) |> 
  # combining recipes with models
  unnest_longer(model) |> 
  # combining them with formulas
  unnest_longer(formula) |>
  rowwise() |>
  mutate(
    # creating workflows
    workflow = list(
      workflow() |>
      add_recipe(recipe) |> 
      add_model(model, formula = formula)  
      )
  )

# ─── Fitting the models with parallel processing ──────────────────────────────

# finding the available cores for parallel processing
n_cores <- parallel::detectCores() - 1

# creating the cluster of cores
parallel_cluster <- 
  parallel::makeCluster(
    n_cores,
    type = "PSOCK"
  )

# registering the cluster for `foreach`
doParallel::registerDoParallel(cl = parallel_cluster)
# checking
# foreach::getDoParRegistered()
# foreach::getDoParWorkers()

# creating a new list-column with all the models fitted in parallel
(model_all_workflows_fitted$fitted_model <-
  foreach(
    workflow = model_all_workflows_fitted$workflow,
    task     = model_all_workflows_fitted$task,
    .combine  = "c",
    .packages = c("tidymodels", "multilevelmod")
) %dopar% {
  if(task == "explicit"){
    model_fit <-
      list(
        workflow |> 
        fit(data = df_explicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  else{
    model_fit <-
      list(
        workflow |> 
        fit(data = df_implicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  return(model_fit)
# measuring runtime for benchmarking
}) |> system.time() -> time_parallel

model_all_workflows_fitted <-  
  model_all_workflows_fitted |> 
  select(-c(recipe, model, formula, workflow))

# ─── Examining the quality of the models, estimating the parameters ───────────
model_all_workflows_fitted$parameters <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("parameters")
) %dopar% {
  parameters <- list(model_parameters(fitted_model))
  return(parameters)
}

model_all_workflows_fitted$convergence <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  convergence <- check_convergence(fitted_model)
  return(convergence)
}

model_all_workflows_fitted$model_perf <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  model_perf <- list(model_performance(fitted_model))
  return(model_perf)
}

model_all_workflows_fitted <-
  model_all_workflows_fitted |>
  mutate(
    # extracting model quality indices
    AIC  = model_perf[[1]],
    AICc = model_perf[[2]],
    BIC  = model_perf[[3]]
  ) |>
  ungroup() |> 
  select(-model_perf)

# stopping the cluster when we're done
parallel::stopCluster(cl = parallel_cluster)

# 214 seconds (3min30s)

# ─── Comparing the models' quality among those that converged ─────────────────
model_selection <-
  model_all_workflows_fitted |> 
  filter(
    formula_id != "formula_0" & 
    convergence == TRUE
  ) |> 
  group_by(task, model_id) |> 
  mutate(best_model = ifelse(AICc == min(AICc), TRUE, FALSE)) |> 
  filter(best_model == TRUE)
```

```{r check_the_best_models}
#| fig-label: fig-model_checks
#| fig-cap: "Checking the assumptions of all the models for both tasks."
#| fig-subcap: true
#| layout-ncol: 1

best_glmm_explicit <- model_selection$fitted_model[[1]]
best_glmm_implicit <- model_selection$fitted_model[[5]]

# characteristics to check
model_checks = c("homogeneity", "vif", "outliers", "qq", "reqq")

# ─── Model checks ───────────────────────────
p_glmm_checks_explicit <-
  best_glmm_explicit |> check_model(check = model_checks, detrend = FALSE)
p_glmm_checks_implicit <- 
  best_glmm_implicit |> check_model(check = model_checks, detrend = FALSE)

ggexport(
  p_glmm_checks_explicit,
  filename = "plots/glmm_checks_explicit.png",
  width = 1000,
  height = 800,
  dpi = 600
  )

ggexport(
  p_glmm_checks_implicit,
  filename = "plots/glmm_checks_implicit.png",
  width = 1000,
  height = 800,
  dpi = 600
  )
```

::: {.panel-tabset .column-page-inset-right}
```{r glmm_plots_setup}
# setting dodge width for all the geoms
dw <- .75
# stat text size
st <- 7
# legend and axis text size
txt <- 14

# marginal means predictions
best_glmm_explicit_preds <-
  best_glmm_explicit |>
  estimate_means(at = c("aphantasia", "congruence"))

best_glmm_implicit_preds <-
  best_glmm_implicit |>
  estimate_means(at = c("aphantasia", "congruence"))
  
# overall means
rt_mean_explicit <- df_explicit_rt$rt |> mean()
rt_mean_implicit <- df_implicit_rt$rt |> mean()
```

#### Explicit task

```{r glmm_plots_ex}
#| label: fig-model_plots_ex
#| fig-cap: "Plots for the effects of interest in the explicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the explicit task."
#|   - "Group x Congruence interaction in the LMM for the explicit task."

# ─── Model effects plots ───────────────────────────

# annotations
plot_explicit_annotations <-
  df_explicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(785, 795),
    p.adj.signif = c("ns", "***")
    )

# plot
p_glmm_explicit <-
  df_explicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_explicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_explicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  geom_hline(yintercept = rt_mean_explicit, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    plot_explicit_annotations, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  labs(
    title = "Explicit task",
    x = NULL,
    y = ""
    ) +
  coord_cartesian(
    ylim = c(680, 800)
  ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

#### Implicit task

```{r glmm_plots_im}
#| label: fig-model_plots_im
#| fig-cap: "Plots for the effects of interest in the implicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the implicit task."
#|   - "Group x Congruence interaction in the LMM for the implicit task."

# annotations
plot_implicit_annotations <-
  df_implicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(646, 653),
    p.adj.signif = c("ns", "***")
    )

# ─── Model effects plot ───────────────────────────
p_glmm_implicit <-
  df_implicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_implicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_implicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  geom_hline(yintercept = rt_mean_implicit, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    plot_implicit_annotations, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  labs(
    title = "Implicit task",
    x = NULL,
    y = "Response time (ms)"
    ) +
  coord_cartesian(
    ylim = c(580, 655)
  ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```
:::

```{r glmm_ggarrange}
#| label: fig-glmm_ggarrange

p_glmms <-
  ggarrange(
    p_glmm_implicit,
    p_glmm_explicit,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  )

ggexport(
  p_glmms, 
  filename = "plots/plots_glmm.png", 
  width = 4000,
  height = 1500,
  res = 300
  )
```