---
title: "Sensory priming in aphantasia - Data analysis report"
author: "Maël Delem"
date: 2023-11-03
date-modified: last-modified
date-format: "D/MM/YYYY"
format: html
editor: source
editor_options: 
  
  chunk_output_type: inline
toc: true
toc-depth: 3
toc-expand: 2
number-sections: true
number-depth: 2
---

<!-- CSS options for custom title numbers styling-->

```{=html}
<style>
.header-section-number:after {
  content: ". ";
}
</style>
```
### Preliminary Set-up {.unnumbered}

::: {.callout-note collapse="true" appearance="simple"}
#### Datasets, code availability, software and setup

The code generating all the computations, figures, tables, etc. (only partially exposed here for clarity) can be found in the `analysis_report.qmd` file, for which the best reading software are coding Integrated Development Environments (IDE) like [RStudio](https://posit.co/download/rstudio-desktop/) or [Visual Studio Code](https://code.visualstudio.com/). The file includes detailed commentaries along with the code to ease understanding, accessibility, and potential (welcome) criticism.

This analysis was conducted in R language on [RStudio](https://posit.co/download/rstudio-desktop/). This analysis report was written with [Quarto](https://quarto.org/). Down below is the set-up code, including two essential steps:

1.  Installing the required packages for this data analysis

2.  importing data into the dataframes used throughout.

```{r setup}
#| echo: true
#| output: false
#| code-summary: "Packages"

# ═══ Packages ═════════════════════════════════════════════════════════════════

# The package `librairian` will ease the package management with the "shelf" 
# function, which automatically: 
# 1) checks if a package is installed 
# 2) installs it if need be
# 3) loads the package like the "library()" function would.
if (!require(librarian)) install.packages(librarian)
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── essential package collections ───
  tidyverse,      # modern R ecosystem
  easystats,      # data analysis framework
  tidymodels,     # modelling framework
  doParallel,     # parallel execution
  
  # ─── tidymodels friends ──────────────
  corrr,          # correlational analyses
  tidybayes,      # bayesian inference
  multilevelmod,  # multilevel modelling with lmer and tidymodels
  
  # ─── modelling ───────────────────────
  lme4,           # mixed models
  mclust,         # mixture clustering
  rstanarm,       # bayesian models
  BayesFactor,    # BFs
  emmeans,        # marginal estimates
  statmod,        # easystats dependency
  
  # ─── data management ─────────────────
  readxl,         # importing xlsx
  openxlsx,       # exporting xlsx
  
  #  data visualization ──────────────
  # plot types and geoms
  ricardo-bion/ggradar,  # radar plots
  ggbeeswarm,            # scatter violin plots
  GGally,         # complex plots
  # layout and options
  ggpubr,         # publication plots
  patchwork,      # layout control
  rstatix,        # ggplot stat tools
  # palettes
  ggsci,          # scientific palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly         # interactive plots
)

# ─── Global cosmetic theme ───q
theme_set(theme_bw(base_size = 14))

# ─── Fixing a seed for reproducibility ───
set.seed(14051998)
```

```{r importing_data}
#| echo: true
#| output: false
#| code-summary: "Importing data"

# Implicit task
df_implicit <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_implicit"
    ) |> 
  rename("rt" = rt_implicit)

# Explicit task
df_explicit <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_explicit"
    ) |> 
  rename("rt" = rt_explicit)

# Questionnaires
df_questionnaires <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_questionnaires"
    ) %>% 
  # creating the a/phantasia groups
  mutate(
    aphantasia = ifelse(vviq80 < 32, "yes", "no"),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    )

# updating all the dataframes with the questionnaires data
dfs <- 
  list(
    implicit_task = df_implicit, 
    explicit_task = df_explicit
    ) |> 
  # adding the vviq and group column to every dataframe
  imap(~left_join(
    .x, 
    df_questionnaires |>  
      select(
        # column to match with the correct participants
        subjectid, 
        # new variables to add
        age,
        sexe,
        vviq80,
        aphantasia), by = "subjectid") |>  
      rename("sex" = sexe) |>  
      # reordering the final dfs
      select(
        subjectid, 
        age, sex, aphantasia,
        vviq80,
        everything())
    )

df_implicit <- dfs$implicit_task
df_explicit <- dfs$explicit_task
rm(dfs)
```
:::

::: {.callout-tip collapse="true" appearance="simple"}
#### Interactive figures

Many figures in this report are interactive: hover over the plots to see some of the tools available. You can select a zone to zoom on a plot, hover over bars to see details about data, select only specific groups in the legend, among many other features.
:::

# Exploratory Data Analysis (EDA) {#sec-eda}

## Accuracy

The distribution of the percentages of errors across the two tasks is displayed in @fig-tasks_errors, and [-@fig-tasks_errors2].

We are going to remove missed trials in the analyses of the RTs. Nonetheless, the main noticeable aspect of these plots are the obvious outliers in each of the tasks. This information is crucial as it could reflect a wrong understanding of the task, or random responses: in any case, participants that have high error rates would require a deletion of up to 47% of their trials, which could become a bias in the modelling of the overall data. As there appear to be a cut-off of the main distribution around 16% of errors in all tasks, this value will be chosen as threshold. **For each task separately, the results of participants with an error rate superior to 16% will be removed** *(i.e., five participants in the explicit task and four in the implicit task)***, prior to removing all the missed trials of the remaining participants for modelling RTs.**

```{r finding_and_removing_accuracy_outliers}
#| code-summary: "Removing incorrect trials and task-wise accuracy outliers"
#| echo: true
#| output: false

# ═══ Accuracy outliers analysis ═══════════════════════════════════════════════

# ─── Listing error rates per subject ───
list(df_explicit, df_implicit) |> 
  imap(
    ~.x |> 
      select(subjectid, starts_with("correct"), aphantasia) |> 
      group_by(subjectid) |> 
      count(pick(2)) |> 
      filter(pick(1) == 1) |> 
      ungroup() |> 
      mutate(
        n_tot = max(n),
        prop = (n_tot - n) / n_tot * 100  # analyzing the error rate per subject
      ) %>%
      arrange(desc(prop)) |> 
      select(1, 5)
    )
# 5 accuracy outliers in the explicit task, 4 in the implicit one

# ─── Removing incorrect trials and task-wise accuracy outliers ───
df_explicit_rt <-
  df_explicit |> 
  # filtering out...
  filter(
    # incorrect trials
    correct_explicit == 1 &
    # participants identified with with high error rates
    !(subjectid %in% c( 
      "aknezevic",
      "lbrunie", 
      "agayou", 
      "bluciani",
      "ldossantos"))
    ) |>  
  # removing irrelevant variables
  select(-c(sex, vviq80, orientation, response, correct_explicit))

# percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit)
# 8842 trials left = 8.5% removed

df_implicit_rt <- 
  df_implicit |>  
  filter(
    correct_implicit == 1 &
    !(subjectid %in% c( 
      "bdispaux", 
      "eleveque", 
      "aleclaire", 
      "dchimenton"))
      ) |>  
  select(-c(sex, vviq80, orientation, response, correct_implicit))

# percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit) 
# 8765 trials left = 9.3% removed
```

## Response times

### Extreme RTs

As a preprocessing, response times below 300ms and above 3000ms will be removed as they could represent false alarms.

```{r removing_extreme_rt_outlier_trials}
#| code-summary: "Removing extreme RTs"
#| echo: true
#| output: false

# ═══ First broad RT outlier trials removal ════════════════════════════════════

df_explicit_rt <- 
  df_explicit_rt |>  
  # filtering out extreme RTs
  filter(rt > 300 & rt < 3000)

# total percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit)
# 8753 trials left = 9.4% removed

df_implicit_rt <- 
  df_implicit_rt |>  
  filter(rt > 300 & rt < 3000)

# total percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit)
# 8706 left = 9.9% removed
```

### Aberrant RT means

A first level of the RT outliers analysis is based on the overall mean RT of the participants. Participants with abnormally high RT means could represent various specific situations out of the range of our study: pathological conditions, attention deficits, or even more trivially low attention to the task. Such participants could bias both the overall results *and the analysis of the individual trials*: just like the accuracy outliers, the removal of their slow trials could result in a deletion of a large part of their responses, thus biasing the distributions. Let's visualize the distribution of the RT means in @fig-rt_means1 and [-@fig-rt_means2]

#### RT means table

```{r rt_means_table}
#| label: tbl-rt_means
#| tbl-cap: "Descriptive statistics of the average RTs across the sample in both tasks."

# ═══ RT means table - Both tasks ══════════════════════════════════════════════

df_rt <- 
  list(
  df_explicit_rt |>  rename("Explicit task mean RTs" = rt), 
  df_implicit_rt |>  rename("Implicit task mean RTs" = rt)
  ) |>  
  imap(
    ~select(.x, contains("RT")) |> 
      report() |>  
      as.data.frame() |>  
      select(1:10)
    )

df_rt <- bind_rows(df_rt[[1]], df_rt[[2]])
df_rt |>  display()
```

The median RT mean among participants is `r df_rt$Median[1]` in the explicit task, and `r df_rt$Median[2]` in the implicit one. Using an outlier detection procedure based on MADs, a common scaling factor to determine acceptable values is $\pm 3 \times MAD$: as 300ms is our chosen minimal value, this would result in our case in an acceptable range (in ms) of \[300,`r round(df_rt$Median[1] + 3*df_rt$MAD[1])`\] for the explicit task and \[300,`r round(df_rt$Median[2] + 3*df_rt$MAD[2])`\] for the implicit task.

```{r removing_rt_means_outliers}
#| code-summary: "Removing outliers with aberrant RT means"
#| echo: true
#| output: false

# ═══ Aberrant RT means outliers removal ═══════════════════════════════════════

# ─── Finding outliers ───

df_explicit_rt |> 
  group_by(subjectid) |> 
  summarise(mean_rt_explicit = mean(rt)) |> 
  ungroup() |> 
  filter(mean_rt_explicit > 1384) |> 
  arrange(desc(mean_rt_explicit))
# 7 outliers in the explicit task

df_implicit_rt |> 
  group_by(subjectid) |> 
  summarise(mean_rt_implicit = mean(rt)) |> 
  ungroup() |> 
  filter(mean_rt_implicit > 1063) |> 
  arrange(desc(mean_rt_implicit))
# 4 outliers in the implicit task

# ─── Removing specific outliers ───

df_explicit_rt <- 
  df_explicit_rt |> 
  filter(
    !(subjectid %in% c(
      "dchimenton",
      "fc",
      "mbilodeau",
      "llhermitte",
      "hmassot",
      "mbillon",
      "nnguyenphuoc"
    ))
  )

# 8603 trials left
# (9664-8603)/9664 # 11% out

df_implicit_rt <-
  df_implicit_rt |> 
  filter(
    !(subjectid %in% c(
      "mbilodeau",
      "rcarnez",
      "cbertrand",
      "omeyer"
    ))
  )

# 8777 trials left
# (9664-8777)/9664 # 9% out
```

In the end, 18 participants have been excluded due to high error rates or overall abnormally slow RTs. Let's visualize the RT means distributions again in @fig-rt_means3 and [-@fig-rt_means4].

The distributions are still positively skewed, yet in more acceptable ranges for the phenomena we are investigating.

### Outlier trials per participant

We will conduct a more detailed inspection of the RTs on a by-participant level. The RT distributions per participant in each task are presented in @fig-rt_per_participant_1 and [-@fig-rt_per_participant_2].

As expected, most of the participants have a small amount of very slow outlier trials, even by their own standards of RT. Once again, we are going to detect these outliers using MAD thresholds. The RTs have been group by subsets of conditions, as we know that our experimental conditions could have affected them: thus, using a global median would necessarily flatten within-participant outcomes and have a major impact on the analyses.

```{r removing_rt_outlier_trials_per_participant}
#| code-summary: "Removing outlier trials by-participant"
#| echo: true
#| output: false

# ═══ By-participant MAD outlier detection ═════════════════════════════════════

# ─── Explicit task: ───
# From 8229 observations
# 3 MADs threshold
df_explicit_rt <-
  df_explicit_rt |> 
  group_by(subjectid, congruence, color) |> 
  mutate(
    rt_median = median(rt),
    rt_mad = mad(rt)
    ) |> 
  filter(
    rt < rt_median + 5 * rt_mad &
    rt > rt_median - 2.5 * rt_mad
  ) |>
  ungroup()
# 8366 trials left
# (9664-8366)/9664 # 13.4% trials out total
# (8603-8366)/8603 # 2.7% of remaining trials

# ─── Implicit task: ───
# From 8166 observations
# 3 MADs threshold
df_implicit_rt <-
  df_implicit_rt |> 
  group_by(subjectid, congruence, color) |> 
  mutate(
    rt_median = median(rt),
    rt_mad = mad(rt)
    ) |> 
  filter(
    rt < rt_median + 5 * rt_mad &
    rt > rt_median - 2.5 * rt_mad
  ) |>
  ungroup()
# 8409 trials left
# (9664-8409)/9664 # 13% trials out total
# (8777-8409)/8777 # 4.2% of remaining trials
```

The range defined by $\pm 3 \times MAD$ resulted in a deletion of 6.6% of trials in the explicit task and 8.2% of trials in the implicit task. Let's visualize the final distribution of RTs for each participant in @fig-rt_per_participant_3 and [-@fig-rt_per_participant_4]

We now have distributions in way more sensible ranges, what's more defined by the characteristics of the participants, as opposed to the broad arbitrary RT thresholds we chose at the beginning.

# Inferential analyses

## Generalized Linear Mixed Models

### Rationale

We are going to fit Generalized Linear Mixed Models (GLMM) on the RT data. As opposed to Linear Mixed Models (LMM), GLMMs are able to accommodate the shape of the skewed RT distribution, thus removing the need to transform the dependent variable. This will allow us to work with raw RT data: GLMMs allow assumptions regarding the relationship between the predictors and the dependent variable (i.e. RT here) to be tested independently of assumptions regarding the distribution of dependent variable (i.e. skewed in our case). In LMMs, the two are confounded because the relationship between the predictors and the dependent variable is dictated by the transformation selected to normalize the distribution of the dependent variable (e.g. log transformation, Box-Cox transformation, etc.). By contrast, GLMMs allows the form of the link function to be determined by the theoretical issues under consideration (Lo & Andrews, 2015).

### Model fitting

The predictors of the RT outcomes in the models are the **Group**, the **Congruence** condition and the **Color** condition. The random factors here are participants. The models could account for the varying RT means of each participant with a *random intercept per participant*, the varying effect of **Congruence** for each participant with a *slope on Congruence per participant*, and the varying effect of **Color** for each participant with a *slope on Color per participant*. These random effects will be added if they contribute significantly to the quality of the models. To accomodate the RTs, several models and distributions will be tested and compared using model quality indices: Gamma distribution with an identity link function.

Alternatively, we could also transform our data using a Box-Cox transformation to bring the RTs closer to normality and fit LMMs. Using the `tidymodels` workflows, we can easily fit all of these models and compare their quality.

Only two-way interactions between the three factors have been kept, as three-way interactions were not improving the quality of the models, and were sometimes even preventing the models from converging at all.

```{r fitting_glmms}
#| output: false
 
# ═══ Fitting Generalized Linear Mixed Models in parallel ══════════════════════

# ─── Preparing variable roles ─────────────────────────────────────────────────
model_recipe_glmm_ex <- 
  df_explicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

model_recipe_glmm_im <- 
  df_implicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

# ─── Specifying the distributions for the models ──────────────────────────────

# GLMM, Gamma distribution, identity link
glmm_gamma_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = Gamma(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, identity link
glmm_inverse_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, log link
glmm_inverse_log <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "log")
  )

# LMM = GLMM with a Gaussian distribution and identity link
glmm_gaussian <-
  linear_reg() |> 
  set_engine("lmer")

# Listing these models
# GLMMs
model_specs_glmm <- list(
  glmm_gamma_id    = glmm_gamma_id,
  glmm_inverse_id  = glmm_inverse_id,
  glmm_inverse_log = glmm_inverse_log,
  glmm_gaussian    = glmm_gaussian
)

# ─── Writing down the formulas of our models ──────────────────────────────────
# null model
formula_0 <- rt ~ (1|subjectid)
# intercept by-participant only
formula_1 <- rt ~ (aphantasia + congruence + color)^2 + (1|subjectid)
# intercept and slope on congruence by-participant
formula_2 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid)
# intercept and slope on color by-participant
formula_3 <- rt ~ (aphantasia + congruence + color)^2 + (color|subjectid)
# intercept and slope on congruence and color by-participant
formula_4 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid) + (color|subjectid)

# Listing these formulas
model_formulas <- list(
  formula_0 = formula_0,
  formula_1 = formula_1,
  formula_2 = formula_2,
  formula_3 = formula_3,
  formula_4 = formula_4
  )

# ─── Table to combine everything in workflows and fit the models ──────────────
# (
model_all_workflows_fitted <- 
  tribble(       ~recipe,     ~ task,           ~model,       ~formula,
    model_recipe_glmm_ex, "explicit", model_specs_glmm, model_formulas,
    model_recipe_glmm_im, "implicit", model_specs_glmm, model_formulas
  ) |> 
  # combining recipes with models
  unnest_longer(model) |> 
  # combining them with formulas
  unnest_longer(formula) |>
  rowwise() |>
  mutate(
    # creating workflows
    workflow = list(
      workflow() |>
      add_recipe(recipe) |> 
      add_model(model, formula = formula)  
      )
  )

# ─── Fitting the models with parallel processing ──────────────────────────────

# finding the available cores for parallel processing
n_cores <- parallel::detectCores() - 1

# creating the cluster of cores
parallel_cluster <- 
  parallel::makeCluster(
    n_cores,
    type = "PSOCK"
  )

# registering the cluster for `foreach`
doParallel::registerDoParallel(cl = parallel_cluster)
# checking
# foreach::getDoParRegistered()
# foreach::getDoParWorkers()

# creating a new list-column with all the models fitted in parallel
(model_all_workflows_fitted$fitted_model <-
  foreach(
    workflow = model_all_workflows_fitted$workflow,
    task     = model_all_workflows_fitted$task,
    .combine  = "c",
    .packages = c("tidymodels", "multilevelmod")
) %dopar% {
  if(task == "explicit"){
    model_fit <-
      list(
        workflow |> 
        fit(data = df_explicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  else{
    model_fit <-
      list(
        workflow |> 
        fit(data = df_implicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  return(model_fit)
# measuring runtime for benchmarking
}) |> system.time() -> time_parallel

model_all_workflows_fitted <-  
  model_all_workflows_fitted |> 
  select(-c(recipe, model, formula, workflow))

# ─── Examining the quality of the models, estimating the parameters ───────────
model_all_workflows_fitted$parameters <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("parameters")
) %dopar% {
  parameters <- list(model_parameters(fitted_model))
  return(parameters)
}

model_all_workflows_fitted$convergence <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  convergence <- check_convergence(fitted_model)
  return(convergence)
}

model_all_workflows_fitted$model_perf <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  model_perf <- list(model_performance(fitted_model))
  return(model_perf)
}

model_all_workflows_fitted <-
  model_all_workflows_fitted |>
  mutate(
    # extracting model quality indices
    AIC  = model_perf[[1]],
    AICc = model_perf[[2]],
    BIC  = model_perf[[3]]
  ) |>
  ungroup() |> 
  select(-model_perf)

# stopping the cluster when we're done
parallel::stopCluster(cl = parallel_cluster)

# 214 seconds (3min30s)

# ─── Comparing the models' quality among those that converged ─────────────────
model_selection <-
  model_all_workflows_fitted |> 
  filter(
    formula_id != "formula_0" & 
    convergence == TRUE
  ) |> 
  group_by(task, model_id) |> 
  mutate(best_model = ifelse(AICc == min(AICc), TRUE, FALSE)) |> 
  filter(best_model == TRUE)
```


```{r check_the_best_models}
#| fig-label: fig-model_checks
#| fig-cap: "Checking the assumptions of all the models for both tasks."
#| fig-subcap: true
#| layout-ncol: 1

best_glmm_explicit <- model_selection$fitted_model[[1]]
best_glmm_implicit <- model_selection$fitted_model[[5]]

# characteristics to check
model_checks = c("homogeneity", "vif", "outliers", "qq", "reqq")

# ─── Model checks ───────────────────────────
p_glmm_checks_explicit <-
  best_glmm_explicit |> check_model(check = model_checks, detrend = FALSE)
p_glmm_checks_implicit <- 
  best_glmm_implicit |> check_model(check = model_checks, detrend = FALSE)

ggexport(
  p_glmm_checks_explicit,
  filename = "plots/glmm_checks_explicit.png",
  width = 1000,
  height = 800,
  dpi = 600
  )

ggexport(
  p_glmm_checks_implicit,
  filename = "plots/glmm_checks_implicit.png",
  width = 1000,
  height = 800,
  dpi = 600
  )
```

::: {.panel-tabset .column-page-inset-right}

```{r glmm_plots_setup}
# setting dodge width for all the geoms
dw <- .75
# stat text size
st <- 7
# legend and axis text size
txt <- 14

# marginal means predictions
best_glmm_explicit_preds <-
  best_glmm_explicit |>
  estimate_means(at = c("aphantasia", "congruence"))

best_glmm_implicit_preds <-
  best_glmm_implicit |>
  estimate_means(at = c("aphantasia", "congruence"))
  
# overall means
rt_mean_explicit <- df_explicit_rt$rt |> mean()
rt_mean_implicit <- df_implicit_rt$rt |> mean()
```

#### Explicit task

```{r glmm_plots_ex}
#| label: fig-model_plots_ex
#| fig-cap: "Plots for the effects of interest in the explicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the explicit task."
#|   - "Group x Congruence interaction in the LMM for the explicit task."

# ─── Model effects plots ───────────────────────────

# annotations
plot_explicit_annotations <-
  df_explicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(785, 795),
    p.adj.signif = c("ns", "***")
    )

# plot
p_glmm_explicit <-
  df_explicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_explicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_explicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  geom_hline(yintercept = rt_mean_explicit, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    plot_explicit_annotations, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  labs(
    title = "Explicit task",
    x = NULL,
    y = ""
    ) +
  coord_cartesian(
    ylim = c(680, 800)
  ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

#### Implicit task

```{r glmm_plots_im}
#| label: fig-model_plots_im
#| fig-cap: "Plots for the effects of interest in the implicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the implicit task."
#|   - "Group x Congruence interaction in the LMM for the implicit task."

# annotations
plot_implicit_annotations <-
  df_implicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(646, 653),
    p.adj.signif = c("ns", "***")
    )

# ─── Model effects plot ───────────────────────────
p_glmm_implicit <-
  df_implicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_implicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_implicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  geom_hline(yintercept = rt_mean_implicit, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    plot_implicit_annotations, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  labs(
    title = "Implicit task",
    x = NULL,
    y = "Response time (ms)"
    ) +
  coord_cartesian(
    ylim = c(580, 655)
  ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```
:::

``` {r glmm_ggarrange}
#| label: fig-glmm_ggarrange

p_glmms <-
  ggarrange(
    p_glmm_implicit,
    p_glmm_explicit,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  )

ggexport(
  p_glmms, 
  filename = "plots/plots_glmm.png", 
  width = 4000,
  height = 1500,
  res = 300
  )
```

# Further EDA: continuous analyses

## Continuous predictors: PCA

Theoretically, the four questionnaire variables - VVIQ, OSIQ-Object and Spatial, and SUIS - could be used as predictors (independent variables) for the RT outcomes (dependent variable) of the experiment. Three of these - VVIQ, OSIQ-Object and SUIS - evaluate visual imagery: yet here we only modeled the VVIQ as representative of visual imagery, what's more as a categorical predictor by dividing two groups based on it. In order to explore the potential of all these scales, we are going to run a Principal Component Analysis (PCA) on the four questionnaires' standardized scores to evaluate how many different constructs they represent. The suitability of the data for a PCA was assessed with Bartlett's test of sphericity (Bartlett, 1951), suggesting there was sufficient correlation in the data ($\chi^{2}$(55) = 408.01, p < .001) and the KMO measure of sampling adequacy (KMO = 0.73; see Kaiser, H. F., & Rice, J. (1974)). The Eigenvalues and variance explained by the PCA components computed, along with the loadings of each score on them, are displayed in @tbl-pca_tables.

```{r pca}
#| code-summary: "Computing the PCA"
#| echo: true

# ─── Principal Component Analysis ───
pca <- 
  principal_components(
    df_questionnaires[,4:7],
    n = "max",
    sort = TRUE,
    standardize = TRUE
    )
```

```{r pca_tables}
#| label: tbl-pca_tables
#| tbl-cap: "Results of the Principal Components Analysis."
#| tbl-subcap: 
#|   - "Loadings of each variable on the three components extracted by the PCA."
#|   - "Eigenvalues and variance explained by the three components extracted by the PCA."
#| layout-ncol: 2

# ─── Loadings ───
pca |> 
  as.data.frame() |>  
  mutate(Variable = case_when(
    Variable == "suis60" ~ "SUIS",
    Variable == "vviq80" ~ "VVIQ",
    Variable == "osiq_o75" ~ "OSIQ-O",
    Variable == "osiq_s75" ~ "OSIQ-S",
    TRUE ~ Variable
  )) |> 
  display()

# ─── Eigenvalues and variance ───
pca |> 
  summary() |> 
  as.data.frame() |>
  display()
```

The two first components (PC1 and PC2) explained 95% of the variance of the sample on the four scores (PC1 Eigenvalue = 2.84, 71% of the total variance explained, PC2 = 0.97, 24% of the total variance explained) and are the only meaningful ones in the PCA (PC3 explains 3% of variance). PC1 correlates highly with the VVIQ, OSIQ-Object and SUIS: this component can logically be interpreted as representing visual imagery (VI variable). PC2 also explains a significant part of variance, and correlates highly with the OSIQ-Spatial: we chose to interpret this component as representing spatial imagery (SI variable). These two variables, decorrelated by the PCA, are represented together in @fig-vi_si_corr.

```{r pca_data}
#| code-summary: "Adding the predicted PCA components to the data"
#| echo: true

# ─── Adding components to the data ───
pca_components <- pca |> predict()

df_questionnaires <-
  bind_cols(df_questionnaires[,1:8], pca_components[,1:2]) %>% 
  mutate(PC2 = -PC2) %>% 
  rename(
    "visual_imagery" = PC1,
    "spatial_imagery" = PC2
    )

# updating all the dataframes with the imagery variables
dfs <- 
  list( 
    implicit_task = df_implicit_rt, 
    explicit_task = df_explicit_rt
    ) %>%
  # adding the new variables to each dataframe
  imap(~left_join(
    .x |> select(-c(rt_median, rt_mad)), 
    df_questionnaires %>% 
      select(
        # column to match with the correct participants
        subjectid, 
        # new variables to add
        visual_imagery, 
        spatial_imagery), 
    by = "subjectid") %>% 
      # reordering the final dfs
      select(
        subjectid, 
        age, aphantasia,
        visual_imagery, spatial_imagery,
        everything())
    )

df_implicit_rt <- dfs$implicit_task
df_explicit_rt <- dfs$explicit_task
rm(dfs)
```



```{r fitting_glmms_continuous}
#| output: false
 
# ═══ Fitting Generalized Linear Mixed Models in parallel ══════════════════════

# ─── Preparing variable roles ─────────────────────────────────────────────────
model_recipe_glmm_ex_continuous <- 
  df_explicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, visual_imagery, spatial_imagery, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

model_recipe_glmm_im_continuous <- 
  df_implicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, visual_imagery, spatial_imagery, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

# ─── Writing down the formulas of our models ──────────────────────────────────
# null model
formula_0_continuous <- rt ~ (1|subjectid)
# intercept by-participant only
formula_1_continuous <- rt ~ (visual_imagery + spatial_imagery + congruence + color)^2 + (1|subjectid)
# intercept and slope on congruence by-participant
formula_2_continuous <- rt ~ (visual_imagery + spatial_imagery + congruence + color)^2 + (congruence|subjectid)
# intercept and slope on color by-participant
formula_3_continuous <- rt ~ (visual_imagery + spatial_imagery + congruence + color)^2 + (color|subjectid)
# intercept and slope on congruence and color by-participant
# formula_4_continuous <- rt ~ (visual_imagery + spatial_imagery + congruence + color)^2 + (congruence|subjectid) + (color|subjectid)

# Listing these formulas
model_formulas_continuous <- list(
  formula_0 = formula_0_continuous,
  formula_1 = formula_1_continuous,
  formula_2 = formula_2_continuous,
  formula_3 = formula_3_continuous
  # formula_4 = formula_4_continuous
  )

# ─── Table to combine everything in workflows and fit the models ──────────────
# (
model_all_fitted_continuous <- 
  tribble(       ~recipe,     ~ task,           ~model,       ~formula,
    model_recipe_glmm_ex_continuous, "explicit", model_specs_glmm, model_formulas_continuous,
    model_recipe_glmm_im_continuous, "implicit", model_specs_glmm, model_formulas_continuous
  ) |> 
  # combining recipes with models
  unnest_longer(model) |> 
  # combining them with formulas
  unnest_longer(formula) |>
  rowwise() |>
  mutate(
    # creating workflows
    workflow = list(
      workflow() |>
      add_recipe(recipe) |> 
      add_model(model, formula = formula)  
      )
  )

# ─── Fitting the models with parallel processing ──────────────────────────────

# finding the available cores for parallel processing
n_cores <- parallel::detectCores() - 1

# creating the cluster of cores
parallel_cluster <- 
  parallel::makeCluster(
    n_cores,
    type = "PSOCK"
  )

# registering the cluster for `foreach`
doParallel::registerDoParallel(cl = parallel_cluster)
# checking
# foreach::getDoParRegistered()
# foreach::getDoParWorkers()

# creating a new list-column with all the models fitted in parallel
(model_all_fitted_continuous$fitted_model <-
  foreach(
    workflow = model_all_fitted_continuous$workflow,
    task     = model_all_fitted_continuous$task,
    .combine  = "c",
    .packages = c("tidymodels", "multilevelmod")
) %dopar% {
  if(task == "explicit"){
    model_fit <-
      list(
        workflow |> 
        fit(data = df_explicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  else{
    model_fit <-
      list(
        workflow |> 
        fit(data = df_implicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  return(model_fit)
# measuring runtime for benchmarking
}) |> system.time() -> time_parallel

model_all_fitted_continuous <-  
  model_all_fitted_continuous |> 
  select(-c(recipe, model, formula, workflow))

# ─── Examining the quality of the models, estimating the parameters ───────────
model_all_fitted_continuous$parameters <-
  foreach(
    fitted_model = model_all_fitted_continuous$fitted_model,
    .combine  = "c",
    .packages = c("parameters")
) %dopar% {
  parameters <- list(model_parameters(fitted_model))
  return(parameters)
}

model_all_fitted_continuous$convergence <-
  foreach(
    fitted_model = model_all_fitted_continuous$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  convergence <- check_convergence(fitted_model)
  return(convergence)
}

model_all_fitted_continuous$model_perf <-
  foreach(
    fitted_model = model_all_fitted_continuous$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  model_perf <- list(model_performance(fitted_model))
  return(model_perf)
}

model_all_fitted_continuous <-
  model_all_fitted_continuous |>
  mutate(
    # extracting model quality indices
    AIC  = model_perf[[1]],
    AICc = model_perf[[2]],
    BIC  = model_perf[[3]]
  ) |>
  ungroup() |> 
  select(-model_perf)

# stopping the cluster when we're done
parallel::stopCluster(cl = parallel_cluster)

# 214 seconds (3min30s)

# ─── Comparing the models' quality among those that converged ─────────────────
model_selection_continuous <-
  model_all_fitted_continuous |> 
  filter(
    formula_id != "formula_0" & 
    convergence == TRUE
  ) |> 
  group_by(task, model_id) |> 
  mutate(best_model = ifelse(AICc == min(AICc), TRUE, FALSE)) |> 
  filter(best_model == TRUE)
```

#### Plotting VI x Congruence

```{r plot_glmm_ex_continuous_congruence}

best_glmm_explicit_continuous <- model_selection_continuous[[4]][[1]]
best_glmm_implicit_continuous <- model_selection_continuous[[4]][[3]]

vizdata_glmm_explicit_congruence <- 
  df_explicit_rt |> 
  visualisation_matrix(
    at = c("visual_imagery", "congruence"),
    length = 100,
    preserve_range = TRUE
)

vizdata_glmm_explicit_congruence <- 
  vizdata_glmm_explicit_congruence |> 
  mutate(predicted = insight::get_predicted(
    best_glmm_explicit_continuous, 
    vizdata_glmm_explicit_congruence
    )) |> 
  mutate(congruence = ifelse(congruence == "congruent", "Congruent", "Incongruent"))

p_glmm_explicit_continuous_congruence <-
  df_explicit_rt |> 
  mutate(congruence = ifelse(congruence == "congruent", "Congruent", "Incongruent")) |>  
  ggplot(aes(
    x = visual_imagery,
    y = rt
  )) +
  geom_line(
    data = vizdata_glmm_explicit_congruence,
    aes(
      y = predicted,
      color = congruence,
      linetype = congruence,
      group = congruence
    ),
    size = 1.5
  ) +
  geom_vline(xintercept = 0, linetype = 2) +
  coord_cartesian(
    ylim = c(450,570)
  ) +
  labs(
    title = "Explicit task",
    x = "",
    y = ""
    ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_color_lancet(name = "Congruence") +
  scale_linetype(name = "Congruence") +
  theme(
    title = element_text(size = txt),
    axis.text = element_text(size = txt/1.5),
    legend.text = element_text(size = txt),
    legend.title = element_text(size = txt)
    )
```

```{r plot_glmm_im_continuous_congruence}

vizdata_glmm_implicit_congruence <- 
  df_implicit_rt |> 
  visualisation_matrix(
    at = c("visual_imagery", "congruence"),
    length = 100,
    preserve_range = TRUE
)

vizdata_glmm_implicit_congruence <- 
  vizdata_glmm_implicit_congruence |> 
  mutate(predicted = insight::get_predicted(
    best_glmm_implicit_continuous, 
    vizdata_glmm_implicit_congruence
    )) |> 
  mutate(congruence = ifelse(congruence == "congruent", "Congruent", "Incongruent"))

p_glmm_implicit_continuous_congruence <-
  df_implicit_rt |> 
  mutate(congruence = ifelse(congruence == "congruent", "Congruent", "Incongruent")) |>  
  ggplot(aes(
    x = visual_imagery,
    y = rt
  )) +
  geom_line(
    data = vizdata_glmm_implicit_congruence,
    aes(
      y = predicted,
      color = congruence,
      linetype = congruence,
      group = congruence
    ),
    size = 1.5
  ) +
  geom_vline(xintercept = 0, linetype = 2) +
  coord_cartesian(
    ylim = c(450,570)
  ) +
  labs(
    title = "Implicit task",
    x = "",
    y = "Response time (ms)"
    ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_color_lancet(name = "Congruence") +
  scale_linetype(name = "Congruence") +
  theme(
    title = element_text(size = txt),
    axis.text = element_text(size = txt/1.5),
    legend.text = element_text(size = txt),
    legend.title = element_text(size = txt)
    )
```

``` {r glmm_continuous_congruence_ggarrange}
#| label: fig-glmm_continuous_ggarrange

p_glmms_continuous_congruence <-
  ggarrange(
    p_glmm_implicit_continuous_congruence,
    p_glmm_explicit_continuous_congruence,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  ) |> 
  annotate_figure(
    bottom = text_grob(
      "Visual imagery (standardized PCA variable)", 
      size = txt,
      vjust = -1
      )
    )

ggexport(
  p_glmms_continuous_congruence, 
  filename = "plots/plots_glmm_continuous_congruence.png", 
  width = 4000,
  height = 1500,
  res = 300
  )
```

#### Plotting VI x SI

```{r plot_glmm_ex_continuous_si}

vizdata_glmm_explicit_si <-
  df_explicit_rt |> 
  visualisation_matrix(c("visual_imagery", "spatial_imagery"), length = 100) |> 
  visualisation_matrix("visual_imagery", length = 15, numerics = "all") 

vizdata_glmm_explicit_si$predicted <- 
  insight::get_predicted(
    best_glmm_explicit_continuous, 
    vizdata_glmm_explicit_si
    )

p_glmm_explicit_continuous_si <-
  df_explicit_rt |> 
  ggplot(aes(
    x = visual_imagery,
    y = rt,
    color = spatial_imagery
    )) +
  geom_line(
    data = vizdata_glmm_explicit_si,
    aes(
      y = predicted,
      group = spatial_imagery
      ),
    size = 1
  ) +
  geom_vline(xintercept = 0, linetype = 2) +
  coord_cartesian(
    ylim = c(200, 850)
  ) +
  labs(
    title = "Explicit task",
    x = "",
    y = ""
    ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_color_viridis(name = "Spatial imagery (PCA)") +
  theme(
    title = element_text(size = txt),
    axis.text = element_text(size = txt/2),
    legend.text = element_text(size = txt/2),
    legend.title = element_text(size = txt)
    )
```

```{r plot_glmm_im_continuous_si}
vizdata_glmm_implicit_si <-
  df_implicit_rt |> 
  visualisation_matrix(c("visual_imagery", "spatial_imagery"), length = 100) |> 
  visualisation_matrix("visual_imagery", length = 15, numerics = "all") 

vizdata_glmm_implicit_si$predicted <- 
  insight::get_predicted(
    best_glmm_implicit_continuous, 
    vizdata_glmm_implicit_si
    )

p_glmm_implicit_continuous_si <-
  df_implicit_rt |> 
  ggplot(aes(
    x = visual_imagery,
    y = rt,
    color = spatial_imagery
    )) +
  geom_line(
    data = vizdata_glmm_implicit_si,
    aes(
      y = predicted,
      group = spatial_imagery
      ),
    size = 1
  ) +
  geom_vline(xintercept = 0, linetype = 2) +
  coord_cartesian(
    ylim = c(225, 825)
  ) +
  labs(
    title = "Implicit task",
    x = "",
    y = "Response time (ms)"
    ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_color_viridis(name = "Spatial imagery (PCA)") +
  theme(
    title = element_text(size = txt),
    axis.text = element_text(size = txt/2),
    legend.text = element_text(size = txt/2),
    legend.title = element_text(size = txt)
    )
```

``` {r glmm_continuous_si_ggarrange}
#| label: fig-glmm_continuous_si_ggarrange

p_glmms_continuous_si <-
  ggarrange(
    p_glmm_implicit_continuous_si,
    p_glmm_explicit_continuous_si,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  ) |> 
  annotate_figure(
    bottom = text_grob(
      "Visual imagery (standardized PCA variable)", 
      size = txt,
      vjust = -1
      )
    )

ggexport(
  p_glmms_continuous_si, 
  filename = "plots/plots_glmm_continuous_si.png", 
  width = 4000,
  height = 1500,
  res = 300
  )
```
