
```{r setup}
#| echo: true
#| output: false
#| code-summary: "Packages"

# ═══ Packages ═════════════════════════════════════════════════════════════════

# The package `librairian` will ease the package management with the "shelf" 
# function, which automatically: 
# 1) checks if a package is installed 
# 2) installs it if need be
# 3) loads the package like the "library()" function would.
if (!require(librarian)) install.packages(librarian)
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── essential package collections ───
  tidyverse,      # modern R ecosystem
  easystats,      # data analysis framework
  tidymodels,     # modelling framework
  doParallel,     # parallel execution
  
  # ─── tidymodels friends ──────────────
  corrr,          # correlational analyses
  tidybayes,      # bayesian inference
  multilevelmod,  # multilevel modelling with lmer and tidymodels
  
  # ─── modelling ───────────────────────
  lme4,           # mixed models
  mclust,         # mixture clustering
  rstanarm,       # bayesian models
  BayesFactor,    # BFs
  emmeans,        # marginal estimates
  statmod,        # easystats dependency
  
  # ─── data management ─────────────────
  readxl,         # importing xlsx
  openxlsx,       # exporting xlsx
  
  #  data visualization ──────────────
  # plot types and geoms
  ricardo-bion/ggradar,  # radar plots
  ggbeeswarm,            # scatter violin plots
  GGally,         # complex plots
  # layout and options
  ggpubr,         # publication plots
  patchwork,      # layout control
  rstatix,        # ggplot stat tools
  # palettes
  ggsci,          # scientific palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly         # interactive plots
)

# ─── Global cosmetic theme ───q
theme_set(theme_bw(base_size = 14))

# ─── Fixing a seed for reproducibility ───
set.seed(14051998)
```

```{r importing_data}
#| echo: true
#| output: false
#| code-summary: "Importing data"

# Implicit task
df_implicit <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_implicit"
    ) |> 
  rename("rt" = rt_implicit)

# Explicit task
df_explicit <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_explicit"
    ) |> 
  rename("rt" = rt_explicit)

# Questionnaires
df_questionnaires <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_questionnaires"
    ) |>  
  # creating the a/phantasia groups
  mutate(
    aphantasia = ifelse(vviq80 < 32, "yes", "no"),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    ) |>
  filter(vviq80 <= 16.5 | vviq80 >= 42)

# updating all the dataframes with the questionnaires data
dfs <- 
  list(
    implicit_task = df_implicit, 
    explicit_task = df_explicit
    ) |> 
  # adding the vviq and group column to every dataframe
  imap(~right_join(
    .x, 
    df_questionnaires |>  
      select(
        # column to match with the correct participants
        subjectid, 
        # new variables to add
        age,
        sexe,
        vviq80,
        aphantasia), by = "subjectid") |>  
      rename("sex" = sexe) |>  
      # reordering the final dfs
      select(
        subjectid, 
        age, sex, aphantasia,
        vviq80,
        everything())
    )

df_implicit <- dfs$implicit_task
df_explicit <- dfs$explicit_task
rm(dfs)
```

```{r finding_and_removing_accuracy_outliers}
#| code-summary: "Removing incorrect trials and task-wise accuracy outliers"
#| echo: true
#| output: false

# ═══ Accuracy outliers analysis ═══════════════════════════════════════════════

# ─── Listing error rates per subject ───
list(df_explicit, df_implicit) |> 
  imap(
    ~.x |> 
      select(subjectid, starts_with("correct"), aphantasia) |> 
      group_by(subjectid) |> 
      count(pick(2)) |> 
      filter(pick(1) == 1) |> 
      ungroup() |> 
      mutate(
        n_tot = max(n),
        prop = (n_tot - n) / n_tot * 100  # analyzing the error rate per subject
      ) %>%
      arrange(desc(prop)) |> 
      select(1, 5)
    )
# 4 accuracy outliers in the explicit task, 4 in the implicit one

# ─── Removing incorrect trials and task-wise accuracy outliers ───
df_explicit_rt <-
  df_explicit |> 
  # filtering out...
  filter(
    # incorrect trials
    correct_explicit == 1 &
    # participants identified with with high error rates
    !(subjectid %in% c( 
      "aknezevic",
      "lbrunie",
      "bluciani",
      "ldossantos"))
    ) |>  
  # removing irrelevant variables
  select(-c(sex, vviq80, orientation, response, correct_explicit))

# percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit)
# 5988 trials left = 6.4% removed

df_implicit_rt <- 
  df_implicit |>  
  filter(
    correct_implicit == 1 &
    !(subjectid %in% c( 
      "bdispaux", 
      "eleveque", 
      "aleclaire", 
      "dchimenton"))
      ) |>  
  select(-c(sex, vviq80, orientation, response, correct_implicit))

# percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit) 
# 5936 trials left = 7.2% removed
```

```{r removing_extreme_rt_outlier_trials}
#| code-summary: "Removing extreme RTs"
#| echo: true
#| output: false

# ═══ First broad RT outlier trials removal ════════════════════════════════════

df_explicit_rt <- 
  df_explicit_rt |>  
  # filtering out extreme RTs
  filter(rt > 300 & rt < 3000)

# total percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit)
# 5950 trials left = 7% removed

df_implicit_rt <- 
  df_implicit_rt |>  
  filter(rt > 300 & rt < 3000)

# total percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit)
# 5896 left = 7.8% removed
```

```{r rt_means_table}
#| label: tbl-rt_means
#| tbl-cap: "Descriptive statistics of the average RTs across the sample in both tasks."

# ═══ RT means table - Both tasks ══════════════════════════════════════════════

df_rt <- 
  list(
  df_explicit_rt |>  rename("Explicit task mean RTs" = rt), 
  df_implicit_rt |>  rename("Implicit task mean RTs" = rt)
  ) |>  
  imap(
    ~select(.x, contains("RT")) |> 
      report() |>  
      as.data.frame() |>  
      select(1:10)
    )

df_rt <- bind_rows(df_rt[[1]], df_rt[[2]])
df_rt |>  display()
```

```{r removing_rt_means_outliers}
#| code-summary: "Removing outliers with aberrant RT means"
#| echo: true
#| output: false

# ═══ Aberrant RT means outliers removal ═══════════════════════════════════════

# ─── Finding outliers ───

df_explicit_rt |> 
  group_by(subjectid) |> 
  summarise(mean_rt = mean(rt)) |> 
  ungroup() |> 
  filter(mean_rt > 1360) |> 
  arrange(desc(mean_rt))
# 3 outliers in the explicit task

df_implicit_rt |> 
  group_by(subjectid) |> 
  summarise(mean_rt = mean(rt)) |> 
  ungroup() |> 
  filter(mean_rt > 1062) |> 
  arrange(desc(mean_rt))
# 2 outliers in the implicit task

# ─── Removing specific outliers ───

df_explicit_rt <- 
  df_explicit_rt |> 
  filter(
    !(subjectid %in% c(
      "dchimenton",
      "mbillon",
      "rcarnez"
    ))
  )

# 5769 trials left
(6400-5769)/6400 # 9.8% out

df_implicit_rt <-
  df_implicit_rt |> 
  filter(
    !(subjectid %in% c(
      "rcarnez",
      "cbertrand"
    ))
  )

# 5777 trials left
# (6400-5777)/6400 # 9.7% out
```

```{r removing_rt_outlier_trials_per_participant}
#| code-summary: "Removing outlier trials by-participant"
#| echo: true
#| output: false

# ═══ By-participant MAD outlier detection ═════════════════════════════════════

# ─── Explicit task: ───
# From 5769 observations
df_explicit_rt <-
  df_explicit_rt |> 
  group_by(subjectid, congruence, color) |> 
  mutate(
    rt_median = median(rt),
    rt_mad = mad(rt)
    ) |> 
  filter(
    rt < rt_median + 5 * rt_mad &
    rt > rt_median - 2.5 * rt_mad
  ) |>
  ungroup()
# 5613 trials left
# (6400-5613)/6400 # 12.3% trials out total
# (5769-5613)/5769 # 2.7% of remaining trials

# ─── Implicit task: ───
# From 5777 observations
df_implicit_rt <-
  df_implicit_rt |> 
  group_by(subjectid, congruence, color) |> 
  mutate(
    rt_median = median(rt),
    rt_mad = mad(rt)
    ) |> 
  filter(
    rt < rt_median + 5 * rt_mad &
    rt > rt_median - 2.5 * rt_mad
  ) |>
  ungroup()
# 5542 trials left
(6400-5542)/6400 # 13.4% trials out total
(5777-5542)/5777 # 4% of remaining trials
```

```{r fitting_glmms}
#| output: false
 
# ═══ Fitting Generalized Linear Mixed Models in parallel ══════════════════════

# ─── Preparing variable roles ─────────────────────────────────────────────────
model_recipe_glmm_ex <- 
  df_explicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

model_recipe_glmm_im <- 
  df_implicit_rt |> 
  recipe() |> 
  update_role(rt, new_role = "outcome") %>%
  update_role(
    subjectid, age, aphantasia, color, congruence, 
    new_role = "predictor"
  ) |> 
  add_role(subjectid, new_role = "group")

# ─── Specifying the distributions for the models ──────────────────────────────

# GLMM, Gamma distribution, identity link
glmm_gamma_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = Gamma(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, identity link
glmm_inverse_id <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "identity")
  )

# GLMM, Inverse Gaussian distribution, log link
glmm_inverse_log <-
  linear_reg() |> 
  set_engine(
    "glmer",
    family = inverse.gaussian(link = "log")
  )

# LMM = GLMM with a Gaussian distribution and identity link
glmm_gaussian <-
  linear_reg() |> 
  set_engine("lmer")

# Listing these models
# GLMMs
model_specs_glmm <- list(
  glmm_gamma_id    = glmm_gamma_id,
  glmm_inverse_id  = glmm_inverse_id,
  glmm_inverse_log = glmm_inverse_log,
  glmm_gaussian    = glmm_gaussian
)

# ─── Writing down the formulas of our models ──────────────────────────────────
# null model
formula_0 <- rt ~ (1|subjectid)
# intercept by-participant only
formula_1 <- rt ~ (aphantasia + congruence + color)^2 + (1|subjectid)
# intercept and slope on congruence by-participant
formula_2 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid)
# intercept and slope on color by-participant
formula_3 <- rt ~ (aphantasia + congruence + color)^2 + (color|subjectid)
# intercept and slope on congruence and color by-participant
formula_4 <- rt ~ (aphantasia + congruence + color)^2 + (congruence|subjectid) + (color|subjectid)

# Listing these formulas
model_formulas <- list(
  formula_0 = formula_0,
  formula_1 = formula_1,
  formula_2 = formula_2,
  formula_3 = formula_3,
  formula_4 = formula_4
  )

# ─── Table to combine everything in workflows and fit the models ──────────────
# (
model_all_workflows_fitted <- 
  tribble(       ~recipe,     ~ task,           ~model,       ~formula,
    model_recipe_glmm_ex, "explicit", model_specs_glmm, model_formulas,
    model_recipe_glmm_im, "implicit", model_specs_glmm, model_formulas
  ) |> 
  # combining recipes with models
  unnest_longer(model) |> 
  # combining them with formulas
  unnest_longer(formula) |>
  rowwise() |>
  mutate(
    # creating workflows
    workflow = list(
      workflow() |>
      add_recipe(recipe) |> 
      add_model(model, formula = formula)  
      )
  )

# ─── Fitting the models with parallel processing ──────────────────────────────

# finding the available cores for parallel processing
n_cores <- parallel::detectCores() - 1

# creating the cluster of cores
parallel_cluster <- 
  parallel::makeCluster(
    n_cores,
    type = "PSOCK"
  )

# registering the cluster for `foreach`
doParallel::registerDoParallel(cl = parallel_cluster)
# checking
# foreach::getDoParRegistered()
# foreach::getDoParWorkers()

# creating a new list-column with all the models fitted in parallel
(model_all_workflows_fitted$fitted_model <-
  foreach(
    workflow = model_all_workflows_fitted$workflow,
    task     = model_all_workflows_fitted$task,
    .combine  = "c",
    .packages = c("tidymodels", "multilevelmod")
) %dopar% {
  if(task == "explicit"){
    model_fit <-
      list(
        workflow |> 
        fit(data = df_explicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  else{
    model_fit <-
      list(
        workflow |> 
        fit(data = df_implicit_rt) |> 
        extract_fit_engine()
      )
  }
  
  return(model_fit)
# measuring runtime for benchmarking
}) |> system.time() -> time_parallel

model_all_workflows_fitted <-  
  model_all_workflows_fitted |> 
  select(-c(recipe, model, formula, workflow))

# ─── Examining the quality of the models, estimating the parameters ───────────
model_all_workflows_fitted$parameters <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("parameters")
) %dopar% {
  parameters <- list(model_parameters(fitted_model))
  return(parameters)
}

model_all_workflows_fitted$convergence <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  convergence <- check_convergence(fitted_model)
  return(convergence)
}

model_all_workflows_fitted$model_perf <-
  foreach(
    fitted_model = model_all_workflows_fitted$fitted_model,
    .combine  = "c",
    .packages = c("performance")
) %dopar% {
  model_perf <- list(model_performance(fitted_model))
  return(model_perf)
}

model_all_workflows_fitted <-
  model_all_workflows_fitted |>
  mutate(
    # extracting model quality indices
    AIC  = model_perf[[1]],
    AICc = model_perf[[2]],
    BIC  = model_perf[[3]]
  ) |>
  ungroup() |> 
  select(-model_perf)

# stopping the cluster when we're done
parallel::stopCluster(cl = parallel_cluster)

# 214 seconds (3min30s)

# ─── Comparing the models' quality among those that converged ─────────────────
model_selection <-
  model_all_workflows_fitted |> 
  filter(
    formula_id != "formula_0" & 
    convergence == TRUE
  ) |> 
  group_by(task, model_id) |> 
  mutate(best_model = ifelse(AICc == min(AICc), TRUE, FALSE)) |> 
  filter(best_model == TRUE)
```

```{r check_the_best_models}
#| fig-label: fig-model_checks
#| fig-cap: "Checking the assumptions of all the models for both tasks."
#| fig-subcap: true
#| layout-ncol: 1

best_glmm_explicit <- model_selection$fitted_model[[2]]
best_glmm_implicit <- model_selection$fitted_model[[5]]
```

```{r glmm_plots_setup}
# setting dodge width for all the geoms
dw <- .75
# stat text size
st <- 7
# legend and axis text size
txt <- 14

# marginal means predictions
best_glmm_explicit_preds <-
  best_glmm_explicit |>
  estimate_means(at = c("aphantasia", "congruence"))

best_glmm_implicit_preds <-
  best_glmm_implicit |>
  estimate_means(at = c("aphantasia", "congruence"))
  
# overall means
rt_mean_explicit <- df_explicit_rt$rt |> mean()
rt_mean_implicit <- df_implicit_rt$rt |> mean()
```

#### Explicit task

```{r glmm_plots_ex}
#| label: fig-model_plots_ex
#| fig-cap: "Plots for the effects of interest in the explicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the explicit task."
#|   - "Group x Congruence interaction in the LMM for the explicit task."

# ─── Model effects plots ───────────────────────────

# annotations
plot_explicit_annotations <-
  df_explicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(785, 795),
    p.adj.signif = c("ns", "***")
    )

# plot
p_glmm_explicit <-
  df_explicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_explicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_explicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  # geom_hline(yintercept = rt_mean_explicit, linetype = 2) +
  
  # stats display
  # stat_pvalue_manual(
  #   plot_explicit_annotations, 
  #   label = "p.adj.signif", 
  #   tip.length = 0,
  #   size = st
  #   ) +
  labs(
    title = "Explicit task",
    x = NULL,
    y = ""
    ) +
  # coord_cartesian(
  #   ylim = c(680, 800)
  # ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

#### Implicit task

```{r glmm_plots_im}
#| label: fig-model_plots_im
#| fig-cap: "Plots for the effects of interest in the implicit task."
#| fig-subcap:
#|   - "Group x Congruence interaction in the GLMM for the implicit task."
#|   - "Group x Congruence interaction in the LMM for the implicit task."

# annotations
plot_implicit_annotations <-
  df_implicit_rt |>
  group_by(aphantasia) |> 
  pairwise_wilcox_test(rt ~ congruence) |> 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) |> 
  mutate(
    y.position = c(646, 653),
    p.adj.signif = c("ns", "***")
    )

# ─── Model effects plot ───────────────────────────
p_glmm_implicit <-
  df_implicit_rt |>
  ggplot(aes(
    x = congruence,
    y = rt,
    color = aphantasia
  )) +
  # linear modelling of the differences
  geom_line(
    data = best_glmm_implicit_preds,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1
  ) +
  # predicted means of the model
  geom_pointrange2(
    data = best_glmm_implicit_preds,
    aes(
      x = congruence,
      y = Mean,
      ymin = Mean - SE,
      ymax = Mean + SE,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  # intercept
  # geom_hline(yintercept = rt_mean_implicit, linetype = 2) +
  
  # stats display
  # stat_pvalue_manual(
  #   plot_implicit_annotations, 
  #   label = "p.adj.signif", 
  #   tip.length = 0,
  #   size = st
  #   ) +
  labs(
    title = "Implicit task",
    x = NULL,
    y = "Response time (ms)"
    ) +
  # coord_cartesian(
  #   ylim = c(580, 655)
  # ) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_color_okabeito(name = "", labels = c("Aphantasia", "Controls")) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```
:::

``` {r glmm_ggarrange}
#| label: fig-glmm_ggarrange

p_glmms <-
  ggarrange(
    p_glmm_implicit,
    p_glmm_explicit,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  )

ggexport(
  p_glmms, 
  filename = "plots/plots_glmm_floor_vviq.png", 
  width = 4000,
  height = 1500,
  res = 300
  )
```

```{r congruence_effect}
#| echo: true
#| code-summary: "Computing the Congruence effect per subject"

# ═══ Congruence effect by-participant ═════════════════════════════════════════

# ─── Empirical approach ───────────────────────────────────────────────────────

# explicit task
congruence_effect_ex_empiric <-
  df_explicit_rt |> 
  group_by(subjectid, congruence) |> 
  summarise(mean_rt = mean(rt)) |> 
  pivot_wider(
    names_from = congruence,
    values_from = mean_rt
  ) |> 
  mutate(
    congruence_eff_ex_emp = uncongruent - congruent,
    .keep = "unused"
  )

# implicit task
congruence_effect_im_empiric <-
  df_implicit_rt |> 
  group_by(subjectid, congruence) |> 
  summarise(mean_rt = mean(rt)) |> 
  pivot_wider(
    names_from = congruence,
    values_from = mean_rt
  ) |> 
  mutate(
    congruence_eff_im_emp = uncongruent - congruent,
    .keep = "unused"
  )

# ─── Merging both ─────────────────────────────────────────────────────────────

df_congruence_effect <-
  inner_join(
    congruence_effect_ex_empiric,
    # subjectid, aphantasia, visual and spatial imagery
    df_questionnaires |> select(1, 4:8),
    by = "subjectid"
  ) |> 
  inner_join(
    congruence_effect_im_empiric,
    by = "subjectid"
  ) |> 
  select(subjectid, aphantasia, vviq80:suis60, 2, 8) |>
  mutate(across(c(vviq80:congruence_eff_im_emp), ~round(as.numeric(.x), digits = 2))) |> 
  ungroup()
```

```{r correlation_congruence_effect}
#| code-summary: "Computing correlations"
#| echo: true

# ═══ Bayesian partial correlations ════════════════════════════════════════════

congruence_correlations_explicit <-
  df_congruence_effect |> 
  select(!c(
    # visual_imagery, spatial_imagery, 
    contains("congruence_eff_im"))) |>
  correlation(
    # bayesian = TRUE,
    # partial = TRUE,
    # partial_bayesian = TRUE,
    # bayesian_test = "bf"
    )
  # summary() |> 
  # plot()

congruence_correlations_implicit <-
  df_congruence_effect |> 
  select(!c(
    # visual_imagery, spatial_imagery, 
    contains("congruence_eff_ex"))) |>  
  correlation(
    # bayesian = TRUE,
    # partial = TRUE,
    # partial_bayesian = TRUE,
    # bayesian_test = "bf"
    )
  # summary() |> 
  # plot()

p_congruence_corr_ex <-
  congruence_correlations_explicit |> 
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
      .x == "vviq80" ~ "VVIQ",
      .x == "osiq_o75" ~ "OSIQ-O",
      .x == "osiq_s75" ~ "OSIQ-S",
      .x == "suis60" ~ "SUIS",
      .x == "congruence_eff_ex_emp" ~ "Congruence\neffect (Empirical)",
      TRUE ~ .x))
    ) |>  
  summary(digits = 2) |>  
  plot(
    text = list(size = 5),
    labs = list(title = "Explicit task")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14),
    title = element_text(size = 14)
    )

p_congruence_corr_im <-
  congruence_correlations_implicit |> 
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
      .x == "vviq80" ~ "VVIQ",
      .x == "osiq_o75" ~ "OSIQ-O",
      .x == "osiq_s75" ~ "OSIQ-S",
      .x == "suis60" ~ "SUIS",
      .x == "congruence_eff_im_emp" ~ "Congruence\neffect (Empirical)",
      TRUE ~ .x))
    ) |>  
  summary(digits = 2) |>  
  plot(
    text = list(size = 5),
    labs = list(title = "Implicit task")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14),
    title = element_text(size = 14)
    )

df_congruence_effect |> 
  ggplot(aes(x = suis60, y = congruence_eff_im_emp)) + 
  geom_point()

p_congruence_correlations <-
  ggarrange(
    p_congruence_corr_ex,
    p_congruence_corr_im,
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    align = "v"
  )

ggexport(
  p_congruence_correlations,
  filename = "plots/plots_congruence_effects_floor_vviq.png",
  width = 7000,
  height = 3000,
  res = 300
  )
```
