---
title: "Sensory priming in aphantasia - Data analysis"
author: "MaÃ«l Delem"
date: 2023-09-13
date-modified: last-modified
date-format: "D/MM/YYYY"
format: html
execute: 
  echo: false
editor_options: 
  chunk_output_type: inline
toc: true
toc-depth: 3
toc-expand: 2
number-sections: true
number-depth: 2
fig-width: 6
fig-asp: .618
---

<!-- CSS options for custom title numbers styling-->
<style>
.header-section-number:after {
  content: ". ";
}
</style>

### Preliminary Set-up {.unnumbered}

::: {.callout-note collapse="true"}

#### Datasets and code availability
The folder containing this HTML file also has a "`data/`" folder, wherein can be found the main Excel file from which the datasets analysed here are sourced, i.e. "`aphantasia_priming_tidy_data.xlsx`". It has several sheets:

- `data_asso`/`_implicit`/`_explicit`/`_rotation`/`_questionnaires` are raw datasheets retrieved from old Excels and cleaned up

- `data_processed_explicit`/`_implicit` are the datasets of the two main tasks after being processed here and in their final state after [exploratory analysis](#sec-eda), and before the [inferential analyses conducted afterwards](#sec-inferential).

The code generating all the computations, figures, tables, etc. (only partially exposed here for clarity) can be found in the `aphantasia_priming_analysis.qmd` file, for which the best reading software are coding Integrated Development Environments (IDE) like [RStudio](https://posit.co/download/rstudio-desktop/) or [Visual Studio Code](https://code.visualstudio.com/). The file includes detailed commentaries along with the code to ease understanding, accessibility, and potential (welcome) criticism.

:::

:::{.callout-note collapse="true"}

#### Software and setup

This analysis was conducted in R language on [RStudio](https://posit.co/download/rstudio-desktop/). This analysis report was written with [Quarto](https://quarto.org/). Just down below is the set-up code, including two essential steps: 

1. Installing the required packages for this data analysis

2. importing data into the dataframes used throughout. 

I left this (usually hidden) step here for reference of the tools used in a view of transparency for the interested reader.

```{r setup}
#| output: false
#| echo: true
#| code-summary: "Packages"

# The package `librairian` will ease the package management with the "shelf" 
# function, which automatically: 
# 1) checks if a package is installed 
# 2) installs it if need be
# 3) loads the package like the "library()" function would.
if (!require(librarian)) install.packages(librarian)
library(librarian)

# now putting packages on our library's shelves:
shelf(
  rstatix,        # ggplot stat tools
  readxl,         # Excel files management
  openxlsx,       # Excel files management
  easystats,      # collection of data analysis packages
  tidymodels,     # collection of packages for a tidy modelling workflow
  bayesian,       # bayesian modelling with brms and tidymodels
  multilevelmod,  # multilevel modelling with lmer and tidymodels
  mclust,         # mixture clustering
  ggpubr,         # ggplot tools
  ggbeeswarm,     # ggplot tools
  plotly,         # interactive plots
  viridis,        # ggplot colour-blind friendly palettes
  tidyverse       # the essentials
)

# --- Global cosmetic theme ---
theme_set(theme_bw(base_size = 14))

# --- Fixing a seed for reproducibility ---
set.seed(14051998)
```

```{r importing_data}
#| echo: true
#| code-summary: "Importing data"

# Association task
df_asso <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_asso")

# Implicit task
df_implicit <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_implicit")

# Explicit task
df_explicit <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_explicit")

# Rotation task
df_rotation <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_rotation")

# Questionnaires
df_questionnaires <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_questionnaires"
    ) %>% 
  # creating the a/phantasia groups
  mutate(
    aphantasia = ifelse(vviq80 < 32, "yes", "no"),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    )
```

:::

:::{.callout-tip appearance="simple"}

#### Interactive figures

Many figures in this report are interactive: hover over the plots to see some of the tools available. You can select a zone to zoom on a plot, hover over bars to see details about data, select only specific groups in the legend, among many other features.

:::

# Exploratory Data Analysis (EDA) {#sec-eda}

## Questionnaire data and imagery

### Correlations

The four questionnaire variables - VVIQ, OSIQ-Object and Spatial, and SUIS - are meant to be used as predictors (independent variables) for the behavioural outcomes of the experiment (dependent variables, RTs and potentially accuracy). Three of these - VVIQ, OSIQ-Object and SUIS - evaluate visual imagery: as such, in a view of parsimony, we are going to evaluate the correlation between these continuous predictors and judge if they should be merged/omitted. The results of the Bayesian partial correlations between the four variables are displayed in @fig-cor_matrix and @tbl-cor_stats. Partial correlations have been chosen here to account for the (theoretically huge) covariance between visual imagery variables. It will result in more conservative estimations of the correlations, and a better understanding of the relations between visual and spatial imagery measures.

```{r correlation}
#| code-summary: "Computing correlations"
#| echo: true

# --- Bayesian partial correlations between questionnaire scores ---
cor_questionnaires <-
  df_questionnaires %>%
  select("vviq80", "suis60","osiq_o75", "osiq_s75") %>% 
  standardise %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )
```

:::{.panel-tabset}

#### Correlation matrix

```{r correlation_matrix}
#| label: fig-cor_matrix
#| fig-cap: "Correlation matrix of the questionnaire variables. The stars indicate the amount of evidence in favour of a correlation, as assessed by the $BF_{10}$: No star = Anecdotal evidence, * = Weak evidence, ** = Moderate evidence, *** = Extreme evidence."

# --- Correlation matrix ---
cor_questionnaires %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "vviq80" ~ "VVIQ",
    .x == "suis60" ~ "SUIS",
    .x == "osiq_o75" ~ "OSIQ-O",
    .x == "osiq_s75" ~ "OSIQ-S"))
    ) %>% 
  summary(digits = 2) %>% 
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )
```

#### Correlation table

```{r correlation_table}
#| label: tbl-cor_stats
#| tbl-cap: "Detailed correlation table of the questionnaire variables."

# --- Correlation table ---
cor_questionnaires %>%
  format %>% 
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "vviq80" ~ "VVIQ",
    .x == "suis60" ~ "SUIS",
    .x == "osiq_o75" ~ "OSIQ-O",
    .x == "osiq_s75" ~ "OSIQ-S"))
    ) %>% 
  rename(
    "Variable 1" = Parameter1,
    "Variable 2" = Parameter2,
    "$\\rho$" = rho,
    "$BF_{10}$" = BF
  ) %>% 
  knitr::kable()
```

:::

As we can see, while the three visual imagery variables are highly correlated, the association between these visual variables and spatial imagery (OSIQ-S) is inconsistent: VVIQ and OSIQ-S are positively correlated although with anecdotal evidence, SUIS and OSIQ-S have a moderately evidenced positive correlation, while OSIQ-Object and OSIQ-Spatial are negatively correlated. We'll run a Principal Component Analysis to evaluate the possibility of a reduction of dimensions by considering visual imagery as a single variable.

### Principal Component Analysis

```{r pca}
#| code-summary: "Computing the PCA"
#| echo: true

# --- Principal Component Analysis ---
pca <- 
  principal_components(
    df_questionnaires[,4:7],
    n = "max",
    sort = TRUE,
    standardize = TRUE
    ) 
```

:::{.panel-tabset}

#### PCA loadings

```{r loadings}
#| label: fig-loadings
#| fig-cap: "Loadings of each variable on the three components extracted by a PCA."

# --- Loadings ---
pca %>% plot + 
  scale_y_discrete(labels = c("OSIQ-S", "OSIQ-O", "VVIQ", "SUIS")) +
  scale_fill_viridis(alpha = .6) +
  labs(title = NULL) +
  theme(text = element_text(size = 14))
```

#### Eigenvalues / variance explained

```{r eigenvalues}
#| label: tbl-eigenvalues
#| tbl-cap: "Eigenvalues and variance explained by the three components extracted by a PCA."

# --- Eigenvalues and variance ---
pca %>% 
  summary %>% 
  rename(
    "Component 1" = PC1,
    "Component 2" = PC2,
    "Component 3" = PC3,
  ) %>%
  format(digits = 2) %>% 
  display
```

:::

As expected, the visual imagery variables are grouped in a single component ("PC1" in @fig-loadings or "Component 1" in @tbl-eigenvalues) with an Eigenvalue of 2.84, therefore accounting for 71% of the total variance of the data. The second component (PC2 / Component 2) has an Eigenvalue of .97, accounts for 24% of the total variance, and is mostly loaded negatively on the OSIQ-S: this brings the total explained proportion of variance to 95%. Traditionally, the threshold used for selecting a principal component is an Eigenvalue of 1. Nevertheless, given the Eigenvalue of .97 and the properties of this component, i.e. singling out the specificity of the spatial scale, this component will be kept for analysis.

Standardized predicted values for each component will be computed and added to the data, renamed ***Visual Imagery*** (PC1, mostly VVIQ + SUIS + OSIQ-O with an influence of OSIQ-S) and ***Spatial Imagery*** (PC2, mostly OSIQ-S, with an influence of SUIS). In order to align the Spatial Imagery variable with the OSIQ-Spatial, the variable will be reversed (they are correlated negatively as it is, as shown by the -.96 loading on PC2).

```{r pca_components}
#| code-summary: "Adding the predicted PCA components to the data"
#| echo: true

pca_components <- pca %>% predict

df_questionnaires <- 
  bind_cols(df_questionnaires, pca_components[,1:2]) %>% 
  mutate(
    PC1 = standardize(PC1),
    PC2 = -standardize(PC2)
    ) %>% 
  rename(
    "visual_imagery" = PC1,
    "spatial_imagery" = PC2
    )

rm(pca, pca_components)
```

### VVIQ, visual and spatial imagery

Now we can see that the main visual imagery variables, namely the (conventional) VVIQ and the *Visual Imagery* component created through PCA are completely decorrelated from the *Spatial Imagery* component, as shown in @fig-spatial_corr1 and [-@fig-spatial_corr2].

<!-- ggpubr ggarranges replacd by plotly panels further down -->
```{r spatial_correlations_ggarrange}
#| out-width: 100%
#| eval: false

# # --- correlation between visual and spatial ---
# p_cor_spatial_vis <-
#   df_questionnaires %>%
#   mutate(
#     visual_imagery = rescale(visual_imagery),
#     spatial_imagery = rescale(spatial_imagery),
#     aphantasia = fct_relevel(aphantasia, "yes", "no")
#     ) %>%
#   ggplot(aes(x = visual_imagery, y = spatial_imagery)) +
#   geom_point(aes(color = aphantasia)) +
#   scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
#   geom_smooth(
#     aes(x = visual_imagery, y = spatial_imagery),
#     color = "black",
#     method = "lm",
#     stat = "smooth") +
#   stat_cor(
#     aes(x = visual_imagery,
#         y = spatial_imagery),
#     inherit.aes = FALSE,
#     cor.coef.name = "r",
#     label.x.npc = .5,
#     label.y.npc = .7) +
#   labs(x = "Visual Imagery",
#        y = "Spatial Imagery")
# 
# # --- correlation between vviq and spatial ---
# p_cor_spatial_vviq <-
#   df_questionnaires %>%
#   mutate(
#     vviq80 = rescale(vviq80),
#     spatial_imagery = rescale(spatial_imagery)
#     ) %>%
#   ggplot(aes(x = vviq80, y = spatial_imagery)) +
#   geom_point(aes(color = aphantasia)) +
#   scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
#   geom_smooth(
#     aes(x = vviq80, y = spatial_imagery),
#     color = "black",
#     method = "lm",
#     stat = "smooth") +
#   stat_cor(
#     aes(x = vviq80,
#         y = spatial_imagery),
#     inherit.aes = FALSE,
#     cor.coef.name = "r",
#     label.x.npc = .5,
#     label.y.npc = .7) +
#   labs(x = "VVIQ",
#        y = "Spatial Imagery")
# 
# p_spatial_correlations <-
#   ggarrange(
#   p_cor_spatial_vis,
#   p_cor_spatial_vviq + rremove("ylab"),
#   common.legend = TRUE,
#   legend = "bottom"
#   )
#
# rm(p_cor_spatial_vis, p_cor_spatial_vviq)
# p_spatial_correlations
```

:::{.panel-tabset}

#### Spatial and Visual Imagery

```{r spatial_correlations_plotly_1}
#| label: fig-spatial_corr1
#| fig-cap: "Correlation between the visual imagery PCA component and the spatial imagery one."
#| out-width: 100%

# --- correlation between visual and spatial ---
(df_questionnaires %>% 
  mutate(
    visual_imagery = rescale(visual_imagery),
    spatial_imagery = rescale(spatial_imagery),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    ) %>%
  ggplot(aes(x = visual_imagery, y = spatial_imagery)) +
  geom_point(aes(color = aphantasia)) +
  scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  geom_smooth(
    aes(x = visual_imagery, y = spatial_imagery), 
    color = "black", 
    method = "lm", 
    stat = "smooth") +
  # stat_cor(
  #   aes(x = visual_imagery,
  #       y = spatial_imagery),
  #   inherit.aes = FALSE,
  #   cor.coef.name = "r",
  #   label.x.npc = .5,
  #   label.y.npc = .7) +
  labs(x = "Visual Imagery",
       y = "Spatial Imagery")) %>% 
  ggplotly
```

#### Spatial imagery and VVIQ

```{r spatial_correlations_plotly_2}
#| label: fig-spatial_corr2
#| fig-cap: "Correlation between the VVIQ and the spatial imagery PCA component."
#| out-width: 100%

# --- correlation between VVIQ and spatial ---
(df_questionnaires %>% 
  mutate(
    vviq80 = rescale(vviq80),
    spatial_imagery = rescale(spatial_imagery)
    ) %>%
  ggplot(aes(x = vviq80, y = spatial_imagery)) +
  geom_point(aes(color = aphantasia)) +
  scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  geom_smooth(
    aes(x = vviq80, y = spatial_imagery), 
    color = "black", 
    method = "lm", 
    stat = "smooth") +
  # stat_cor(
  #   aes(x = vviq80,
  #       y = spatial_imagery),
  #   inherit.aes = FALSE,
  #   cor.coef.name = "r",
  #   label.x.npc = .5,
  #   label.y.npc = .7) +
  labs(x = "VVIQ",
       y = "Spatial Imagery")) %>% 
  ggplotly
```
:::

The consequence of this analysis is that from this point onwards two options are available to model the data:

1. Using **Aphantasia (i.e. *****Group*****) as a categorical variable** by binning the participants in two categories based on the VVIQ (cut-off at 32, Aphantasics/Phantasics).

2. Using **Visual Imagery as a continuous variable.**

In the main inferential analyses conducted down below, the first option has been selected for its ease of interpretation (and closeness to the results that would be obtained with the continuous variable anyway). Nevertheless, the two continuous variables of visual and spatial imagery defined here will be explored later in [further data exploration](#sec-further).

```{r df_updates_with_imagery}

# updating all the dataframes with the imagery variables
dfs <- 
  list(
    association_task = df_asso, 
    implicit_task = df_implicit, 
    explicit_task = df_explicit, 
    rotation_task = df_rotation
    ) %>%
  # adding the vviq and group column to every dataframe
  imap(~left_join(
    .x, 
    df_questionnaires %>% 
      select(
        # column to match with the correct participants
        subjectid, 
        # new variables to add
        age,
        sexe,
        vviq80, 
        visual_imagery, 
        spatial_imagery,
        aphantasia), by = "subjectid") %>% 
      rename("sex" = sexe) %>% 
      # reordering the final dfs
      select(
        subjectid, 
        age, sex, aphantasia,
        vviq80, visual_imagery, spatial_imagery,
        everything())
    )

df_asso     <- dfs$association_task
df_implicit <- dfs$implicit_task
df_explicit <- dfs$explicit_task
df_rotation <- dfs$rotation_task
rm(dfs)
```

## Outcomes: accuracy and RTs?

### Accuracy

While response time is an obvious outcome to model when it comes to decision tasks, the use of accuracy is more debatable, as the tasks were not specifically designed to be challenging. Traditionally, accuracy is mostly used to remove incorrect trials from RT modelling. We'll describe accuracy data to evaluate its relevance in this analysis. The distribution of the percentages of errors across the three tasks is displayed in @fig-tasks_errors, [-@fig-tasks_errors2], and [-@fig-tasks_errors3].

<!-- ggpubr ggarrange replaced by plotly panels further down -->
```{r tasks_errors_ggarrange}

# p_errors_asso <-
#   (df_asso %>%
#   select(subjectid, correct_association, aphantasia) %>% 
#   group_by(subjectid, aphantasia) %>% 
#   count(correct_association) %>% 
#   filter(correct_association == 1) %>% 
#   mutate(n = 100 - n) %>% 
#   ggplot(aes(
#     x = n, 
#     color = aphantasia,
#     fill = aphantasia
#     )) +
#   geom_bar(
#     aes(y = after_stat(prop)),
#     position = position_dodge(),
#     width = .8,
#     alpha = .5
#     ) +
#   scale_x_continuous(
#     name = "Percentage of errors",
#     breaks = breaks_pretty(n = 20)
#     ) +
#   scale_y_continuous(
#     name = "Proportion of the group",
#     breaks = breaks_pretty(n = 10)
#     ) +
#   coord_cartesian(
#     xlim = c(0.5, 24),
#     ylim = c(.01,.36)
#     ) +
#   labs(
#     title = "Association task",
#     # x = NULL,
#     # y = "Proportion of the group"
#     # y = NULL
#     ) +
#   scale_color_okabeito(guide = NULL) +
#   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
#   # ggplotly-ing it
#   ggplotly #%>%
#   # layout(
#   #   annotations = list(
#   #     list(
#   #       x = .01, 
#   #       y = 1.11, 
#   #       text = "Association task", 
#   #       showarrow = F, 
#   #       xref='paper', 
#   #       yref='paper'
#   #       )))
# 
# p_errors_implicit <-
#   (df_implicit %>%
#   select(subjectid, correct_implicit, aphantasia) %>% 
#   group_by(subjectid, aphantasia) %>% 
#   count(correct_implicit) %>% 
#   filter(correct_implicit == 1) %>% 
#   mutate(n = (64 - n)/64 * 100) %>% 
#   ggplot(aes(
#     x = n, 
#     color = aphantasia,
#     fill = aphantasia
#     )) +
#   geom_bar(
#     aes(y = after_stat(prop)),
#     position = position_dodge(),
#     width = .6,
#     alpha = .6
#     ) +
#   scale_x_continuous(
#     name = "Percentage of errors",
#     breaks = breaks_pretty(n = 20)
#     ) +
#   scale_y_continuous(
#     name = "Proportion of the group",
#     breaks = breaks_pretty(n = 10)
#     ) +
#   coord_cartesian(
#     xlim = c(1, 40),
#     ylim = c(.01,.28)
#     ) +
#   labs(
#     title = "Implicit task",
#     # x = "Percentage of errors",
#     # y = "Proportion of the group"
#     # y = NULL
#     ) +
#   scale_color_okabeito(guide = NULL) +
#   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
#   # plotly-ing it
#   ggplotly #%>%
#   # layout(
#   #   annotations = list(
#   #     list(
#   #       x = .01, 
#   #       y = 1, 
#   #       text = "Implicit task", 
#   #       showarrow = F, 
#   #       xref='paper', 
#   #       yref='paper'
#   #       ))) %>% 
#   # style(showlegend = FALSE)
# 
# p_errors_explicit <-
#   (df_explicit %>%
#   select(subjectid, correct_explicit, aphantasia) %>% 
#   group_by(subjectid, aphantasia) %>% 
#   count(correct_explicit) %>% 
#   filter(correct_explicit == 1) %>% 
#   mutate(n = (64 - n)/64 * 100) %>% 
#   ggplot(aes(
#     x = n, 
#     color = aphantasia,
#     fill = aphantasia
#     )) +
#   geom_bar(
#     aes(y = after_stat(prop)),
#     position = position_dodge(),
#     width = .6,
#     alpha = .6
#     ) +
#   scale_x_continuous(
#     name = "Percentage of errors",
#     breaks = breaks_pretty(n = 20)
#     ) +
#   scale_y_continuous(
#     name = "Proportion of the group",
#     breaks = breaks_pretty(n = 10)
#     ) +
#   coord_cartesian(
#     xlim = c(1, 46),
#     ylim = c(.01,.36)
#     ) +
#   labs(
#     title = "Explicit task",
#     # x = NULL,
#     # y = "Proportion of the group"
#     ) +
#   scale_color_okabeito(guide = NULL) +
#   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
#   # plotly-ing it
#   ggplotly #%>%
#   # layout(
#   #   annotations = list(
#   #     list(
#   #       x = .01, 
#   #       y = 1.065, 
#   #       text = "Explicit task", 
#   #       showarrow = F, 
#   #       xref='paper', 
#   #       yref='paper'
#   #       ))) %>% 
#   # style(showlegend = FALSE)
  
# # garrange
# p_tasks_error_perc <-
#   ggarrange(
#   p_errors_asso,
#   p_errors_explicit,
#   p_errors_implcit,
#   ncol = 1,
#   common.legend = TRUE,
#   legend = "right"
#   ) %>%
#   annotate_figure(
#     left = text_grob("Proportion of the group", size = 20, rot = 90),
#     bottom = text_grob("Percentage of errors", size = 20)
#     )

# rm(p_errors_asso, p_errors_explicit, p_errors_implcit)
# p_tasks_error_perc
```

:::{.panel-tabset}

#### Association task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_asso}
#| echo: false
#| label: fig-tasks_errors
#| fig-cap: "Distribution of errors between groups in the association task."
#| out-width: 100%
#| fig-height: 4

(df_asso %>%
  select(subjectid, correct_association, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_association) %>% 
  filter(correct_association == 1) %>% 
  mutate(n = 100 - n) %>% 
  ggplot(aes(
    x = n,
    fill = aphantasia,
    
    )) +
  geom_bar(
    aes(
    y = after_stat(prop), 
    color = aphantasia),
    position = position_dodge(),
    width = .8,
    alpha = .5
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(0.5, 24),
    ylim = c(.01,.36)
    ) +
  # labs(title = "Association task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # ggplotly-ing it
  ggplotly
```

#### Explicit task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_explicit}
#| echo: false
#| label: fig-tasks_errors2
#| fig-cap: "Distribution of errors between groups in the explicit task."
#| out-width: 100%
#| fig-height: 4


(df_explicit %>%
  select(subjectid, correct_explicit, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_explicit) %>% 
  filter(correct_explicit == 1) %>% 
  mutate(n = (64 - n)/64 * 100) %>% 
  ggplot(aes(
    x = n, 
    color = aphantasia,
    fill = aphantasia
    )) +
  geom_bar(
    aes(y = after_stat(prop)),
    position = position_dodge(),
    width = .6,
    alpha = .6
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(1, 46),
    ylim = c(.01,.36)
    ) +
  # labs(title = "Explicit task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # plotly-ing it
  ggplotly
```

#### Implicit task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_implicit}
#| echo: false
#| label: fig-tasks_errors3
#| fig-cap: "Distribution of errors between groups in the implicit task."
#| out-width: 100%
#| fig-height: 4

(df_implicit %>%
  select(subjectid, correct_implicit, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_implicit) %>% 
  filter(correct_implicit == 1) %>% 
  mutate(n = (64 - n)/64 * 100) %>% 
  ggplot(aes(
    x = n, 
    color = aphantasia,
    fill = aphantasia
    )) +
  geom_bar(
    aes(y = after_stat(prop)),
    position = position_dodge(),
    width = .6,
    alpha = .6
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(1, 40),
    ylim = c(.01,.28)
    ) +
  # labs(title = "Implicit task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # plotly-ing it
  ggplotly
```

:::

We can see that the distributions are skewed yet very similar across groups and tasks: as such, we are going to use accuracy only to remove missed trials in the analyses of the RTs. Nonetheless, the main noticeable aspect of these plots are the obvious outliers in each of the tasks. This information is crucial as it could reflect a wrong understanding of the task, or random responses: in any case, participants that have high error rates would require a deletion of up to 47% of their trials, which could become a bias in the modelling of the overall data. As there appear to be a cut-off of the main distribution around 16% of errors in all tasks, this value will be chosen as threshold. **For each task separately, the results of participants with an error rate superior to 16% will be removed ***(i.e., nine participants in the explicit task and four in the implicit task)***, prior to removing all the missed trials of the remaining participants for modelling RTs.**

<!-- Finding and removing accuracy outliers -->
```{r acc_outliers}
#| code-summary: "Removing incorrect trials and task-wise accuracy outliers"
#| output: false

# --- Accuracy outliers analysis ---
list(df_explicit, df_implicit) %>%
  imap(
    ~.x %>%
      select(subjectid, starts_with("correct"), aphantasia) %>%
      group_by(subjectid) %>%
      count(pick(2)) %>%
      filter(pick(1) == 1) %>%
      ungroup() %>%
      mutate(
        n_tot = max(n),
        prop = (n_tot - n) / n_tot * 100  # analysing the error rate per subject
      ) %>%
      arrange(desc(prop)) %>%
      select(1, 5)
    )
# 9 accuracy outliers in the explicit task, 4 in the implicit one

# --- Removing incorrect trials and task-wise accuracy outliers ---
df_explicit_rt <-
  df_explicit %>%
  # filtering out...
  filter(
    # incorrect trials
    correct_explicit == 1 &
    # participants identified with with high error rates
    !(subjectid %in% c(
      "ldossantos", 
      "aknezevic", 
      "agayou", 
      "bluciani", 
      "lbrunie", 
      "cmarcenytheault", 
      "llhermitte", 
      "jnaruse", 
      "fc"))
    ) %>% 
  # removing irrelevant variables
  select(-c(sex, vviq80, orientation, response, correct_explicit))

df_implicit_rt <- 
  df_implicit %>% 
  filter(
    correct_implicit == 1 &
    !(subjectid %in% c(
      "bdispaux", 
      "eleveque", 
      "aleclaire", 
      "dchimenton"))
      ) %>% 
  select(-c(sex, vviq80, orientation, response, correct_implicit))
```

### Response times

As a preprocessing, response times below 300ms and above 3000ms will be removed as they could represent false alarms. The removal of accuracy outliers, incorrect trials and extreme RTs amounted to 5.3% of trials in both tasks. There could also be outliers with overall abnormally fast or slow RTs. This analysis will be conducted on mean RTs, the distribution of which is displayed in @fig-rt_means1 and [-@fig-rt_means2], along with the overall RT statistics in @tbl-rt_means. 

<!-- Removing extreme RT outlier trials -->
```{r removing_first_rt_outliers}
#| code-summary: "Removing extreme RTs"
#| echo: true

# --- First broad RT outlier trials removal ---
df_explicit_rt <- 
  df_explicit_rt %>% 
  # filtering out extreme RTs
  filter(rt_explicit > 300 & rt_explicit < 3000)

df_implicit_rt <- 
  df_implicit %>% 
  filter(rt_implicit > 300 & rt_implicit < 3000)
```

:::{.panel-tabset}

#### Explicit task
<!-- RT means -->

```{r explicit_task_rt_means_plotly}
#| label: fig-rt_means1
#| fig-cap: "Distribution of the mean RTs of participants in the explicit task (very high RT means have been cut for better readability)."
#| fig-height: 4

(df_explicit_rt %>% 
  group_by(subjectid) %>% 
  summarise(mean = mean(rt_explicit)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(
    fill = "aquamarine2", 
    color = "aquamarine4", 
    alpha = .3,
    bins = 70
    # adjust = .4
    ) +
  labs(x = "Mean RT", y = "Number of participants") +
  scale_y_continuous(breaks = seq(0, 13, by = 1))) %>% 
  ggplotly
```

#### Implicit task
<!-- RT means -->

```{r implicit_task_rt_means_plotly}
#| label: fig-rt_means2
#| fig-cap: "Distribution of the mean RTs of participants in the implicit task (very high RT means have been cut for better readability)."
#| fig-height: 4

(df_implicit_rt %>%
  group_by(subjectid) %>% 
  summarise(mean = mean(rt_implicit)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(
    fill = "coral2", 
    color = "coral4", 
    alpha = .3,
    bins = 70
    # adjust = .4
    ) +
  labs(x = "Mean RT", y = "Number of participants") +
  scale_y_continuous(breaks = seq(0, 9, by = 1))) %>% 
  ggplotly
```

:::

<!-- RT means table -->
```{r rt_means_table}
#| label: tbl-rt_means
#| tbl-cap: "Descriptive statistics of the average RTs across the sample in both tasks."

df_rt <-
  list(
  df_explicit_rt %>% rename("Explicit task mean RTs" = rt_explicit), 
  df_implicit_rt %>% rename("Implicit task mean RTs" = rt_implicit)
  ) %>% 
  imap(
    ~select(.x, contains("RT")) %>%
      report() %>% 
      as.data.frame() %>% 
      select(1, 2, 5:10)
    )

df_rt <- bind_rows(df_rt[[1]], df_rt[[2]])
df_rt %>% display
```

For a more precise analysis of mean RTs, we are going to use an outlier detection method based on the Median Absolute Deviation (MAD). Common thresholds used for outlier detection in Gaussian distributions with MADs are $median \ \pm \ 3 \cdot MAD$ (Hampel filter; see Enderlein, 1987). However, the low boundary of this interval would be aberrant in our case of a skewed distribution, as all the participants with fast RT means stay above the $median \ - \ 1.5 \cdot MAD$. As for the high threshold, the choice the $median \ + \ 3 \cdot MAD$ would set a highly conservative maximum (e.g. 1361ms for the explicit task) and bring the total number of removed trials over 13%. Therefore, after careful examination of the ranges, and in order to keep as many trials as possible, we chose to set the upper threshold at $median \ + \ 5 \cdot MAD$ (i.e. 1797ms for the explicit task, and 1386ms for the implicit task), resulting in an overall deletion of 7.9% of trials in the explicit task and 8.9% of trials in the implicit one in this whole data preprocessing. The resulting distributions of RTs for each participant are represented in @fig-rt_per_participant_1 and [-@fig-rt_per_participant_2].

<!-- Removing second step RT outlier trials -->
```{r removing_second_rt_outliers}
#| code-summary: "Removing individual outlier trials"
#| output: false

# --- second precise RT outlier trials removal ---
df_explicit_rt <-
  df_explicit_rt %>% 
  filter(rt_explicit < 1797)

df_implicit_rt <-
  df_implicit_rt %>% 
  filter(rt_implicit < 1386)

# --- checking individual participant stats for lone outliers ---
df_explicit_rt %>%
  group_by(subjectid) %>%
  summarise(
    mean = mean(rt_explicit),
    mad = mad(rt_explicit),
    min = min(rt_explicit),
    max = max(rt_explicit),
    obs = length(rt_explicit)
  )

df_implicit_rt %>%
  group_by(subjectid) %>%
  summarise(
    mean = mean(rt_implicit),
    mad = mad(rt_implicit),
    min = min(rt_implicit),
    max = max(rt_implicit),
    obs = length(rt_implicit)
  )

# --- removing the last outlier (unusually high mean RT) ---
df_explicit_rt <- df_explicit_rt %>% filter(subjectid != "dchimenton")
```

:::{.panel-tabset}

#### Explicit task
<!-- RT distribution for each participant -->

```{r explicit_rt_per_participant_plotly}
#| label: fig-rt_per_participant_1
#| fig-cap: "Resulting distribution of RTs in the explicit task for each participant. Each colored distribution represents a single participant."
#| fig-height: 4

(df_explicit_rt %>% 
  ggplot(aes(x = rt_explicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(10)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  coord_cartesian(
    xlim = c(380, 1700),
    ylim = c(0.0005, .01)
  ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

#### Implicit task
<!-- RT distribution for each participant -->

```{r implicit_rt_per_participant_plotly}
#| label: fig-rt_per_participant_2
#| fig-cap: "Resulting distribution of RTs in the implicit task for each participant. Each colored distribution represents a single participant."
#| fig-height: 4

(df_implicit_rt %>% 
  ggplot(aes(x = rt_implicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(5)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  coord_cartesian(
    xlim = c(350, 1320),
    ylim = c(0.0005, .01)
  ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

:::

# Inferential Analyses {#sec-inferential}

## Model description (Multilevel modelling)

### Predictors and outcome

Our analyses aim to evaluate the predictive abilities of several variables on one outcome, the **Response times** of the participants in the tasks. Our potential predictors are:

- **Group** membership, with two levels: aphantasia/phantasia

- **Congruence** conditions, with two levels: congruent and incongruent

- **Color** conditions, with two levels: colored and uncolored.

Sub-levels of the responses ("random effects") can also be accounted for:

- the dependency between response times for **each participant** (also referred to as *subjects*) caused by repeated measures

- the varying effects of **Congruence for each participant**

- the varying effects of **Color for each participant**.

This structure will be modeled using Multilevel Models (also called hierarchical or mixed models), allowing us to add the sub-levels of the hierarchy within participant measures we described above in the model. **RT data will be transformed using a Box-Cox transformation** to bring the distributions closer to normality and improve the quality of the models.

For a subject $i$ with a given *Group*/visual imagery, in the *Congruence* condition $j$ and the *Color* condition $k$, The maximal model of the resulting response time $RT_{ijk}$ including all the predictors can be specified as such:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}}) \cdot Color_{k}
\end{aligned}
$$
Where $\alpha$ is the global intercept, $\alpha_{subject[i]}$ is a random intercept by subject accounting for the inherent randomness tied to participants, and $\beta_{1/2/3}$ are the parameters representing the effect of each predictor. The "$\times$" signs represent the potential interactions along with fixed effects. Additionally, as one might expect the effect of the Congruence or Color conditions to have important variations between subjects (tied to cognitive processes and strategies), a slope $\beta_{2/3_{subject[i]}}$ by subject can be added to the Congruence or Color parameters to account for this variance.

### Potential models

**RT data will be standardized before modelling**, therefore the standardized coefficients reported will scale on *standard deviations of the RT variable*. This allows to interpret these parameters (regression coefficients) roughly with the same guidelines as usual indices, e.g., Cohen's *d*. Several models will be fitted in turn. we are not going to fit reduced models that exclude some main effects, as we would like to model all of them in any case to produce inferences (as opposed to raw predictions of future data). We'll specify the following: 

1. The minimal model, accounting only for the random effect of subjects, will serve as a comparison baseline: 

$$
\begin{aligned}
RT_{i} = \alpha + \alpha_{subject[i]}
\end{aligned}
$$

2. A model accounting for the fixed effects only:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} + \beta_{2} \cdot Congruence_{j} + \beta_{3} \cdot Color_{k}
\end{aligned}
$$

3. A model with all interactions:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times \beta_{2} \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

4. A model with a slope by subject on Congruence:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

5. A model with a slope by subject on Color:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times \beta_{2} \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}})  \cdot Color_{k}
\end{aligned}
$$

6. The maximal model:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}})  \cdot Color_{k}
\end{aligned}
$$

The significant parameters will thereafter be analysed with **marginal contrasts analyses** (robust model-based analogues of "post-hoc" statistical tests) to evaluate the differences between factor levels.

## Fitting the models

```{r model_recipes_workflows_and_engines}

# --- preprocessing recipe for explicit data ---
recipe_explicit <- 
  df_explicit_rt %>%
  recipe %>%
  update_role(
    rt_explicit, 
    new_role = "outcome"
    ) %>%
  update_role(
    subjectid, age,
    aphantasia, visual_imagery, spatial_imagery,
    color, congruence,
    new_role = "predictor"
    ) %>%
  add_role(
    subjectid, 
    new_role = "group"
    ) %>%
  step_BoxCox(rt_explicit) %>% 
  step_normalize(rt_explicit, age)

# --- preprocessing recipe for implicit data ---
recipe_implicit <- 
  df_implicit_rt %>%
  recipe %>%
  update_role(
    rt_implicit, 
    new_role = "outcome"
    ) %>%
  update_role(
    subjectid, age,
    aphantasia, visual_imagery, spatial_imagery,
    color, congruence,
    new_role = "predictor"
    ) %>%
  add_role(
    subjectid,
    new_role = "group"
    ) %>%
  step_BoxCox(rt_implicit) %>% 
  step_normalize(rt_implicit, age) 

# --- workflows ---
workflow_explicit <- workflow() %>% add_recipe(recipe_explicit)
workflow_implicit <- workflow() %>% add_recipe(recipe_implicit)

# --- type of bayesian models to be fitted ---
# mod_0_bayesian <-
#   bayesian (
#     family = skew_normal()
#   ) %>%
#   set_engine("brms") %>% 
#   set_mode("regression")

# --- mixed models to be fitted ---
mod_0_lmer <-
  linear_reg() %>% 
  set_engine("lmer")
```

```{r model_specifications}
# --- Specifying all models ---

# --- Explicit task ---

# model 1: intercepts only
mod_explicit_1 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ (1|subjectid)
    )

# model 2: fixed effects
mod_explicit_2 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia + congruence + color + (1|subjectid)
  )

# model 3: interactions
mod_explicit_3 <- 
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1|subjectid)
  )

# model 4: slope by subject on congruence
mod_explicit_4 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1 + congruence|subjectid)
  )

# model 5: slope by subject on color
mod_explicit_5 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1 + color|subjectid)
  )

# model 6: maximal model
mod_explicit_6 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + 
      (1 + congruence|subjectid) + (1 + color|subjectid)
  )

# --- Implicit task ---

# model 1: intercepts only
mod_implicit_1 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ (1|subjectid)
    )

# model 2: group effect
mod_implicit_2 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia + congruence + color + (1|subjectid)
  )

# model 3: congruence effect
mod_implicit_3 <- 
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1|subjectid)
  )

# model 4: color effect
mod_implicit_4 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1 + congruence|subjectid)
  )

# model 5: slope by subject on color
mod_implicit_5 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1 + color|subjectid)
  )

# model 6: maximal model
mod_implicit_6 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + 
      (1 + congruence|subjectid) + (1 + color|subjectid)
  )
```

```{r model_fitting}

# --- Fitting all models ---

# --- Explicit task ---
fit_explicit_mods <-
  list(
    mod_explicit_1 = mod_explicit_1, 
    mod_explicit_2 = mod_explicit_2, 
    mod_explicit_3 = mod_explicit_3,
    mod_explicit_4 = mod_explicit_4,
    mod_explicit_5 = mod_explicit_5#,
    # mod_explicit_6 = mod_explicit_6
    ) %>% 
  imap(~ .x %>% fit(data = df_explicit_rt) %>% extract_fit_engine())

# --- Implicit task ---
fit_implicit_mods <-
  list(
    mod_implicit_1 = mod_implicit_1,
    mod_implicit_2 = mod_implicit_2,
    mod_implicit_3 = mod_implicit_3,
    mod_implicit_4 = mod_implicit_4,
    mod_implicit_5 = mod_implicit_5#,
    # mod_implicit_6 = mod_implicit_6
    ) %>%
  imap(~ .x %>% fit(data = df_implicit_rt) %>% extract_fit_engine())

# --- nested list of all the lists of models ---
all_models_fitted <-
  list(
    explicit_mods = fit_explicit_mods,
    implicit_mods = fit_implicit_mods
    )
```

All of these models have been fitted for both tasks and their predictive performances compared using the AIC, AICc (Akaike corrected Information Criterion) and BIC (Bayesian Information Criterion). The maximal model did not converge, so only the first five models were evaluated. The results of these analyses are displayed in @tbl-model_perfs-1 and @tbl-model_perfs-2.

```{r model_performances}
#| label: tbl-model_perfs
#| tbl-cap: "Performances of the fitted models for both tasks."
#| tbl-subcap: 
#|   - "**Explicit** task models."
#|   - "**Implicit** task models."
#| layout-ncol: 1

# --- Performance of the models ---
all_models_performances <- 
  all_models_fitted %>% 
  imap(
    ~ .x %>% 
      compare_performance(metrics = c("AIC", "AICc", "BIC")) %>% 
      select(-Model) %>%
      display
    )

all_models_performances[["explicit_mods"]]

all_models_performances[["implicit_mods"]]


```

The fourth model including a slope by subject on Congruence is the most supported for the **explicit task**, while the third model including only an intercept by subject has the best performance among models including interactions for the **implicit task**. The parameters of these two models will be analysed in the following.

```{r model_parameters}

# --- Parameter estimations ---
all_models_parameters <-
  all_models_fitted %>% 
  imap(
    ~ .x %>%
      imap(
        ~ .x %>% 
          model_parameters(effects = "fixed") %>%
          display
        )
    )
```

## Explicit task results

```{r model_explicit_contrasts}

# model 4 contrasts between aphantasia and congruence levels
c_ex_gcn <-
  all_models_fitted[[1]][[4]] %>% 
  estimate_contrasts(contrast = c("aphantasia", "congruence")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))

# model 4 contrasts between color levels
c_ex_col <-
  all_models_fitted[[1]][[4]] %>% 
  estimate_contrasts(contrast = c("color")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))
```

The parameters of the best model for the explicit task are displayed in @tbl-model_explicit_params_table. The significant parameters in the explicit task are the Color and the interaction between the Group and the Congruence conditions. The **main effect of Color** indicate that participants responded quicker in the colored condition (Marginal contrast (colored - uncolored) = `r c_ex_col[1,3]`, CI = $[`r c_ex_col[1,4]` ; `r c_ex_col[1,5]`]$, SE = `r c_ex_col[1,6]`, $p < .001$), although this effect has no interaction with the other variables. The **interaction between Group and Congruence** reveals that non-aphantasics responded slower in the uncongruent condition, as this contrast is the only significant one in this interaction (Marginal contrast (non-aphantasic congruent - incongruent) = `r c_ex_gcn[1,3]`, CI = $[`r c_ex_gcn[1,4]` ; `r c_ex_gcn[1,5]`]$, SE = `r c_ex_gcn[1,6]`, $p < .001$). These effects are illustrated in @fig-model_explicit_ggarrange. Both estimates are $\lt 0.2$, which denotes a *very small* effect size according to most standardized differences interpretation guidelines (e.g. Cohen, 1988; Gignac & Szodorai, 2016; Lovakov & Agadullina, 2021).

```{r model_explicit_params_table}
#| label: tbl-model_explicit_params_table
#| tbl-cap: "Parameters of the optimal model for the explicit task including a random intercept and slope by subject on Congruence."

all_models_parameters[["explicit_mods"]][["mod_explicit_4"]]
```

<!-- Plot settings -->
```{r model_plots_general_settings}

# setting dodge width for all the geoms
dw <- .75
# stat text size
st <- 7
# legend and axis text size
txt <- 22
```

<!-- GCN -->

<!-- Explicit model means and stats for congruence and group -->
```{r model_explicit_gcn_plot_stats}

# model explicit gcn marginal means predictions
preds_mod_4_g_cng <- 
  all_models_fitted[[1]][[4]] %>% 
  estimate_means(at = c("aphantasia", "congruence"))

# stats for the gcn interaction
stats_mod_4_g_cng_full <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = 3.1) %>% 
  filter(aphantasia == "no")

stats_mod_4_g_cng_zoom <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = c(.22, .3))
```

<!-- Explicit model full group and congruence plot -->
```{r model_explicit_gcn_full_plot}
p_mod_ex_gcn_full <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = dw,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_cng,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_cng,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_cng_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = 18)
    )
```

<!-- Explicit model zoomed group and congruence plot -->
```{r model_explicit_gcn_zoomed_plot}
p_mod_ex_gcn_zoom <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = dw,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_cng,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_cng,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_cng_zoom, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(
    # x = "Congruence condition", 
    x = NULL, 
    y = NULL
    ) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.text = element_text(size = 18),
    # axis.title.x = element_text(size = 18, vjust = -.5),
    legend.title = element_text(size = txt)
    )
```

<!-- GCOL -->

<!-- Explicit model means and stats for color and group -->
```{r model_explicit_gcol_plot_stats}
# model explicit gcol marginal means predictions
preds_mod_4_g_col <- 
  all_models_fitted[[1]][[4]] %>% 
  estimate_means(at = c("aphantasia", "color"))

# stats for the gcol interaction
stats_mod_4_g_col_full <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>%
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ color) %>% 
  add_xy_position(x = "color", dodge = dw) %>% 
  mutate(y.position = 3.1)

# stat for the color effect
stats_mod_4_g_col_zoom <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>%
  group_by(aphantasia) %>%
  pairwise_wilcox_test(rt_explicit ~ color) %>% 
  add_xy_position(x = "color", dodge = dw) %>% 
  mutate(y.position = .3)
```

<!-- Explicit model full group and color plot -->
```{r model_explicit_gcol_full_plot}
p_mod_ex_gcol_full <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = color,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = .8),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = .8,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_col,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_col,
    aes(
      x = color,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_col_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = 18)
    )
```

<!-- Explicit model zoomed group and color plot -->
```{r model_explicit_gcol_zoomed_plot}
p_mod_ex_gcol_zoom <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = color,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = .8),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = .8,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_col,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_col,
    aes(
      x = color,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_col_zoom,
    label = "p.adj.signif",
    tip.length = 0.005,
    size = st
    ) +
    
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(
    # x = "Color condition", 
    x = NULL,
    y = NULL
    ) +
  scale_x_discrete(labels = c("Colored", "Uncolored")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.text = element_text(size = 18),
    # axis.title.x = element_text(size = 18, vjust = -.5),
    legend.title = element_text(size = txt)
    )
```

<!-- GCN and GCOL -->

```{r model_explicit_ggarrange}
#| label: fig-model_explicit_ggarrange
#| fig-cap: "Visualization of the predictions of the optimal model for the explicit task. The bottom plots are 'zoomed' versions of the top ones on shorter (standard) RT ranges for clearer views of the model. The horizontal colored lines in the 'violins' (colored shapes outlining the distribution of responses) represent quantiles at 25, 50 and 75%. The black dots indicate marginal means predicted by the model whereas the black bars represent the credible intervals of these estimations. **Left:** The *Congruence* plots represent the estimates of the model at each Congruence condition and Group, highlighting the significant interaction between the two variables. **Right**: The *Color* plots represent the estimates at each Color condition and Group, highlighting the main effect of Color and non-significance of the interaction with the Group factor."
#| fig-width: 14
#| fig-height: 11

p_mod_explicit_ggarrange <-
  ggarrange(
    p_mod_ex_gcn_full,
    p_mod_ex_gcol_full,
    p_mod_ex_gcn_zoom,
    p_mod_ex_gcol_zoom,
    labels = c("Congruence", "Color"),
    label.x = c(-.1,.75),
    vjust = -.1,
    font.label = list(size = 26),
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    nrow = 2
  ) %>% 
  annotate_figure(
    left = text_grob("Response time (standardized)", size = txt, rot = 90)
    )

p_mod_explicit_ggarrange
```

## Implicit task results

```{r model_implicit_contrasts}

# model 3 contrasts between aphantasia and congruence levels
c_im_gcn <-
  all_models_fitted[[2]][[3]] %>% 
  estimate_contrasts(contrast = c("aphantasia", "congruence")) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))
```

The parameters of the best model for the **implicit task** are displayed in @tbl-model_implicit_params_table. The only significant parameter is the **interaction between Group and Congruence**. This interaction shows that non-aphantasics only responded slower in the incongruent condition, which is the only significant contrast (Marginal contrast (non-aphantasic congruent - incongruent) = `r c_im_gcn[1,3]`, CI = $[`r c_im_gcn[1,4]`;`r c_im_gcn[1,5]`]$, SE = `r c_im_gcn[1,6]`, $p = `r c_im_gcn[1,9]`$). This effect is represented in @fig-model_implicit_ggarrange. This estimate $\lt 0.2$, which denotes a *very small* effect size.

```{r model_implicit_params_table}
#| label: tbl-model_implicit_params_table
#| tbl-cap: "Implicit task  - Parameters of the optimal model including a random intercept by subject."

all_models_parameters[["implicit_mods"]][["mod_implicit_3"]]
```

<!-- Implicit model group and congruence means -->
```{r model_implicit_gcn_plot_stats}

# model 3 marginal means predictions
preds_mod_3_fixed_congruence <- 
  all_models_fitted[[2]][[3]] %>% 
  estimate_means(at = c("aphantasia", "congruence"))
```

<!-- Implicit model full group and congruence plot -->
```{r model_implicit_gcn_full_plot}
p_mod_im_gcn_full <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_implicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = dw,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_congruence,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_congruence,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank()
    )
```

<!-- Implicit model zoomed group and congruence plot -->
```{r model_implicit_gcn_zoomed_plot}
p_mod_im_gcn_interaction_zoom <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_implicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  geom_beeswarm(
    cex = .5,
    dodge.width = dw,
    alpha = 0.25,
    size = .5
  ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_congruence,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_congruence,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = 18)
    )
```

<!-- Implicit model group and congruence ggarrange -->
```{r model_implicit_ggarrange}
#| label: fig-model_implicit_ggarrange
#| fig-cap: "Visualization of the Congruence and Group variables in the optimal model for the implicit task. The bottom plot is a 'zoomed' version of the first one on a shorter (standard) RT range for a clearer view of the model. The horizontal colored lines in the 'violins' represent quantiles at 25, 50 and 75%. The black dots indicate marginal means predicted by the model, whereas the black bars represent the credible intervals of these estimations." 
#| fig-width: 8
#| fig-height: 10

p_mod_implicit_ggarrange <-
  ggarrange(
    p_mod_im_gcn_full,
    p_mod_im_gcn_interaction_zoom,
    common.legend = TRUE,
    ncol = 1
  ) %>% 
  annotate_figure(
    left = text_grob("Response time (standardized)", size = 18, rot = 90),
    bottom = text_grob("Congruence condition", size = 20)
    )

p_mod_implicit_ggarrange
```

# Further EDA {#sec-further}

:::{.callout-important}

#### Note for the team

I struggled to understand the variables used for clustering in the actual state of the manuscript. The clustering algorithms need continuous variables, and I only found three of them: *Visual imagery* (VVIQ, besides other redundant questionnaires), and two *Congruence effects* (transformed RT difference variables) respectively in the *Explicit* and *Implicit* tasks. On my end, I will also add a fourth variable, the *Spatial* component of the PCA, in the exploratory process. However, there is a mention in the manuscript of adding *Color* as a clustering variable: how was this possible? The question comes from the fact that Color is a sub-structure of the task, therefore adding Color would mean adding *four* Congruence variables instead of two (the Congruence effect in the Colored or Uncolored condition in the Explicit or Implicit task), along with an additional layer of complexity for the understanding of the meaning and relevance of the variables chosen (an essential aspect of unsupervised analysis).

I would very much like to read the full analysis through JASP inputs and outputs to be able to offer a more informed critique of the process.
:::

## The Congruence effect

As we have seen, besides the lonesome main effect of Color in the explicit task, the only significant effects identified by our best models are those of the interactions $Group \times Congruence$, revealing that non-aphantasics tend to respond slower in the incongruent trials in both tasks whereas aphantasics have a constant performance. We will call this effect the **Congruence effect**, and it appears to vary between the groups. Now it would be very interesting to investigate the Congruence effect as a *continuous variable that varies across individuals*, a phenomenon that is hidden in our *group-wise* (or sample-wise, population-wise) analysis by averaging and searching for the main effect. To analyse the Congruence effect, we would need to calculate it per subject first. Here we have two possible approaches:

- An **empirical approach**, by averaging the RTs per condition across all trials and subtracting Congruent mean RT minus Incongruent.

- A **model-based approach**.

### Empirical approach

The empirical approach would be an obvious manoeuvre if we only had the Congruence condition: this would be a simple transformation of an isolated within-subject predictor. Yet an added layer of complexity exists in our study, which lies in the *Color* condition. Manually averaging the RTs by Congruence level for each participant would involve blending in the two Color levels. This choice has assumptions that we can't verify empirically, such as assuming that the Congruence effect is comparable between Color levels, or assuming that the eventual variation between Color levels would be averaged in the process and that the resulting estimate would reflect the "true" isolated Congruence effect per subject. Betting on the veracity of these assumptions is adventurous at best, and we argue that a model-based approach would be more suited to evaluate the Congruence effect.

### Model-based approach

The model-based approach consists in using the multilevel models we fitted to isolate an estimate representing more accurately the Congruence effect per subject. The first step lies in the identification of what we are searching for in our model. We'll fetch the model described earlier that contains the most information about Congruence, Model 4. It is specified as such:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

What is the "Congruence effect per subject" here?

The whole $(\beta_{2} + \beta_{2_{subject[i]}})$ is the parameter describing the influence of the Congruence condition on the RT outcome. $\beta_{2}$ is the *sample-wise* parameter, that denotes the common effect of Congruence across the sample. Now $\beta_{2_{subject[i]}}$ is what we called the "*varying slope by subject*", i.e. the ***Congruence effect by subject!*** A model-based estimation would therefore consist in extracting this $\beta_{2_{subject[i]}}$ for each subject $i$, which would become our estimation of the effect - while, this time, accounting for the other parameters of the model, including Color. This $\beta_{2_{subject[i]}}$ will be detached from the global effect, so it will be centered on 0 (the "norm" for the Congruence effect).

The implementation of this calculation in R is shown below, and relies on the (aptly named) package `modelbased`.

<!-- Predicting the congruence effect for each subject -->
```{r search_for_congruence_effect}
#| echo: true
#| code-summary: "Computing the model-based Congruence effect per subject"

# first in the explicit task
congruence_effect_explicit <-
  
  # selecting the random slope on congruence by subject model
  all_models_fitted[["explicit_mods"]][["mod_explicit_4"]] %>% 
  
  # estimating the coefficent and intercept by subject
  estimate_grouplevel %>% 
  
  # keeping only the coefficient and reshaping like the original dataframe
  reshape_grouplevel(indices = "Coefficient") %>% 
  
  # "summary" reduces to one row per subject
  summary %>% 
  
  # keeping only the Coefficient column
  select(-Intercept) %>% 
  
  # renaming for clarity
  rename("congruence_effect_explicit" = congruenceuncongruent) %>% 
  
  # joining with the useful columns from the original data
  left_join(
    # subjectid, aphantasia, visual and spatial imagery
    df_questionnaires %>% select(1, 8:10),
    by = "subjectid"
  )

# completing with the implicit task
df_congruence_effect <-
  left_join(
    congruence_effect_explicit,
    
    # selecting the random slope on congruence by subject model
    all_models_fitted[["implicit_mods"]][["mod_implicit_4"]] %>% 
      # estimating the coefficent and intercept by subject
      estimate_grouplevel %>% 
      # keeping only the coefficient and reshaping like the original dataframe
      reshape_grouplevel(indices = "Coefficient") %>%
      # "summary" reduces to one row per subject
      summary %>% 
      # keeping only the Coefficient column
      select(-Intercept) %>% 
      # renaming for clarity
      rename("congruence_effect_implicit" = congruenceuncongruent),
    
    # adding the effect to each subject
    by = "subjectid"
    ) %>% 
  # reordering columns
  select(subjectid, aphantasia, 4, 5, 2, 6)
```

## Congruence effect and mental imagery

### Rationale and variables

Why investigate the Congruence effect and mental imagery? The hypothesis is that the Congruence effect reflects Visual imagery, and by extension that **it could be used to screen aphantasia**. The testing of this hypothesis requires to examine all the "within-participants" variables (i.e. cognitive features), that are, in this study, the mental imagery modalities. To that end, we are going to use the two **PCA components** we extracted from the questionnaire scores - the ***Visual imagery*** and ***Spatial imagery*** variables. The assumption behind the choice of using the PCA components instead of the raw questionnaire scores is that these components better reflect the cognitive features of the participants since (1) they are decorrelated and eliminated the covariance shared by the old scores, all the while (2) the Spatial component remains highly correlated with the OSIQ-S as does the Visual component with the VVIQ, OSIQ-O and SUIS, thus justifying our interpretation of these variables as distinct imagery modalities.

### Distributions

In sum, we kept four variables: the Congruence effect in the Explicit and Implicit task, and the Visual and Spatial imageries. The distributions of these four variables are displayed in @fig-congruence_effect_explicit, [-@fig-congruence_effect_implicit], [-@fig-visual_imagery], and [-@fig-spatial_imagery]. Note that the Congruence effect is **not** $(Congruent \ RT - Incongruent \ RT)$, but the $standardized \ parameter \ \beta_{2_{subject[i]}}$ from the model for the Congruence effect per subject: therefore, a ***positive Congruence effect denotes a higher response time in the incongruent condition*** (which is arguably more intuitive).

:::{.panel-tabset}

#### Explicit - Congruence

```{r congruence_effect_explicit_distribution}
#| label: fig-congruence_effect_explicit
#| fig-cap: "Distribution of the *Explicit task Congruence effect* in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = congruence_effect_explicit)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Congruence effect", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Implicit - Congruence

```{r congruence_effect_implicit_distribution}
#| label: fig-congruence_effect_implicit
#| fig-cap: "Distribution of the *Implicit task Congruence effect* in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = congruence_effect_implicit)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Congruence effect", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Visual imagery

```{r visual_imagery_distribution}
#| label: fig-visual_imagery
#| fig-cap: "Distribution of *Visual Imagery* in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = visual_imagery)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Visual Imagery", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Spatial imagery

```{r spatial_imagery_distribution}
#| label: fig-spatial_imagery
#| fig-cap: "Distribution of *Spatial Imagery* in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = spatial_imagery)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Spatial Imagery", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

:::

### Correlational analysis

We want to characterize the Congruence effect with imagery variables. Thus, we will study the correlations between the three variables: 1) Congruence effect, 2) visual and 3) spatial imagery, *separately* in the explicit and implicit task. Let's analyse this link with Bayesian partial correlations, displayed in @fig-cor_matrix_congruence_explicit and [-@fig-cor_matrix_congruence_implicit].

```{r correlation_congruence_effect}
#| code-summary: "Computing correlations"
#| echo: true

# --- Bayesian partial correlations ---
congruence_correlations_explicit <-
  df_congruence_effect %>%
  select(-congruence_effect_implicit) %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )

congruence_correlations_implicit <-
  df_congruence_effect %>%
  select(-congruence_effect_explicit) %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )

```

:::{.panel-tabset}

#### Explicit task

```{r cor_matrix_congruence_explicit}
#| label: fig-cor_matrix_congruence_explicit
#| fig-cap: "Correlation matrix with the Congruence effect in the explicit task. The stars indicate the amount of evidence in favour of a correlation, as assessed by the $BF_{10}$: No star = Anecdotal evidence, * = Weak evidence, ** = Moderate evidence, *** = Extreme evidence."

congruence_correlations_explicit %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "visual_imagery" ~ "Visual imagery",
    .x == "spatial_imagery" ~ "Spatial imagery",
    .x == "congruence_effect_explicit" ~ "Congruence effect",
    TRUE ~ .x))
    ) %>% 
  summary(digits = 2) %>% 
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )
```

#### Implicit task

```{r cor_matrix_congruence_implicit}
#| label: fig-cor_matrix_congruence_implicit
#| fig-cap: "Correlation matrix with the Congruence effect in the implicit task. The stars indicate the amount of evidence in favour of a correlation, as assessed by the $BF_{10}$: No star = Anecdotal evidence, * = Weak evidence, ** = Moderate evidence, *** = Extreme evidence."

congruence_correlations_implicit %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "visual_imagery" ~ "Visual imagery",
    .x == "spatial_imagery" ~ "Spatial imagery",
    .x == "congruence_effect_implicit" ~ "Congruence effect",
    TRUE ~ .x))
    ) %>% 
  summary(digits = 2) %>%
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )
```

:::

The Congruence effect does not seem to be correlated with visual or spatial imagery when these variables are taken as continua. We'll examine the relations from another point of view, by trying to model the Congruence effect with the two imagery variables.

### Modeling the Congruence effect

We will fit a simple model to investigate the effect:

$$
Congruence \ effect = \beta_{1} \cdot Visual \ imagery \times \beta_{2} \cdot Spatial \ imagery
$$

The parameters of this model for each task are shown in @tbl-model_congruence_effect-1 and @tbl-model_congruence_effect-2. As we could expect from the correlations, the estimates are negligible and far from significance.

```{r model_congruence_effect}
#| label: tbl-model_congruence_effect
#| tbl-cap: "Parameters of the models fitted on the Congruence effect in both tasks."
#| tbl-subcap: 
#|   - "Parameters of the **Explicit task** model."
#|   - "Parameters of the **Implicit task** model."

mod_cng_effect_explicit <-
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_explicit ~ visual_imagery * spatial_imagery,data = df_congruence_effect) %>% 
  extract_fit_engine

mod_cng_effect_implicit <-
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_implicit ~ visual_imagery * spatial_imagery,data = df_congruence_effect) %>% 
  extract_fit_engine

mod_cng_effect_explicit %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display

mod_cng_effect_implicit %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display

```

We can visualize these models interactively by plotting the  151 participants in a space defined by the three variables: such projections are presented in @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit]. The predictions of our model are represented as a plane that associates, for each Visual imagery value $x$ and Spatial imagery value $y$, a predicted value $z$ (the "*Predicted effect*"): the model's plane is the collection of those points (it is colored by a gradient to ease the reading of a high/low predicted Congruence effect).

```{r model_predictions_congruence_effect}

preds_explicit_congruence_effect <- 
  estimate_expectation(mod_cng_effect_explicit) %>% 
  select(1,3) %>%
  rename("predicted_explicit" = Predicted) %>% 
  left_join(
    df_congruence_effect,
    by = "visual_imagery"
  )

df_preds_congruence_effect <-
  estimate_expectation(mod_cng_effect_implicit) %>% 
  select(1,3) %>% 
  rename("predicted_implicit" = Predicted) %>% 
  left_join(
    preds_explicit_congruence_effect,
    by = "visual_imagery"
  ) %>% 
  select(4,5,1,6,7,8,3,2)
```

:::{.panel-tabset}

#### Explicit task model

```{r 3d_congruence_effect_explicit}
#| label: fig-3d_congruence_effect_explicit
#| fig-cap: "3D mapping of the three variables of interest in the **Explicit** task: Congruence effect, Visual, and Spatial imagery."

df_preds_congruence_effect %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_explicit
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    size = 6
    ) %>% 
  add_trace(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ predicted_explicit,
    type = "mesh3d",
    intensity = ~ predicted_explicit, 
    colorscale = "Viridis",
    colorbar = list(title = list(text = "Predicted effect")),
    hovertext = "z = Predicted effect"
    ) %>% 
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Implicit task model

```{r 3d_congruence_effect_implicit}
#| label: fig-3d_congruence_effect_implicit
#| fig-cap: "3D mapping of the three variables of interest in the **Implicit** task: Congruence effect, Visual and Spatial imagery."

df_preds_congruence_effect %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_implicit
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    size = 6
    ) %>% 
  add_trace(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ predicted_implicit,
    type = "mesh3d",
    intensity = ~ predicted_implicit, 
    colorscale = "Viridis",
    colorbar = list(title = list(text = "Predicted effect")),
    hovertext = "z = Predicted effect"
    ) %>% 
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

:::

Although the effects of the mental imagery variables are statistically anecdotal, this method of visualization shows that the model's planes are not flat, reflecting the interaction dynamics between visual and spatial imagery, and allows us to intuitively understand this interaction. Some interesting observations (you can rotate @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit] to visualize the effects described here):

- In the Explicit task:

    - The Congruence effect diminishes with spatial imagery *for high visual imagers*, but stays even across spatial imagery for *low visual imagers* (mostly aphantasics)
    
    - The Congruence effect augments with visual imagery for *low spatial imagers*, but diminishes with visual imagery for *high spatial imagers* (i.e. completely contrary dynamics depending on the spatial imagery abilities).
    
- In the Implicit task:

    - The Congruence effect diminishes with spatial imagery *for high visual imagers*, but stays even across spatial imagery for *low visual imagers / aphantasics* (i.e. the same dynamic as the Explicit task)
    
    - The Congruence effect augments with visual imagery for *low spatial imagers*, but stays even across visual imagery for *high spatial imagers* (i.e. comparable dynamics with the Explicit task, although not as extreme).

### Contradictory findings

Once again, while fairly obvious in visual data exploration, these interaction dynamics between variables have shown to be weak and statistically insignificant. This failure to explain continuous data is reminiscent of another earlier finding, that *Visual imagery is not correlated with Congruence effect*, although we clearly showed in @sec-inferential that the global effect of Congruence varied with the Group (i.e. visual imagery group) in both tasks. This reflects the high variability in the sample both in Congruence effect and mental imagery (clearly visible in @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit]), variance that is only partly explained by the division in two groups based on visual imagery (although this division already yields - very small - significant interaction effects). Said differently, ***the division by the VVIQ (aphantasics/phantasics) is a sufficient model statistically, yet is not satisfying to explain most of the cognitive differences at play in our tasks.***

As we can see that the initial model of a division in two VVIQ groups somehow managed to explain some aspects of the data, we can hypothesize that the partition of the sample in several groups has an underlying potential to explain the data. Therefore, one approach we could choose to tackle our problem is a redefinition of this division model, i.e. a **redefinition of the groups**. To that end, we propose to conduct an analysis independent of our initial hypotheses and models through the use of **unsupervised clustering**.

# Even deeper EDA: Unsupervised clustering

## Rationale

### Variables?

We mentioned that our initial groups were created based on the VVIQ, that is, **one variable**. We then showed that variance in our sample could come from several variables, limiting the explanatory power of this grouping. The clustering approach would consist in accounting for **all of these variables in the grouping**. This is still limited if we ponder about the innumerable reasons in the participants' cognition that could explain variance, but it is an analytic equivalent of "doing our best with what we have". But which variables do we have? Among the continuous within-subject variables, we isolated four, two mental imagery variables and two Congruence effects (one for each task). We could choose to use all four straight away, but that would mean ignoring an important methodological aspect of clustering procedures: as one could theoretically use an infinite number of variables for clustering, it is paramount that this choice be informed by the fact that all variables must be homogeneous in their physical meaning and equally important for classification. In operational terms, that means reducing covariance between variables as much as possible, often through *dimensionality reduction*, e.g., PCA. 

### Dimensionality reductionn, part 2

The Visual and Spatial imagery variables are already the result of a PCA, thus theoretically two distinct and equally meaningful variables. The question of dimensionality reduction comes from the two Congruence effects: do they represent the same "cognitive features"? To try to answer this question, we will run a second PCA on our four variables.

```{r pca_part_2}
#| echo: true
#| code-summary: "Computing a PCA, part 2"

# --- Principal component analysis on the four continuous variables ---
pca_2 <-
  principal_components(
    df_congruence_effect[,3:6],
    n = "max",
    sort = TRUE
  )
```

:::{.panel-tabset}

#### Loadings

```{r pca_2_loadings}
#| label: fig-loadings2
#| fig-cap: "Loadings of each variable on the three components extracted by a PCA."
#| fig-width: 8

# --- Loadings ---
pca_2 %>% 
  plot + 
  scale_y_discrete(labels = c("Visual imagery", "Congruence explicit", "Spatial imagery", "Congruence implicit")) +
  scale_fill_viridis(alpha = .6) +
  labs(title = NULL) +
  theme(text = element_text(size = 14))
```

#### Eigenvalues

```{r pca_2_eigenvalues}
#| label: tbl-eigenvalues2
#| tbl-cap: "Eigenvalues and variance explained by the three components extracted by a PCA."

# --- Eigenvalues and variance ---
pca_2 %>% 
  summary %>% 
  rename(
    "Component 1" = PC1,
    "Component 2" = PC2,
    "Component 3" = PC3,
  ) %>%
  format(digits = 2) %>%
  display

df_congruence_effect <- 
  df_congruence_effect %>% 
  mutate(across(3:6, as.numeric))

df_clustering <-
  left_join(
    df_congruence_effect[,1:3], 
    rotated_data(pca_2),  
    by = "visual_imagery")

```

#### Component space

```{r pca_2_component_space}
#| label: fig-3d_component_space
#| fig-cap: "3D mapping of the three components extracted by the PCA."

df_clustering %>% 
  plot_ly(
    x = ~ PC1,
    y = ~ PC2,
    z = ~ PC3
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    size = 6
    ) %>%
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Component 1"),
      yaxis = list(title = "Component 2"),
      zaxis = list(title = "Component 3")
      )
    )
```

:::

























































