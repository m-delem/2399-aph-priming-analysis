---
title: "Sensory priming in aphantasia - Data analysis report"
author: "Maël Delem"
date: 2023-09-13
date-modified: last-modified
date-format: "D/MM/YYYY"
format: html
execute: 
  echo: false
editor_options: 
  chunk_output_type: console
toc: true
toc-depth: 3
toc-expand: 2
number-sections: true
number-depth: 2
fig-width: 6
fig-asp: .618
---

<!-- CSS options for custom title numbers styling-->
<style>
.header-section-number:after {
  content: ". ";
}
</style>

### Preliminary Set-up {.unnumbered}

::: {.callout-note collapse="true"}

#### Datasets and code availability

The folder containing this HTML file also has a "`data/`" folder, wherein can be found the main Excel file from which the datasets analysed here are sourced, i.e. "`aphantasia_priming_tidy_data.xlsx`". It has several sheets:

- `data_asso`/`_implicit`/`_explicit`/`_rotation`/`_questionnaires` are raw datasheets retrieved from old Excels and cleaned up

- `data_per_subject` is a table containing one row per subject with all the associated information, raw or computed:

    - visual_/spatial_imagery are the two PCA components calculated in [the analysis of questionnaire data](#sec-pca).
    - congruence_effect_explicit/_implicit are the "*Congruence effects per subject*", i.e. the slope by-subject on Congruence extracted from the multilevel (mixed) models of the [inferential analysis](#sec-inferential).
    - PC1/2/3 are the three PCA components calculated before the clustering procedure in @sec-pca2.
    - clusters_2/_4 are the two clustering solutions, respectively with 2 and 4 clusters.
    - explicit/implicit_mean/mad/min/max are the summary statistics of the two main tasks after data tidying and transformation.

The code generating all the computations, figures, tables, etc. (only partially exposed here for clarity) can be found in the `aphantasia_priming_analysis.qmd` file, for which the best reading software are coding Integrated Development Environments (IDE) like [RStudio](https://posit.co/download/rstudio-desktop/) or [Visual Studio Code](https://code.visualstudio.com/). The file includes detailed commentaries along with the code to ease understanding, accessibility, and potential (welcome) criticism.

:::

:::{.callout-note collapse="true"}

#### Software and setup

This analysis was conducted in R language on [RStudio](https://posit.co/download/rstudio-desktop/). This analysis report was written with [Quarto](https://quarto.org/). Just down below is the set-up code, including two essential steps: 

1. Installing the required packages for this data analysis

2. importing data into the dataframes used throughout. 

I left this (usually hidden) step here for reference of the tools used in a view of transparency for the interested reader.

```{r setup}
#| output: false
#| echo: true
#| code-summary: "Packages"

# ═══ Packages ═════════════════════════════════════════════════════════════════

# The package `librairian` will ease the package management with the "shelf" 
# function, which automatically: 
# 1) checks if a package is installed 
# 2) installs it if need be
# 3) loads the package like the "library()" function would.
if (!require(librarian)) install.packages(librarian)
library(librarian)

# now putting packages on our library's shelves:
shelf(
  # ─── essential package collections ───
  tidyverse,      # modern R ecosystem
  easystats,      # data analysis framework
  tidymodels,     # modelling framework
  
  # ─── tidymodels friends ──────────────
  corrr,          # correlational analyses
  tidybayes,      # bayesian inference
  multilevelmod,  # multilevel modelling with lmer and tidymodels
  
  # ─── modelling ───────────────────────
  lme4,           # mixed models
  mclust,         # mixture clustering
  rstanarm,       # bayesian models
  BayesFactor,    # BFs
  
  # ─── data management ─────────────────
  readxl,         # importing xlsx
  openxlsx,       # exporting xlsx
  
  #  data visualization ──────────────
  # plot types and geoms
  ricardo-bion/ggradar,  # radar plots
  ggbeeswarm,            # scatter violin plots
  GGally,         # complex plots
  # layout and options
  ggpubr,         # publication plots
  patchwork,      # layout control
  rstatix,        # ggplot stat tools
  # palettes
  ggsci,          # scientific palettes
  viridis,        # colour-blind friendly palettes
  # interactive
  plotly         # interactive plots
)

# ─── Global cosmetic theme ───
theme_set(theme_bw(base_size = 14))

# ─── Fixing a seed for reproducibility ───
set.seed(14051998)
```

```{r importing_data}
#| echo: true
#| code-summary: "Importing data"

# Association task
df_asso <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_asso")

# Implicit task
df_implicit <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_implicit")

# Explicit task
df_explicit <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_explicit")

# Rotation task
df_rotation <- read_excel("data/aphantasia_priming_tidy_data.xlsx", 
                     sheet = "data_rotation")

# Questionnaires
df_questionnaires <- 
  read_excel(
    "data/aphantasia_priming_tidy_data.xlsx",
    sheet = "data_questionnaires"
    ) %>% 
  # creating the a/phantasia groups
  mutate(
    aphantasia = ifelse(vviq80 < 32, "yes", "no"),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    )
```

:::

:::{.callout-tip appearance="simple"}

#### Interactive figures

Many figures in this report are interactive: hover over the plots to see some of the tools available. You can select a zone to zoom on a plot, hover over bars to see details about data, select only specific groups in the legend, among many other features.

:::

:::{.callout-caution}

#### TL: DR

This report is long. While hoping to have made it organized and user-friendly enough to make people want to read it, here are a few key findings to shorten the process if necessary:

1. Mixed models fitted on RTs showed a **significant interaction between Group (Aphantasics/Phantasics) and Congruence**, in both Explicit and Implicit tasks, although **very small in effect size** in either case. Non-aphantasics answer slightly slower in the incongruent condition, which is not the case for aphantasics.

2. This "**Congruence effect**", when considered as continuous and computed *per subject* as the random parameter for Congruence in the mixed model (i.e. the slope by subject on Congruence), **did not correlate with mental imagery variables**. This contradictory finding might be related to very small effect sizes.

3. A **Gaussian Mixture Model clustering** with 4 clusters, computed with the two congruence effects, visual and spatial imagery variables, revealed an interesting structure in the data, linking **visual *and* spatial imagery** to the congruence effects. While the cluster differences were negligible in the implicit task, the four clusters did manage to explain some variability in the explicit task. A specific group of non-aphantasics with **high visual imagery and low spatial imagery** showed especially high congruence effect, and could be interpreted as driving the effects analysed in (1).

The rest of the report is describing the methods and models, justifying the choices, transformations and assumptions, and visualizing the analyses.
:::

# Exploratory Data Analysis (EDA) {#sec-eda}

## Questionnaire data and imagery

### Correlations

The four questionnaire variables - VVIQ, OSIQ-Object and Spatial, and SUIS - are meant to be used as predictors (independent variables) for the behavioural outcomes of the experiment (dependent variables, RTs and potentially accuracy). Three of these - VVIQ, OSIQ-Object and SUIS - evaluate visual imagery: as such, in a view of parsimony, we are going to evaluate the correlation between these continuous predictors and judge if they should be merged/omitted. The results of the Bayesian partial correlations between the four variables are displayed in @fig-cor_matrix and @tbl-cor_stats. Partial correlations have been chosen here to account for the (theoretically huge) covariance between visual imagery variables. It will result in more conservative estimations of the correlations, and a better understanding of the relations between visual and spatial imagery measures.

```{r correlation}
#| code-summary: "Computing correlations"
#| echo: true

# --- Bayesian partial correlations between questionnaire scores ---
cor_questionnaires <-
  df_questionnaires %>%
  select("vviq80", "suis60","osiq_o75", "osiq_s75") %>% 
  standardise %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )
```

:::{.panel-tabset .column-page-inset-right}

#### Correlation matrix

```{r correlation_matrix}
#| label: fig-cor_matrix
#| fig-cap: "Correlation matrix of the questionnaire variables. The stars indicate the amount of evidence in favour of a correlation, as assessed by the $BF_{10}$: No star = Anecdotal evidence, * = Weak evidence, ** = Moderate evidence, *** = Extreme evidence."

# --- Correlation matrix ---
cor_questionnaires %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "vviq80" ~ "VVIQ",
    .x == "suis60" ~ "SUIS",
    .x == "osiq_o75" ~ "OSIQ-O",
    .x == "osiq_s75" ~ "OSIQ-S"))
    ) %>% 
  summary(digits = 2) %>% 
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )
```

#### Correlation table

```{r correlation_table}
#| label: tbl-cor_stats
#| tbl-cap: "Detailed correlation table of the questionnaire variables."

# --- Correlation table ---
cor_questionnaires %>%
  format %>% 
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "vviq80" ~ "VVIQ",
    .x == "suis60" ~ "SUIS",
    .x == "osiq_o75" ~ "OSIQ-O",
    .x == "osiq_s75" ~ "OSIQ-S"))
    ) %>% 
  rename(
    "Variable 1" = Parameter1,
    "Variable 2" = Parameter2,
    "$\\rho$" = rho,
    "$BF_{10}$" = BF
  ) %>% 
  knitr::kable()
```

:::

As we can see, while the three visual imagery variables are highly correlated, the association between these visual variables and spatial imagery (OSIQ-S) is inconsistent: VVIQ and OSIQ-S are positively correlated although with anecdotal evidence, SUIS and OSIQ-S have a moderately evidenced positive correlation, while OSIQ-Object and OSIQ-Spatial are negatively correlated. We'll run a Principal Component Analysis to evaluate the possibility of a dimensionality reduction by considering visual imagery as a single variable.

### Principal Component Analysis {#sec-pca}

```{r pca}
#| code-summary: "Computing the PCA"
#| echo: true

# --- Principal Component Analysis ---
pca <- 
  principal_components(
    df_questionnaires[,4:7],
    n = "max",
    sort = TRUE,
    standardize = TRUE
    ) 
```

:::{layout-ncol=1}

```{r loadings}
#| label: fig-loadings
#| fig-cap: "Loadings of each variable on the three components extracted by a PCA."
#| fig-width: 8

# --- Loadings ---
pca %>% plot + 
  scale_y_discrete(labels = c("OSIQ-S", "OSIQ-O", "VVIQ", "SUIS")) +
  scale_fill_viridis(alpha = .6) +
  labs(title = NULL) +
  theme(text = element_text(size = 14))
```

```{r eigenvalues}
#| label: tbl-eigenvalues
#| tbl-cap: "Eigenvalues and variance explained by the three components extracted by a PCA."

# --- Eigenvalues and variance ---
pca %>% 
  summary %>% 
  pivot_longer(cols = -Parameter) %>%
  pivot_wider(names_from = Parameter) %>%
  rename("Component" = name) %>%
  display
```

:::

As expected, the visual imagery variables are grouped in a single component ("PC1" in @fig-loadings or "Component 1" in @tbl-eigenvalues) with an Eigenvalue of 2.84, therefore accounting for 71% of the total variance of the data. The second component (PC2 / Component 2) has an Eigenvalue of .97, accounts for 24% of the total variance, and is mostly loaded negatively on the OSIQ-S: this brings the total explained proportion of variance to 95%. Traditionally, the threshold used for selecting a principal component is an Eigenvalue of 1. Nevertheless, given the Eigenvalue of .97 and the properties of this component, i.e. singling out the specificity of the spatial scale, this component will be kept for analysis.

Standardized predicted values for each component will be computed and added to the data, renamed ***Visual Imagery*** (PC1, mostly VVIQ + SUIS + OSIQ-O with an influence of OSIQ-S) and ***Spatial Imagery*** (PC2, mostly OSIQ-S, with an influence of SUIS). In order to align the Spatial Imagery variable with the OSIQ-Spatial, the variable will be reversed (they are correlated negatively as it is, as shown by the -.96 loading on PC2).

```{r pca_components}
#| code-summary: "Adding the predicted PCA components to the data"
#| echo: true

pca_components <- pca %>% predict

df_questionnaires <- 
  bind_cols(df_questionnaires, pca_components[,1:2]) %>% 
  mutate(
    PC1 = standardize(PC1),
    PC2 = -standardize(PC2)
    ) %>% 
  rename(
    "visual_imagery" = PC1,
    "spatial_imagery" = PC2
    )

rm(pca, pca_components)
```

### VVIQ, visual and spatial imagery

Now we can see that the main visual imagery variables, namely the (conventional) VVIQ and the *Visual Imagery* component created through PCA are completely decorrelated from the *Spatial Imagery* component, as shown in @fig-spatial_corr1 and [-@fig-spatial_corr2].

:::{.panel-tabset .column-page-inset-right}

#### Spatial and Visual Imagery

```{r spatial_correlations_plotly_1}
#| label: fig-spatial_corr1
#| fig-cap: "Correlation between the visual imagery PCA component and the spatial imagery one."
#| out-width: 100%

# --- correlation between visual and spatial ---
(df_questionnaires %>% 
  mutate(
    visual_imagery = rescale(visual_imagery),
    spatial_imagery = rescale(spatial_imagery),
    aphantasia = fct_relevel(aphantasia, "yes", "no")
    ) %>%
  ggplot(aes(x = visual_imagery, y = spatial_imagery)) +
  geom_point(aes(color = aphantasia)) +
  scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  geom_smooth(
    aes(x = visual_imagery, y = spatial_imagery), 
    color = "black", 
    method = "lm", 
    stat = "smooth") +
  # stat_cor(
  #   aes(x = visual_imagery,
  #       y = spatial_imagery),
  #   inherit.aes = FALSE,
  #   cor.coef.name = "r",
  #   label.x.npc = .5,
  #   label.y.npc = .7) +
  labs(x = "Visual Imagery",
       y = "Spatial Imagery")) %>% 
  ggplotly
```

#### Spatial imagery and VVIQ

```{r spatial_correlations_plotly_2}
#| label: fig-spatial_corr2
#| fig-cap: "Correlation between the VVIQ and the spatial imagery PCA component."
#| out-width: 100%

# --- correlation between VVIQ and spatial ---
(df_questionnaires %>% 
  mutate(
    vviq80 = rescale(vviq80),
    spatial_imagery = rescale(spatial_imagery)
    ) %>%
  ggplot(aes(x = vviq80, y = spatial_imagery)) +
  geom_point(aes(color = aphantasia)) +
  scale_color_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  geom_smooth(
    aes(x = vviq80, y = spatial_imagery), 
    color = "black", 
    method = "lm", 
    stat = "smooth") +
  # stat_cor(
  #   aes(x = vviq80,
  #       y = spatial_imagery),
  #   inherit.aes = FALSE,
  #   cor.coef.name = "r",
  #   label.x.npc = .5,
  #   label.y.npc = .7) +
  labs(x = "VVIQ",
       y = "Spatial Imagery")) %>% 
  ggplotly
```

:::

The consequence of this analysis is that from this point onwards two options are available to model the data:

1. Using **Aphantasia (i.e. *****Group*****) as a categorical variable** by binning the participants in two categories based on the VVIQ (cut-off at 32, Aphantasics/Phantasics).

2. Using **Visual Imagery as a continuous variable.**

In the main inferential analyses conducted down below, the first option has been selected for its ease of interpretation (and closeness to the results that would be obtained with the continuous variable anyway). Nevertheless, the two continuous variables of visual and spatial imagery defined here will be explored later in [further data exploration](#sec-further).

```{r df_updates_with_imagery}

# updating all the dataframes with the imagery variables
dfs <- 
  list(
    association_task = df_asso, 
    implicit_task = df_implicit, 
    explicit_task = df_explicit, 
    rotation_task = df_rotation
    ) %>%
  # adding the vviq and group column to every dataframe
  imap(~left_join(
    .x, 
    df_questionnaires %>% 
      select(
        # column to match with the correct participants
        subjectid, 
        # new variables to add
        age,
        sexe,
        vviq80, 
        visual_imagery, 
        spatial_imagery,
        aphantasia), by = "subjectid") %>% 
      rename("sex" = sexe) %>% 
      # reordering the final dfs
      select(
        subjectid, 
        age, sex, aphantasia,
        vviq80, visual_imagery, spatial_imagery,
        everything())
    )

df_asso     <- dfs$association_task
df_implicit <- dfs$implicit_task
df_explicit <- dfs$explicit_task
df_rotation <- dfs$rotation_task
rm(dfs)
```

## Outcomes: accuracy and RTs?

### Accuracy

While response time is an obvious outcome to model when it comes to decision tasks, the use of accuracy is more debatable, as the tasks were not specifically designed to be challenging. Traditionally, accuracy is mostly used to remove incorrect trials from RT modelling. We'll describe accuracy data to evaluate its relevance in this analysis. The distribution of the percentages of errors across the three tasks is displayed in @fig-tasks_errors, [-@fig-tasks_errors2], and [-@fig-tasks_errors3].

:::{.panel-tabset .column-page-inset-right}

#### Association task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_asso}
#| echo: false
#| label: fig-tasks_errors
#| fig-cap: "Distribution of errors between groups in the association task."
#| out-width: 100%
#| fig-height: 4

(df_asso %>%
  select(subjectid, correct_association, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_association) %>% 
  filter(correct_association == 1) %>% 
  mutate(n = 100 - n) %>% 
  ggplot(aes(
    x = n,
    fill = aphantasia,
    
    )) +
  geom_bar(
    aes(
    y = after_stat(prop), 
    color = aphantasia),
    position = position_dodge(),
    width = .8,
    alpha = .5
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(0.5, 24),
    ylim = c(.01,.36)
    ) +
  # labs(title = "Association task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # ggplotly-ing it
  ggplotly
```

#### Explicit task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_explicit}
#| echo: false
#| label: fig-tasks_errors2
#| fig-cap: "Distribution of errors between groups in the explicit task."
#| out-width: 100%
#| fig-height: 4


(df_explicit %>%
  select(subjectid, correct_explicit, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_explicit) %>% 
  filter(correct_explicit == 1) %>% 
  mutate(n = (64 - n)/64 * 100) %>% 
  ggplot(aes(
    x = n, 
    color = aphantasia,
    fill = aphantasia
    )) +
  geom_bar(
    aes(y = after_stat(prop)),
    position = position_dodge(),
    width = .6,
    alpha = .6
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(1, 46),
    ylim = c(.01,.36)
    ) +
  # labs(title = "Explicit task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # plotly-ing it
  ggplotly
```

#### Implicit task
<!-- Percentage of errors -->

```{r tasks_errors_plotly_implicit}
#| echo: false
#| label: fig-tasks_errors3
#| fig-cap: "Distribution of errors between groups in the implicit task."
#| out-width: 100%
#| fig-height: 4

(df_implicit %>%
  select(subjectid, correct_implicit, aphantasia) %>% 
  group_by(subjectid, aphantasia) %>% 
  count(correct_implicit) %>% 
  filter(correct_implicit == 1) %>% 
  mutate(n = (64 - n)/64 * 100) %>% 
  ggplot(aes(
    x = n, 
    color = aphantasia,
    fill = aphantasia
    )) +
  geom_bar(
    aes(y = after_stat(prop)),
    position = position_dodge(),
    width = .6,
    alpha = .6
    ) +
  scale_x_continuous(
    name = "Percentage of errors",
    breaks = breaks_pretty(n = 20)
    ) +
  scale_y_continuous(
    name = "Proportion of the group",
    breaks = breaks_pretty(n = 10)
    ) +
  coord_cartesian(
    xlim = c(1, 40),
    ylim = c(.01,.28)
    ) +
  # labs(title = "Implicit task") +
  scale_color_okabeito(guide = NULL) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No"))) %>% 
  # plotly-ing it
  ggplotly
```

:::

We can see that the distributions are skewed yet very similar across groups and tasks: as such, we are going to use accuracy only to remove missed trials in the analyses of the RTs. Nonetheless, the main noticeable aspect of these plots are the obvious outliers in each of the tasks. This information is crucial as it could reflect a wrong understanding of the task, or random responses: in any case, participants that have high error rates would require a deletion of up to 47% of their trials, which could become a bias in the modelling of the overall data. As there appear to be a cut-off of the main distribution around 16% of errors in all tasks, this value will be chosen as threshold. **For each task separately, the results of participants with an error rate superior to 16% will be removed ***(i.e., five participants in the explicit task and four in the implicit task)***, prior to removing all the missed trials of the remaining participants for modelling RTs.**

<!-- Finding and removing accuracy outliers -->
```{r acc_outliers}
#| code-summary: "Removing incorrect trials and task-wise accuracy outliers"
#| output: false

# --- Accuracy outliers analysis ---
list(df_explicit, df_implicit) %>%
  imap(
    ~.x %>%
      select(subjectid, starts_with("correct"), aphantasia) %>%
      group_by(subjectid) %>%
      count(pick(2)) %>%
      filter(pick(1) == 1) %>%
      ungroup() %>%
      mutate(
        n_tot = max(n),
        prop = (n_tot - n) / n_tot * 100  # analysing the error rate per subject
      ) %>%
      arrange(desc(prop)) %>%
      select(1, 5)
    )
# 5 accuracy outliers in the explicit task, 4 in the implicit one

# --- Removing incorrect trials and task-wise accuracy outliers ---
df_explicit_rt <-
  df_explicit %>%
  # filtering out...
  filter(
    # incorrect trials
    correct_explicit == 1 &
    # participants identified with with high error rates
    !(subjectid %in% c( 
      "aknezevic", 
      "lbrunie",  
      "agayou", 
      "bluciani",
      "ldossantos"))
    ) %>% 
  # removing irrelevant variables
  select(-c(sex, vviq80, orientation, response, correct_explicit))

# percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit) 
# 9091 trials left, 573 = 5.9% removed

df_implicit_rt <- 
  df_implicit %>% 
  filter(
    correct_implicit == 1 &
    !(subjectid %in% c(
      "bdispaux", 
      "eleveque", 
      "aleclaire", 
      "dchimenton"))
      ) %>% 
  select(-c(sex, vviq80, orientation, response, correct_implicit))

# percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit) 
# 9078 left, 586 = 6% removed

```

### Response times

As a preprocessing, response times below 300ms and above 3000ms will be removed as they could represent false alarms. The removal of accuracy outliers, incorrect trials and extreme RTs amounted to 6.9% of trials in the explicit and 6.7 in the implicit task. There could also be outliers with overall abnormally fast or slow RTs. This analysis will be conducted on mean RTs, the distribution of which is displayed in @fig-rt_means1 and [-@fig-rt_means2], along with the overall RT statistics in @tbl-rt_means. 

<!-- Removing extreme RT outlier trials -->
```{r removing_first_rt_outliers}
#| code-summary: "Removing extreme RTs"
#| echo: true
#| output: false

# --- First broad RT outlier trials removal ---
df_explicit_rt <- 
  df_explicit_rt %>% 
  # filtering out extreme RTs
  filter(rt_explicit > 300 & rt_explicit < 3000)

# total percentage of trials removed in the explicit task
(count(df_explicit) - count(df_explicit_rt)) / count(df_explicit) 
# 9000 trials left, 664 = 6.9% removed

df_implicit_rt <- 
  df_implicit_rt %>% 
  filter(rt_implicit > 300 & rt_implicit < 3000)

# total percentage of trials removed in the implicit task
(count(df_implicit) - count(df_implicit_rt)) / count(df_implicit) 
# 9017 left, 647 = 6.7% removed
```

:::{.panel-tabset .column-page-inset-right}

#### Explicit task
<!-- RT means -->

```{r explicit_task_rt_means_plotly}
#| label: fig-rt_means1
#| fig-cap: "Distribution of the mean RTs of participants in the explicit task (very high RT means have been cut for better readability)."
#| fig-height: 4

(df_explicit_rt %>% 
  group_by(subjectid) %>% 
  summarise(mean = mean(rt_explicit)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(
    fill = "aquamarine2", 
    color = "aquamarine4", 
    alpha = .3,
    bins = 70
    # adjust = .4
    ) +
  labs(x = "Mean RT", y = "Number of participants") +
  scale_y_continuous(breaks = seq(0, 13, by = 1))) %>% 
  ggplotly
```

#### Implicit task
<!-- RT means -->

```{r implicit_task_rt_means_plotly}
#| label: fig-rt_means2
#| fig-cap: "Distribution of the mean RTs of participants in the implicit task (very high RT means have been cut for better readability)."
#| fig-height: 4

(df_implicit_rt %>%
  group_by(subjectid) %>% 
  summarise(mean = mean(rt_implicit)) %>% 
  ggplot(aes(x = mean)) +
  geom_histogram(
    fill = "coral2", 
    color = "coral4", 
    alpha = .3,
    bins = 70
    # adjust = .4
    ) +
  labs(x = "Mean RT", y = "Number of participants") +
  scale_y_continuous(breaks = seq(0, 9, by = 1))) %>% 
  ggplotly
```

#### RT means table
<!-- RT means table -->

```{r rt_means_table}
#| label: tbl-rt_means
#| tbl-cap: "Descriptive statistics of the average RTs across the sample in both tasks."

df_rt <-
  list(
  df_explicit_rt %>% rename("Explicit task mean RTs" = rt_explicit), 
  df_implicit_rt %>% rename("Implicit task mean RTs" = rt_implicit)
  ) %>% 
  imap(
    ~select(.x, contains("RT")) %>%
      report() %>% 
      as.data.frame() %>% 
      select(1, 2, 5:10)
    )

df_rt <- bind_rows(df_rt[[1]], df_rt[[2]])
df_rt %>% display

```

:::

For a more precise analysis of mean RTs, we are going to use an outlier detection method based on the Median Absolute Deviation (MAD). Common thresholds used for outlier detection in Gaussian distributions with MADs are $median \ \pm \ 3 \cdot MAD$ (Hampel filter; see Enderlein, 1987). However, the low boundary of this interval would be aberrant in our case of a skewed distribution, as all the participants with fast RT means even stay above the $median \ - \ 2 \cdot MAD$. As for the high threshold, the choice of the $median \ + \ 3 \cdot MAD$ would set a highly conservative maximum (e.g. 1385ms for the explicit task) and bring the total number of removed trials over 14%. Therefore, after careful examination of the ranges, and in order to keep as many trials as possible, we chose to set the upper threshold at $median \ + \ 5 \cdot MAD$ (i.e. 1797ms for the explicit task, and 1386ms for the implicit task), resulting in an overall deletion of 7.9% of trials in the explicit task and 8.9% of trials in the implicit one in this whole data preprocessing. The resulting distributions of RTs for each participant are represented in @fig-rt_per_participant_1 and [-@fig-rt_per_participant_2]. As we are going to transform the data before modelling it, we also visualized the resulting distributions after having undergone a Box-Cox transformation and a normalization.

<!-- Removing second step RT outlier trials -->
```{r removing_second_rt_outliers}
#| code-summary: "Removing individual outlier trials"
#| output: false

# 2.5 low threshold for both
# For upper threshold:
# Explicit task:
# 3 MADs threshold = 8572 trials --> 11.3% trials out
# 5 MADs threshold = 8835 trials --> 8.6% trials out
# 6 MADs threshold = 8903 trials --> 7.9% trials out --> sensible compromise
# 7 MADs threshold = 8938 trials --> 7.5% trials out

df_explicit_rt <- 
  df_explicit_rt |> 
  group_by(subjectid) |> 
  mutate(
    rt_mean = mean(rt_explicit),
    rt_mad = mad(rt_explicit)
    ) |> 
  filter(
    rt_explicit < rt_mean + 6 * rt_mad &
    rt_explicit > rt_mean - 2.5 * rt_mad
  ) |>
  ungroup()

# For upper threshold:
# Implicit task:
# 3 MADs threshold = 8455 trials --> 12.5% trials out
# 5 MADs threshold = 8730 trials --> 9.7% trials out
# 6 MADs threshold = 8801 trials --> 8.9% trials out 
# 7 MADs threshold = 8850 trials --> 8.4% trials out 
# 8 MADs threshold = 8889 trials --> 8% trials out --> best retention

df_implicit_rt <-
  df_implicit_rt |> 
  group_by(subjectid) |> 
  mutate(
    rt_mean = mean(rt_implicit),
    rt_mad = mad(rt_implicit)
    ) |> 
  filter(
    rt_implicit < rt_mean + 8 * rt_mad &
    rt_implicit > rt_mean - 2.5 * rt_mad
  ) |>
  ungroup()

# df_explicit_rt |>
#   group_by(subjectid) |>
#   count() # 146 participants left
#   # get_summary_stats(rt_explicit) |> 
#   # select(subjectid, n, mean, median, min, max) |> 
#   # arrange(desc(mean))
# 
# df_implicit_rt |>
#   group_by(subjectid) |>
#   count() # 147 participants left
```


<!-- Maybe add "For a similar procedure, see Lefevre et al. (2022), or what she cites, Leys et al. (2013)" -->

:::{.panel-tabset .column-page-inset-right}

#### Explicit task
<!-- RT distribution for each participant -->

```{r explicit_rt_per_participant_plotly}
#| label: fig-rt_per_participant_1
#| fig-cap: "Resulting distribution of RTs in the explicit task for each participant. Each colored distribution represents a single participant. *Hover to see details on the individual distributions*."
#| fig-height: 4

(df_explicit_rt %>% 
  ggplot(aes(x = rt_explicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(10)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  # coord_cartesian(
  #   xlim = c(380, 1700),
  #   ylim = c(0.0005, .01)
  # ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

#### Implicit task
<!-- RT distribution for each participant -->

```{r implicit_rt_per_participant_plotly}
#| label: fig-rt_per_participant_2
#| fig-cap: "Resulting distribution of RTs in the implicit task for each participant. Each colored distribution represents a single participant. *Hover to see details on the individual distributions*."
#| fig-height: 4

(df_implicit_rt %>% 
  ggplot(aes(x = rt_implicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(5)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  # coord_cartesian(
  #   xlim = c(350, 1320),
  #   ylim = c(0.0005, .01)
  # ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

```{r recipes}
# --- preprocessing recipe for explicit data ---
recipe_explicit <- 
  df_explicit_rt %>%
  recipe %>%
  update_role(
    rt_explicit, 
    new_role = "outcome"
    ) %>%
  update_role(
    subjectid, age,
    aphantasia, visual_imagery, spatial_imagery,
    color, congruence,
    new_role = "predictor"
    ) %>%
  add_role(
    subjectid, 
    new_role = "group"
    ) %>%
  step_BoxCox(rt_explicit) %>% 
  step_normalize(rt_explicit, age)

# --- preprocessing recipe for implicit data ---
recipe_implicit <- 
  df_implicit_rt %>%
  recipe %>%
  update_role(
    rt_implicit, 
    new_role = "outcome"
    ) %>%
  update_role(
    subjectid, age,
    aphantasia, visual_imagery, spatial_imagery,
    color, congruence,
    new_role = "predictor"
    ) %>%
  add_role(
    subjectid,
    new_role = "group"
    ) %>%
  step_BoxCox(rt_implicit) %>% 
  step_normalize(rt_implicit, age)
```

#### Box-Cox - Explicit

```{r box_cox_explicit}
(recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>% 
  ggplot(aes(x = rt_explicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(10)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  # coord_cartesian(
  #   xlim = c(380, 1700),
  #   ylim = c(0.0005, .01)
  # ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

#### Box-Cox - Implicit

```{r box_cox_implicit}
(recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>% 
  ggplot(aes(x = rt_implicit, fill = subjectid)) +
  geom_density(alpha = .25, color = "black") +
  scale_x_continuous(name = "Response time", breaks = breaks_pretty(10)) +
  scale_y_continuous(name = "Density") +
  theme(
    axis.text.y = element_blank(),
    axis.ticks.y = element_blank()
    ) +
  # coord_cartesian(
  #   xlim = c(380, 1700),
  #   ylim = c(0.0005, .01)
  # ) +
  scale_fill_viridis_d(guide = NULL) +
  scale_color_viridis_d(guide = NULL)) %>% 
  ggplotly %>% 
  style(showlegend = FALSE)
```

:::

# Inferential Analyses {#sec-inferential}

## Model description (Multilevel modelling)

### Predictors and outcome

Our analyses aim to evaluate the predictive abilities of several variables on one outcome, the **Response times** of the participants in the tasks. Our potential predictors are:

- **Group** membership, with two levels: aphantasia/phantasia

- **Congruence** conditions, with two levels: congruent and incongruent

- **Color** conditions, with two levels: colored and uncolored.

Sub-levels of the responses ("random effects") can also be accounted for:

- the dependency between response times for **each participant** (also referred to as *subjects*) caused by repeated measures

- the varying effects of **Congruence for each participant**

- the varying effects of **Color for each participant**.

This structure will be modeled using Multilevel Models (also called hierarchical or mixed models), allowing us to add the sub-levels of the hierarchy within participant measures we described above in the model. **RT data will be transformed using a Box-Cox transformation** to bring the distributions closer to normality and improve the quality of the models.

For a subject $i$ with a given *Group*/visual imagery, in the *Congruence* condition $j$ and the *Color* condition $k$, the maximal model of the resulting response time $RT_{ijk}$ including all the predictors can be specified as such:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}}) \cdot Color_{k}
\end{aligned}
$$
Where $\alpha$ is the global intercept, $\alpha_{subject[i]}$ is a random intercept by subject accounting for the inherent randomness tied to participants, and $\beta_{1/2/3}$ are the parameters representing the effect of each predictor. The "$\times$" signs represent all the potential interactions along with fixed effects. Additionally, as one might expect the effect of the Congruence or Color conditions to have important variations between subjects (tied to cognitive processes and strategies), a slope $\beta_{2/3_{subject[i]}}$ by subject can be added to the Congruence or Color parameters to account for this variance.

### Potential models

**RT data will be standardized before modelling**, therefore the standardized coefficients reported will scale on *standard deviations of the RT variable*. This allows to interpret these parameters (regression coefficients) roughly with the same guidelines as usual indices, e.g., Cohen's *d*. Several models will be fitted in turn. We are not going to fit reduced models that exclude some main effects, as we would like to model all of them in any case to produce inferences (as opposed to raw predictions of future data). We'll specify the following: 

1. The minimal model, accounting only for the random effect of subjects, will serve as a comparison baseline: 

$$
\begin{aligned}
RT_{i} = \alpha + \alpha_{subject[i]}
\end{aligned}
$$

2. A model accounting for the fixed effects only:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} + \beta_{2} \cdot Congruence_{j} + \beta_{3} \cdot Color_{k}
\end{aligned}
$$

3. A model with all interactions:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times \beta_{2} \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

4. A model with a slope by subject on Congruence:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

5. A model with a slope by subject on Color:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times \beta_{2} \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}})  \cdot Color_{k}
\end{aligned}
$$

6. The maximal model:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times (\beta_{3} + \beta_{3_{subject[i]}})  \cdot Color_{k}
\end{aligned}
$$

The significant parameters will thereafter be analysed with **marginal contrasts analyses** (robust model-based analogues of "post-hoc" statistical tests) to evaluate the differences between factor levels.

## Fitting the models

```{r model_workflows_and_engines}

# --- workflows ---
workflow_explicit <- workflow() %>% add_recipe(recipe_explicit)
workflow_implicit <- workflow() %>% add_recipe(recipe_implicit)

# --- type of bayesian models to be fitted ---
# mod_0_bayesian <-
#   bayesian (
#     family = skew_normal()
#   ) %>%
#   set_engine("brms") %>% 
#   set_mode("regression")

# --- mixed models to be fitted ---
mod_0_lmer <-
  linear_reg() %>% 
  set_engine("lmer")

# --- bayesian hierarchical models ---
# mod_0_bayes <-
#   linear_reg() %>% 
#   set_engine("stan_glmer")
```

```{r model_specifications}
# --- Specifying all models ---

# --- Explicit task ---

# model 1: intercepts only
mod_explicit_1 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ (1|subjectid)
    )

# model 2: fixed effects
mod_explicit_2 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia + congruence + color + (1|subjectid)
  )

# model 3: interactions
mod_explicit_3 <- 
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1|subjectid)
  )

# model 4: slope by subject on congruence
mod_explicit_4 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1 + congruence|subjectid)
  )

# model 5: slope by subject on color
mod_explicit_5 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + (1 + color|subjectid)
  )

# model 6: maximal model
mod_explicit_6 <-
  workflow_explicit %>% add_model(
    mod_0_lmer,
    formula = rt_explicit ~ aphantasia * congruence * color + 
      (1 + congruence|subjectid) + (1 + color|subjectid)
  )

# --- Implicit task ---

# model 1: intercepts only
mod_implicit_1 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ (1|subjectid)
    )

# model 2: group effect
mod_implicit_2 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia + congruence + color + (1|subjectid)
  )

# model 3: congruence effect
mod_implicit_3 <- 
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1|subjectid)
  )

# model 4: color effect
mod_implicit_4 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1 + congruence|subjectid)
  )

# model 5: slope by subject on color
mod_implicit_5 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + (1 + color|subjectid)
  )

# model 6: maximal model
mod_implicit_6 <-
  workflow_implicit %>% add_model(
    mod_0_lmer,
    formula = rt_implicit ~ aphantasia * congruence * color + 
      (1 + congruence|subjectid) + (1 + color|subjectid)
  )
```

```{r model_bayes_test}
# model 4 bayesian
# mod_explicit_bayes <-
#   workflow_explicit %>% add_model(
#     mod_0_bayes,
#     formula = rt_explicit ~ aphantasia * congruence * color + (1 + congruence|subjectid)
#   )
# 
# fit_bayes <- mod_explicit_bayes %>% fit(data = df_explicit_rt)
# fit_bayes %>%
#   extract_fit_engine %>%
  # broom.mixed::tidy()
  # bayesfactor_parameters
  # model_performance
  # model_parameters
  # describe_posterior(test = c("pd", "bf"))
```

```{r model_fitting}

# --- Fitting all models ---

# --- Explicit task ---
fit_explicit_mods <-
  list(
    mod_explicit_1 = mod_explicit_1, 
    mod_explicit_2 = mod_explicit_2, 
    mod_explicit_3 = mod_explicit_3,
    mod_explicit_4 = mod_explicit_4,
    mod_explicit_5 = mod_explicit_5#,
    # mod_explicit_6 = mod_explicit_6
    ) %>% 
  imap(~ .x %>% fit(data = df_explicit_rt) %>% extract_fit_engine())

# --- Implicit task ---
fit_implicit_mods <-
  list(
    mod_implicit_1 = mod_implicit_1,
    mod_implicit_2 = mod_implicit_2,
    mod_implicit_3 = mod_implicit_3,
    mod_implicit_4 = mod_implicit_4,
    mod_implicit_5 = mod_implicit_5#,
    # mod_implicit_6 = mod_implicit_6
    ) %>%
  imap(~ .x %>% fit(data = df_implicit_rt) %>% extract_fit_engine())

# --- nested list of all the lists of models ---
all_models_fitted <-
  list(
    explicit_mods = fit_explicit_mods,
    implicit_mods = fit_implicit_mods
    )
```

All of these models have been fitted for both tasks and their predictive performances compared using the AIC, AICc (Akaike corrected Information Criterion). The maximal model did not converge, so only the first five models were evaluated. The results of these analyses are displayed in @tbl-model_perfs-1 and @tbl-model_perfs-2.

```{r model_performances}
#| label: tbl-model_perfs
#| tbl-cap: "Performances of the fitted models for both tasks."
#| tbl-subcap: 
#|   - "**Explicit** task models."
#|   - "**Implicit** task models."
#| layout-ncol: 1

# --- Performance of the models ---
all_models_performances <- 
  all_models_fitted %>% 
  imap(
    ~ .x %>% 
      compare_performance(metrics = c("AIC", "AICc")) %>% 
      select(-Model) %>%
      display
    )

all_models_performances[["explicit_mods"]]

all_models_performances[["implicit_mods"]]


```

The fifth model including a slope by subject on Congruence is the most supported for the **explicit task**, while the fourth model including only an intercept by subject has the best performance among models for the **implicit task**. The parameters of these two models will be analysed in the following.

```{r model_parameters}

# --- Parameter estimations ---
all_models_parameters <-
  all_models_fitted %>% 
  imap(
    ~ .x %>%
      imap(
        ~ .x %>% 
          model_parameters(effects = "fixed") %>%
          display
        )
    )
```

## Explicit task results

```{r model_explicit_contrasts}

# model 5 contrasts between aphantasia and congruence levels
c_ex_gcn <-
  all_models_fitted[[1]][[5]] %>% 
  estimate_contrasts(
    contrast = c("aphantasia", "congruence"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  ) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))

# model 5 contrasts between color levels
c_ex_col <-
  all_models_fitted[[1]][[5]] %>% 
  estimate_contrasts(
    contrast = c("color"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  ) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))
```

The parameters of the best model for the explicit task are displayed in @tbl-model_explicit_params_table. The significant parameters in the explicit task are the Color and the interaction between the Group and the Congruence conditions. The **main effect of Color** indicate that participants responded quicker in the colored condition (Marginal contrast (colored - uncolored) = `r c_ex_col[1,3]`, CI = $[`r c_ex_col[1,4]` ; `r c_ex_col[1,5]`]$, SE = `r c_ex_col[1,6]`, $p < .001$), although this effect has no interaction with the other variables. The **interaction between Group and Congruence** reveals that non-aphantasics responded slower in the uncongruent condition, as this contrast is the only significant one in this interaction (Marginal contrast (non-aphantasic congruent - incongruent) = `r c_ex_gcn[1,3]`, CI = $[`r c_ex_gcn[1,4]` ; `r c_ex_gcn[1,5]`]$, SE = `r c_ex_gcn[1,6]`, $p < .001$). These effects are illustrated in @fig-model_explicit_ggarrange. Both estimates are $\lt 0.2$, which denotes a *very small* effect size according to most standardized differences interpretation guidelines (e.g. Cohen, 1988; Gignac & Szodorai, 2016; Lovakov & Agadullina, 2021).

:::{.column-page-inset-right}

```{r model_explicit_params_table}
#| label: tbl-model_explicit_params_table
#| tbl-cap: "Parameters of the optimal model for the explicit task including a random intercept and slope by subject on Congruence."

all_models_parameters[["explicit_mods"]][["mod_explicit_5"]]
```

:::

<!-- Plot settings -->
```{r model_plots_general_settings}

# setting dodge width for all the geoms
dw <- .75
# stat text size
st <- 7
# legend and axis text size
txt <- 32
```

<!-- GCN -->

<!-- Explicit model means and stats for congruence and group -->
```{r model_explicit_gcn_plot_stats}

# model explicit gcn marginal means predictions
preds_mod_4_g_cng <- 
  all_models_fitted[[1]][[5]] %>% 
  estimate_means(
    at = c("aphantasia", "congruence"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000
    )

# stats for the gcn interaction
stats_mod_4_g_cng_full <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = 3.1) %>% 
  filter(aphantasia == "no")

stats_mod_4_g_cng_zoom <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = c(.22, .3))
```

<!-- Explicit model full group and congruence plot -->
```{r model_explicit_gcn_full_plot}
p_mod_ex_gcn_full <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_cng,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_cng,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_cng_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

<!-- Explicit model zoomed group and congruence plot -->
```{r model_explicit_gcn_zoomed_plot}
p_mod_ex_gcn_zoom <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .1,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_cng,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_cng,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_cng_zoom, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(
    # x = "Congruence condition", 
    x = NULL, 
    y = NULL
    ) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.text = element_text(size = txt),
    # axis.title.x = element_text(size = 18, vjust = -.5),
    legend.title = element_text(size = txt)
    )
```

<!-- GCOL -->

<!-- Explicit model means and stats for color and group -->
```{r model_explicit_gcol_plot_stats}
# model explicit gcol marginal means predictions
preds_mod_4_g_col <- 
  all_models_fitted[[1]][[5]] %>% 
  estimate_means(
    at = c("aphantasia", "color"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000
    )

# stats for the gcol interaction
stats_mod_4_g_col_full <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>%
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_explicit ~ color) %>% 
  add_xy_position(x = "color", dodge = dw) %>% 
  mutate(y.position = 3.1)

# stat for the color effect
stats_mod_4_g_col_zoom <- 
  df_explicit_rt %>% 
  standardize(rt_explicit) %>%
  group_by(aphantasia) %>%
  pairwise_wilcox_test(rt_explicit ~ color) %>% 
  add_xy_position(x = "color", dodge = dw) %>% 
  mutate(y.position = .3)
```

<!-- Explicit model full group and color plot -->
```{r model_explicit_gcol_full_plot}
p_mod_ex_gcol_full <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = color,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = .8),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = .8,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_col,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_col,
    aes(
      x = color,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_col_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

<!-- Explicit model zoomed group and color plot -->
```{r model_explicit_gcol_zoomed_plot}
p_mod_ex_gcol_zoom <-
  recipe_explicit %>% 
  prep() %>% 
  bake(df_explicit_rt) %>%
  ggplot(aes(
    x = color,
    y = rt_explicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .1,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = .8),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = .8,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_4_g_col,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_4_g_col,
    aes(
      x = color,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_4_g_col_zoom,
    label = "p.adj.signif",
    tip.length = 0.005,
    size = st
    ) +
    
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(
    # x = "Color condition", 
    x = NULL,
    y = NULL
    ) +
  scale_x_discrete(labels = c("Colored", "Uncolored")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia: ", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.text = element_text(size = txt),
    # axis.title.x = element_text(size = 18, vjust = -.5),
    legend.title = element_text(size = txt)
    )
```

:::{.column-page-inset-right}

<!-- GCN and GCOL -->

```{r model_explicit_ggarrange}
#| label: fig-model_explicit_ggarrange
#| fig-cap: "Visualization of the predictions of the optimal model for the explicit task. The bottom plots are 'zoomed' versions of the top ones on shorter (standard) RT ranges for clearer views of the model. The horizontal colored lines in the 'violins' (colored shapes outlining the distribution of responses) represent quantiles at 25, 50 and 75%. The black dots indicate marginal means predicted by the model whereas the black bars represent the credible intervals of these estimations. **Left:** The *Congruence* plots represent the estimates of the model at each Congruence condition and Group, highlighting the significant interaction between the two variables. **Right**: The *Color* plots represent the estimates at each Color condition and Group, highlighting the main effect of Color and non-significance of the interaction with the Group factor."
#| fig-width: 14
#| fig-height: 11

p_mod_explicit_ggarrange <-
  ggarrange(
    p_mod_ex_gcn_full,
    p_mod_ex_gcol_full,
    p_mod_ex_gcn_zoom,
    p_mod_ex_gcol_zoom,
    labels = c("Congruence", "Color"),
    label.x = c(-.1,.7),
    vjust = -.2,
    font.label = list(size = 38),
    common.legend = TRUE,
    legend = "top",
    ncol = 2,
    nrow = 2,
    heights = c(1.2,1),
    align = "v"
  ) %>%
  annotate_figure(
    left = text_grob("Response time (standardized)", size = txt, rot = 90)
    )

p_mod_explicit_ggarrange
```

:::
  
## Implicit task results

```{r model_implicit_contrasts}

# model 4 contrasts between aphantasia and congruence levels
c_im_gcn <-
  all_models_fitted[[2]][[4]] %>% 
  estimate_contrasts(
    contrast = c("aphantasia", "congruence"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  ) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))

# model 4 contrasts between group levels
# c_im_group <-
  all_models_fitted[[2]][[4]] %>% 
  estimate_contrasts(
    contrast = c("aphantasia"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  ) %>% 
  mutate(across(where(is.numeric), ~ round(.x, digits = 2)))
```

The parameters of the best model for the **implicit task** are displayed in @tbl-model_implicit_params_table. The only significant parameter is the **interaction between Group and Congruence**. This interaction shows that non-aphantasics only responded slower in the incongruent condition, which is the only significant contrast (Marginal contrast (non-aphantasic congruent - incongruent) = `r c_im_gcn[1,3]`, CI = $[`r c_im_gcn[1,4]`;`r c_im_gcn[1,5]`]$, SE = `r c_im_gcn[1,6]`, $p = `r c_im_gcn[1,9]`$). This effect is represented in @fig-model_implicit_ggarrange. This estimate $\lt 0.2$, which denotes a *very small* effect size.

:::{.column-page-inset-right}

```{r model_implicit_params_table}
#| label: tbl-model_implicit_params_table
#| tbl-cap: "Implicit task  - Parameters of the optimal model including a random intercept by subject."

all_models_parameters[["implicit_mods"]][["mod_implicit_4"]]
```

:::

<!-- Implicit model stats -->
```{r model_implicit_gcn_plot_stats}

# model 3 marginal means predictions
# interaction ______________________________
preds_mod_3_fixed_congruence <- 
  all_models_fitted[[2]][[4]] %>% 
  estimate_means(
    at = c("aphantasia", "congruence"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  )

# group ________________________________
preds_mod_3_fixed_group <- 
  all_models_fitted[[2]][[4]] %>% 
  estimate_means(
    at = c("aphantasia"), 
    lmerTest.limit = 9000, 
    pbkrtest.limit = 9000,
    p_adjust = "fdr"
  )

# stats for the gcn interaction _________________________________________

# controls comparisons between conditions on full plot
stats_mod_3_g_cng_full <-
  df_implicit_rt %>% 
  standardize(rt_implicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_implicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = 3.6) %>% 
  filter(aphantasia == "no")

# controls and aph comparisons between conditions on zoomed plot
stats_mod_3_g_cng_zoom <-
  df_implicit_rt %>% 
  standardize(rt_implicit) %>% 
  group_by(aphantasia) %>% 
  pairwise_wilcox_test(rt_implicit ~ congruence) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = c(.28, .2))

stats_mod_3_g_cng_full2 <-
  df_implicit_rt %>% 
  standardize(rt_implicit) %>% 
  group_by(congruence) %>% 
  pairwise_wilcox_test(rt_implicit ~ aphantasia) %>% 
  add_xy_position(x = "congruence", group = "aphantasia", dodge = dw) %>% 
  mutate(y.position = 3.2)

# stats for group _____________________________________________
stats_mod_3_group_full <-
  df_implicit_rt %>% 
  standardize(rt_implicit) %>%
  group_by(congruence) |> 
  pairwise_wilcox_test(rt_implicit ~ aphantasia) %>% 
  add_xy_position(x = "aphantasia", dodge = dw) %>% 
  mutate(y.position = 3.2)

stats_mod_3_group_zoom <-
  df_implicit_rt %>% 
  standardize(rt_implicit) %>%
  pairwise_wilcox_test(rt_implicit ~ aphantasia) %>% 
  add_xy_position(x = "aphantasia", dodge = dw) %>% 
  mutate(y.position = .2)
```

<!-- Implicit model full group and congruence plot -->
```{r model_implicit_gcn_full_plot}
p_mod_im_gcn_full <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_implicit
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    aes(
    fill = aphantasia,
    color = aphantasia),
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_congruence,
    aes(
      y = Mean,
      group = aphantasia,
      color = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_congruence,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .5,
    linewidth = .5,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_3_g_cng_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_3_g_cng_full2, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

<!-- Implicit model zoomed group and congruence plot -->
```{r model_implicit_gcn_zoomed_plot}
p_mod_im_gcn_interaction_zoom <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = congruence,
    y = rt_implicit,
    fill = aphantasia,
    color = aphantasia
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    alpha = .1,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_congruence,
    aes(
      y = Mean,
      group = aphantasia
    ),
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_congruence,
    aes(
      x = congruence,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = aphantasia
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_3_g_cng_zoom, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.3,.3)) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

<!-- Implicit model full group plot -->
```{r model_implicit_group_full_plot}
p_mod_im_group_full <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = aphantasia,
    y = rt_implicit
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    aes(
    fill = aphantasia,
    color = aphantasia
    ),
    alpha = .2,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_group,
    aes(
      y = Mean,
      group = 1
    ),
    color = "black",
    position = position_dodge(width = dw),
    linewidth = 1,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_group,
    aes(
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = 1
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = .75,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linewidth = .05, linetype = 2, alpha = .5) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_3_group_full, 
    label = "p.adj.signif", 
    # tip.length = 0,
    size = st
    ) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
  # scale_x_discrete(labels = c("Congruent", "Incongruent")) +
  scale_y_continuous(breaks = breaks_pretty(10)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_blank(),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

<!-- Implicit model zoomed groupplot -->
```{r model_implicit_group_zoomed_plot}
p_mod_im_group_zoom <-
  recipe_implicit %>% 
  prep() %>% 
  bake(df_implicit_rt) %>%
  ggplot(aes(
    x = aphantasia,
    y = rt_implicit
    )
  ) +
  
  # group violin distributions with quantiles
  geom_violin(
    aes(
    fill = aphantasia,
    color = aphantasia
    ),
    alpha = .1,
    trim = FALSE,
    adjust = .6,
    scale = "count",
    draw_quantiles = c(.25,.5,.75),
    position = position_dodge(width = dw),
    linewidth = .6
    ) +
  
  # participant points
  # geom_beeswarm(
  #   cex = .5,
  #   dodge.width = dw,
  #   alpha = 0.25,
  #   size = .5
  # ) +

  # linear modelling of the differences
  geom_line(
    data = preds_mod_3_fixed_group,
    aes(
      y = Mean,
      group = 1
    ),
    color = "black",
    position = position_dodge(width = dw),
    linewidth = 1.5,
    linetype = 1,
    show.legend = FALSE
  ) +
   
  # predicted means of the model
  geom_pointrange2(
    data = preds_mod_3_fixed_group,
    aes(
      x = aphantasia,
      y = Mean,
      ymin = CI_low,
      ymax = CI_high,
      group = 1
      ),
    color = "black",
    position = position_dodge(width = dw),
    size = 1,
    linewidth = .75,
    show.legend = FALSE
  ) +
  
  # intercept at 0
  geom_hline(yintercept = 0, linetype = 2) +
  
  # stats display
  stat_pvalue_manual(
    stats_mod_3_group_zoom, 
    label = "p.adj.signif", 
    tip.length = 0,
    size = st
    ) +
  
  # zoom on a shorter y range
  coord_cartesian(ylim = c(-.26,.26)) +
  
  # color_palettes and options
  labs(x = NULL, y = NULL) +
scale_x_discrete(labels = c("Aphantasics", "Controls")) +
  scale_y_continuous(breaks = breaks_pretty(8)) +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL) +
  theme(
    panel.grid.major.x = element_blank(),
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(size = txt),
    legend.title = element_text(size = txt),
    legend.text = element_text(size = txt)
    )
```

:::{.column-page-inset-right}

<!-- Implicit model group and congruence ggarrange -->
```{r model_implicit_ggarrange}
#| label: fig-model_implicit_ggarrange
#| fig-cap: "Visualization of the Congruence and Group variables in the optimal model for the implicit task. The bottom plot is a 'zoomed' version of the first one on a shorter (standard) RT range for a clearer view of the model. The horizontal colored lines in the 'violins' represent quantiles at 25, 50 and 75%. The black dots indicate marginal means predicted by the model, whereas the black bars represent the credible intervals of these estimations." 
#| fig-width: 8
#| fig-height: 10

p_mod_implicit_ggarrange <-
  ggarrange(
    p_mod_im_group_full,
    p_mod_im_gcn_full,
    p_mod_im_group_zoom,
    p_mod_im_gcn_interaction_zoom,
    ncol = 2,
    nrow = 2,
    labels = c("Group", "Interaction"),
    label.x = c(-.03,.5),
    vjust = -.2,
    font.label = list(size = 38),
    common.legend = TRUE,
    legend = "top",
    heights = c(1.2,1),
    align = "v"
  ) %>% 
  annotate_figure(
    left = text_grob("Response time (standardized)", size = txt, rot = 90)
    )

p_mod_implicit_ggarrange

# ggexport(p_mod_implicit_ggarrange, filename = "plots/model_explicit.png", width = 1300, height = 800)
```

:::

# Further EDA {#sec-further}

:::{.callout-important}

#### Note for the team

I struggled to understand the variables used for clustering in the actual state of the manuscript. The clustering algorithms need continuous variables, and I only found three of them: *Visual imagery* (VVIQ, besides other redundant questionnaires), and two *Congruence effects* (transformed RT difference variables) respectively in the *Explicit* and *Implicit* tasks. On my end, I will also add a fourth variable, the *Spatial* component of the PCA, in the exploratory process. However, there is a mention in the manuscript of adding *Color* as a clustering variable: how was this possible? The question comes from the fact that Color is a sub-structure of the task, therefore adding Color would mean adding *four* Congruence variables instead of two (the Congruence effect in the Colored or Uncolored condition in the Explicit or Implicit task), along with an additional layer of complexity for the understanding of the meaning and relevance of the variables chosen (an essential aspect of unsupervised analysis).

I would very much like to read the full analysis through JASP inputs and outputs to be able to offer a more informed critique of the process.
:::

## The Congruence effect

As we have seen, besides the lonesome main effect of Color in the explicit task, the only significant effects identified by our best models are those of the interactions $Group \times Congruence$, revealing that non-aphantasics tend to respond slower in the incongruent trials in both tasks whereas aphantasics have a constant performance. We will call this effect the **Congruence effect**, and it appears to vary between the groups. Now it would be very interesting to investigate the Congruence effect as a *continuous variable that varies across individuals*, a phenomenon that is hidden in our *group-wise* (or sample-wise, population-wise) analysis by averaging and searching for the main effect. To analyse the Congruence effect, we would need to calculate it per subject first. Here we have two possible approaches:

- An **empirical approach**, by averaging the RTs per condition across all trials and subtracting Congruent mean RT minus Incongruent.

- A **model-based approach**.

### Empirical approach

The empirical approach would be an obvious manoeuvre if we only had the Congruence condition: this would be a simple transformation of an isolated within-subject predictor. Yet an added layer of complexity exists in our study, which lies in the *Color* condition. Manually averaging the RTs by Congruence level for each participant would involve blending in the two Color levels. This choice has assumptions that we can't verify empirically, such as assuming that the Congruence effect is comparable between Color levels, or assuming that the eventual variation between Color levels would be averaged in the process and that the resulting estimate would reflect the "true" isolated Congruence effect per subject. Betting on the veracity of these assumptions is adventurous at best, and we argue that a model-based approach would be more suited to evaluate the Congruence effect.

### Model-based approach

The model-based approach consists in using the multilevel models we fitted to isolate an estimate representing more accurately the Congruence effect per subject. The first step lies in the identification of what we are searching for in our model. We'll fetch the model described earlier that contains the most information about Congruence, Model 4. It is specified as such:

$$
\begin{aligned}
RT_{ijk} = \alpha + \alpha_{subject[i]} + \beta_{1} \cdot \ Group_{i} \times (\beta_{2} + \beta_{2_{subject[i]}}) \cdot Congruence_{j} \times \beta_{3} \cdot Color_{k}
\end{aligned}
$$

What is the "Congruence effect per subject" here?

The whole $(\beta_{2} + \beta_{2_{subject[i]}})$ is the parameter describing the influence of the Congruence condition on the RT outcome. $\beta_{2}$ is the *sample-wise* parameter, that denotes the common effect of Congruence across the sample. Now $\beta_{2_{subject[i]}}$ is what we called the "*varying slope by subject*", i.e. the ***Congruence effect by subject!*** A model-based estimation would therefore consist in extracting this $\beta_{2_{subject[i]}}$ for each subject $i$, which would become our estimation of the effect - while, this time, accounting for the other parameters of the model, including Color. This $\beta_{2_{subject[i]}}$ will be detached from the global effect, so it will be centered on 0 (the "norm" for the Congruence effect, i.e. null, since the Congruence had no main effect).

The implementation of this calculation in R is shown below, and relies on the (aptly named) package `modelbased`.

<!-- Predicting the congruence effect for each subject -->
```{r search_for_congruence_effect}
#| echo: true
#| code-summary: "Computing the model-based Congruence effect per subject"

# first in the explicit task
congruence_effect_explicit <-
  
  # selecting the random slope on congruence by subject model
  all_models_fitted[["explicit_mods"]][["mod_explicit_4"]] %>% 
  
  # estimating the coefficent and intercept by subject
  estimate_grouplevel %>% 
  
  # keeping only the coefficient and reshaping like the original dataframe
  reshape_grouplevel(indices = "Coefficient") %>% 
  
  # "summary" reduces to one row per subject
  summary %>% 
  
  # keeping only the Coefficient column
  select(-Intercept) %>% 
  
  # renaming for clarity
  rename("congruence_effect_explicit" = congruenceuncongruent) %>% 
  
  # joining with the useful columns from the original data
  left_join(
    # subjectid, aphantasia, visual and spatial imagery
    df_questionnaires %>% select(1, 8:10),
    by = "subjectid"
  )

# completing with the implicit task
df_congruence_effect <-
  left_join(
    congruence_effect_explicit,
    
    # selecting the random slope on congruence by subject model
    all_models_fitted[["implicit_mods"]][["mod_implicit_4"]] %>% 
      # estimating the coefficent and intercept by subject
      estimate_grouplevel %>% 
      # keeping only the coefficient and reshaping like the original dataframe
      reshape_grouplevel(indices = "Coefficient") %>%
      # "summary" reduces to one row per subject
      summary %>% 
      # keeping only the Coefficient column
      select(-Intercept) %>% 
      # renaming for clarity
      rename("congruence_effect_implicit" = congruenceuncongruent),
    
    # adding the effect to each subject
    by = "subjectid"
    ) %>% 
  mutate(across(c(visual_imagery, spatial_imagery), ~as.numeric(.x))) |> 
  # reordering columns
  select(subjectid, aphantasia, 4, 5, 2, 6)
```

## Congruence effect and mental imagery

### Rationale and variables

Why investigate the Congruence effect and mental imagery? The hypothesis is that the Congruence effect reflects Visual imagery, and by extension that **it could be used to screen aphantasia**. The testing of this hypothesis requires to examine all the "within-participants" variables (i.e. cognitive features), that are, in this study, the mental imagery modalities. To that end, we are going to use the two **PCA components** we extracted from the questionnaire scores - the ***Visual imagery*** and ***Spatial imagery*** variables. The assumption behind the choice of using the PCA components instead of the raw questionnaire scores is that these components better reflect the cognitive features of the participants since (1) they are decorrelated and eliminated the covariance shared by the old scores, all the while (2) the Spatial component remains highly correlated with the OSIQ-S as does the Visual component with the VVIQ, OSIQ-O and SUIS, thus justifying our interpretation of these variables as distinct imagery modalities.

### Distributions

In sum, we kept four variables: the Congruence effect in the Explicit and Implicit task, and the Visual and Spatial imageries. The distributions of these four variables are displayed in @fig-congruence_effect_explicit, [-@fig-congruence_effect_implicit], [-@fig-visual_imagery], and [-@fig-spatial_imagery]. Note that the Congruence effect is **not** $(Congruent \ RT - Incongruent \ RT)$, but the $standardized \ parameter \ \beta_{2_{subject[i]}}$ from the model for the Congruence effect per subject: therefore, a ***positive Congruence effect denotes a higher response time in the incongruent condition*** (which is arguably more intuitive).

:::{.panel-tabset .column-page-inset-right}

#### Explicit - Congruence

```{r congruence_effect_explicit_distribution}
#| label: fig-congruence_effect_explicit
#| fig-cap: "Distribution of the **Explicit task Congruence effect** in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = congruence_effect_explicit)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Congruence effect", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Implicit - Congruence

```{r congruence_effect_implicit_distribution}
#| label: fig-congruence_effect_implicit
#| fig-cap: "Distribution of the **Implicit task Congruence effect** in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = congruence_effect_implicit)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Congruence effect", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Visual imagery

```{r visual_imagery_distribution}
#| label: fig-visual_imagery
#| fig-cap: "Distribution of **Visual Imagery** in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = visual_imagery)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Visual Imagery", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

#### Spatial imagery

```{r spatial_imagery_distribution}
#| label: fig-spatial_imagery
#| fig-cap: "Distribution of **Spatial Imagery** in the whole sample (red dotted line), aphantasia and non-aphantasia groups."

(df_congruence_effect %>%
   ggplot(aes(x = spatial_imagery)) +
   
   # geoms
   geom_density(aes(color = aphantasia, fill = aphantasia), color = "black", alpha = .5) +
   geom_density(
     color = "red", 
     # alpha = .3,
     linetype = 3
     ) +
   
   # appearance
   scale_x_continuous(name = "Standardized Spatial Imagery", breaks = breaks_pretty(10)) +
   scale_y_continuous(name = "Density") +
   scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
   theme(
     axis.ticks.y = element_blank(),
     axis.text.y = element_blank()
     )
  ) %>% 
  ggplotly
```

:::

### Correlational analysis

We want to characterize the Congruence effect with imagery variables. Thus, we will study the correlations between the three variables: 1) Congruence effect, 2) visual and 3) spatial imagery, *separately* in the explicit and implicit task. Let's analyse this link with Bayesian partial correlations, displayed in @fig-cor_matrix_congruence_explicit<!-- and [-@fig-cor_matrix_congruence_implicit]-->.

```{r correlation_congruence_effect}
#| code-summary: "Computing correlations"
#| echo: true

# ═══ Bayesian partial correlations ════════════════════════════════════════════

congruence_correlations_explicit <-
  df_congruence_effect %>%
  select(-congruence_effect_implicit) %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )

congruence_correlations_implicit <-
  df_congruence_effect %>%
  select(-congruence_effect_explicit) %>% 
  correlation(
    bayesian = TRUE,
    partial = TRUE,
    partial_bayesian = TRUE,
    bayesian_test = "bf"
    )
```

:::{.column-page-inset-right}

```{r cor_matrix_congruence}
#| label: fig-cor_matrix_congruence_explicit
#| fig-cap: "Correlation matrix with the Congruence effect in both tasks. The stars indicate the amount of evidence in favour of a correlation, as assessed by the $BF_{10}$: No star = Anecdotal evidence, * = Weak evidence, ** = Moderate evidence, *** = Extreme evidence."
#| fig-subcap: 
#|   - "Explicit task."
#|   - "Implicit task."
#| layout-ncol: 2

congruence_correlations_explicit %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "visual_imagery" ~ "Visual imagery",
    .x == "spatial_imagery" ~ "Spatial imagery",
    .x == "congruence_effect_explicit" ~ "Congruence effect",
    TRUE ~ .x))
    ) %>% 
  summary(digits = 2) %>% 
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )

congruence_correlations_implicit %>%
  mutate(across(c(Parameter1, Parameter2),
    ~ case_when(
    .x == "visual_imagery" ~ "Visual imagery",
    .x == "spatial_imagery" ~ "Spatial imagery",
    .x == "congruence_effect_implicit" ~ "Congruence effect",
    TRUE ~ .x))
    ) %>% 
  summary(digits = 2) %>%
  plot(
    text = list(size = 5),
    labs = list(title = "")
    ) +
  scale_fill_viridis(
    option = "D",
    guide = NULL,
    # name = expression(rho),
    alpha = .6,
    direction = 1,
    limits = c(-1,1)
    ) +
  theme_bw() +
  theme(
    panel.grid = element_blank(),
    axis.text = element_text(size = 14)
    )
```

:::

The Congruence effect does not seem to be correlated with visual or spatial imagery when these variables are taken as continua. We'll examine the relations from another point of view, by trying to model the Congruence effect with the two imagery variables.

### Modeling the Congruence effect

We will fit a simple model to investigate the effect:

$$
Congruence \ effect = \beta_{1} \cdot Visual \ imagery \times \beta_{2} \cdot Spatial \ imagery
$$

The parameters of this model for each task are shown in @tbl-model_congruence_effect-1 and @tbl-model_congruence_effect-2. As we could expect from the correlations, the estimates are negligible and far from significance.

```{r model_congruence_effect}
#| label: tbl-model_congruence_effect
#| tbl-cap: "Parameters of the models fitted on the Congruence effect in both tasks."
#| tbl-subcap: 
#|   - "Parameters of the **Explicit task** model."
#|   - "Parameters of the **Implicit task** model."

mod_cng_effect_explicit <-
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_explicit ~ visual_imagery * spatial_imagery,data = df_congruence_effect) %>% 
  extract_fit_engine

mod_cng_effect_implicit <-
  linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_implicit ~ visual_imagery * spatial_imagery,data = df_congruence_effect) %>% 
  extract_fit_engine

mod_cng_effect_explicit %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display

mod_cng_effect_implicit %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display

```

We can visualize these models interactively by plotting the  151 participants in a space defined by the three variables: such projections are presented in @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit]. The predictions of our model are represented as a plane that associates, for each Visual imagery value $x$ and Spatial imagery value $y$, a predicted value $z$ (the "*Predicted congruence effect*"): the model's plane is the collection of those points (it is colored by a gradient to ease the reading of a high/low predicted Congruence effect).

```{r model_predictions_congruence_effect}

preds_explicit_congruence_effect <- 
  estimate_expectation(mod_cng_effect_explicit) %>%
  select(1,3) %>%
  as.data.frame() |> 
  mutate(across(everything(), ~as.numeric(.x))) |> 
  rename("predicted_explicit" = Predicted) %>% 
  left_join(
    df_congruence_effect,
    by = "visual_imagery"
  )

df_preds_congruence_effect <-
  estimate_expectation(mod_cng_effect_implicit) %>%
  as.data.frame() |> 
  select(1,3) %>% 
  rename("predicted_implicit" = Predicted) %>% 
  left_join(
    preds_explicit_congruence_effect,
    by = "visual_imagery"
  ) %>% 
  select(4,5,1,6,7,8,3,2)
```

:::{.panel-tabset}

#### Explicit task model

```{r 3d_congruence_effect_explicit}
#| label: fig-3d_congruence_effect_explicit
#| fig-cap: "3D mapping of the three variables of interest in the **Explicit** task: Congruence effect, Visual, and Spatial imagery. The plane represents the predictions of a model explaining the Congruence effect with the mental imagery variables."

df_preds_congruence_effect %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_explicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    opacity = .5
    ) %>% 
  add_trace(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ predicted_explicit,
    type = "mesh3d",
    intensity = ~ predicted_explicit, 
    colorscale = "Viridis",
    colorbar = list(title = list(text = "Predicted effect")),
    hovertext = "z = Predicted effect"
    ) %>% 
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Implicit task model

```{r 3d_congruence_effect_implicit}
#| label: fig-3d_congruence_effect_implicit
#| fig-cap: "3D mapping of the three variables of interest in the **Implicit** task: Congruence effect, Visual and Spatial imagery. The plane represents the predictions of a model explaining the Congruence effect with the mental imagery variables."

df_preds_congruence_effect %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_implicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    opacity = .5
    ) %>% 
  add_trace(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ predicted_implicit,
    type = "mesh3d",
    intensity = ~ predicted_implicit, 
    colorscale = "Viridis",
    colorbar = list(title = list(text = "Predicted effect")),
    hovertext = "z = Predicted effect"
    ) %>% 
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

:::

Although the effects of the mental imagery variables are statistically anecdotal, this method of visualization shows that the model's planes are not flat, reflecting the interaction dynamics between visual and spatial imagery, and allows us to intuitively understand this interaction. Some interesting observations (you can rotate @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit] to visualize the effects described here):

- In the Explicit task:

    - The Congruence effect diminishes with spatial imagery *for high visual imagers*, but stays even across spatial imagery for *low visual imagers* (mostly aphantasics)
    
    - The Congruence effect augments with visual imagery for *low spatial imagers*, but diminishes with visual imagery for *high spatial imagers* (i.e. completely contrary dynamics depending on the spatial imagery abilities).
    
- In the Implicit task:

    - The Congruence effect diminishes with spatial imagery *for high visual imagers*, but stays even across spatial imagery for *low visual imagers / aphantasics* (i.e. the same dynamic as the Explicit task)
    
    - The Congruence effect augments with visual imagery for *low spatial imagers*, but stays even across visual imagery for *high spatial imagers* (i.e. comparable dynamics with the Explicit task, although not as extreme).

These model planes seem to suggest that the higher Congruence effects in high visual imagers are not an even phenomenon, but that **the significant effect we observed in the [inferential analyses](#sec-inferential) might be driven by a sub-group of non-aphantasics with low spatial imagery**.

### Contradictory findings

Once again, while fairly obvious in visual data exploration, these interaction dynamics between variables have shown to be weak and statistically insignificant. This failure to explain continuous data is reminiscent of another earlier finding, that *Visual imagery is not correlated with the Congruence effect*, although we clearly showed in @sec-inferential that the global effect of Congruence varied with the Group (i.e. visual imagery group) in both tasks. This reflects the high variability in the sample both in Congruence effect and mental imagery (clearly visible in @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit]), variance that is only partly explained by the division in two groups based on visual imagery (although this division already yields - very small - significant interaction effects). Said differently, ***the division by the VVIQ (aphantasics/phantasics) is a sufficient model statistically, yet is not satisfying to explain most of the cognitive differences at play in our tasks.***

As we can see that the initial model of a division in two VVIQ groups somehow managed to explain some aspects of the data, we can hypothesize that the partition of the sample in several groups has an underlying potential to explain the data. Therefore, one approach we could choose to tackle our problem is a redefinition of this division model, i.e. a **redefinition of the groups**. To that end, we propose to conduct an analysis independent of our initial hypotheses and models through the use of **unsupervised clustering**.

# Unsupervised Cluster Analysis

## Feature engineering

### Variables?

We mentioned that our initial groups were created based on the VVIQ, that is, **one variable**. We then showed that variance in our sample could come from several variables, limiting the explanatory power of this grouping. The clustering approach would consist in accounting for **all of these variables in the grouping**. This is still limited if we ponder about the innumerable reasons in the participants' cognition that could explain variance, but it is an analytical equivalent of "*doing our best with what we have*". But what variables do we have? Among the continuous within-subject variables, we isolated four, two mental imagery variables and two Congruence effects (one for each task). We could choose to use all four straight away, but that would mean ignoring an important methodological aspect of clustering procedures: as one could theoretically use an infinite number of variables for clustering, it is paramount that this choice be informed by the fact that all variables must be homogeneous in their physical meaning and equally important for classification. In operational terms, that means reducing covariance between variables as much as possible, often through *dimensionality reduction*, e.g., PCA. 

### Dimensionality reduction, part 2 {#sec-pca2}

The Visual and Spatial imagery variables are already the result of a PCA, thus theoretically two distinct and equally meaningful variables: yet, in our new four-dimensional data, we need to account for their correlations with the two new variables. The question of dimensionality reduction therefore comes from the two Congruence effects: do they represent the same "cognitive features"? How do they associate with each other and mental imagery? To try to answer this question, we will run a second Principal Component Analysis on our four variables.

```{r pca_part_2}
#| echo: true
#| code-summary: "Computing a PCA, part 2"

# --- Principal component analysis on the four continuous variables ---
pca_2 <-
  principal_components(
    df_congruence_effect[,3:6],
    n = "max",
    sort = FALSE
  )
```

:::{.panel-tabset}

#### Loadings

```{r pca_2_loadings}
#| label: fig-loadings2
#| fig-cap: "Loadings of each variable on the three components extracted by a PCA."
#| fig-width: 8

# --- Loadings ---
pca_2 %>% 
  plot + 
  scale_y_discrete(labels = c(
    "Congruence implicit", 
    "Congruence explicit", 
    "Spatial imagery", 
    "Visual imagery"
    )) +
  scale_fill_viridis(alpha = .6) +
  labs(title = NULL) +
  theme(text = element_text(size = 14))
```

#### Eigenvalues

```{r pca_2_eigenvalues}
#| label: tbl-eigenvalues2
#| tbl-cap: "Eigenvalues and variance explained by the three components extracted by a PCA."

# --- Eigenvalues and variance ---
pca_2 %>% 
  summary %>% 
  rename(
    "Component 1" = PC1,
    "Component 2" = PC2,
    "Component 3" = PC3,
  ) %>%
  format(digits = 2) %>%
  display

df_congruence_effect <- 
  df_congruence_effect %>% 
  mutate(across(3:6, as.numeric))

df_clustering <-
  left_join(
    df_congruence_effect[,1:3], 
    rotated_data(pca_2),  
    by = "visual_imagery")

```

#### Component space

```{r pca_2_component_space}
#| label: fig-3d_component_space
#| fig-cap: "3D mapping of the three components extracted by the PCA."

df_clustering %>% 
  plot_ly(
    x = ~ PC1,
    y = ~ PC2,
    z = ~ PC3,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ aphantasia,
    colors = ~ c("#E69F00", "#56B4E9"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Aphantasia")),
    scene = list(
      xaxis = list(title = "Component 1"),
      yaxis = list(title = "Component 2"),
      zaxis = list(title = "Component 3")
      )
    )
```

:::

This PCA is a bit harder to interpret than the first one. To begin with, we can see the quality of the three components: they all have good Eigenvalues, account for homogeneous amounts of variance, and explain in total 80% of the whole variance. Now for their interpretation, we can use @fig-3d_congruence_effect_explicit and [-@fig-3d_congruence_effect_implicit] to ease the understanding: 

- **The first component of the PCA** correlates positively with the implicit Congruence effect and Visual imagery, but negatively with Spatial imagery: this captures the variance reflected by the model plane in @fig-3d_congruence_effect_implicit (the implicit 3D projection), i.e. the diagonal gradient from low Congruence effect / low Visual imagery / high Spatial imagery (darkest purple angle of the plane) to high CE / VI and low SI (brightest yellow angle of the plane). The PCA component 1 denotes the importance of this gradient.

- **The second component of the PCA** correlates negatively with the explicit Congruence effect, but positively with the implicit Congruence effect. This highlights a very interesting dynamic that is hard to notice empirically: if we compare @fig-3d_congruence_effect_explicit (explicit) and @fig-3d_congruence_effect_implicit (implicit), we can notice that the model planes do not have the same surface shape! This affects particularly low Visual imagery (theoretically aphantasics), where a higher Spatial imagery is associated with a higher Congruence effect in the explicit task, but a lower one in the implicit; said differently, the Congruence effect / Spatial imagery relation is inverted at low Visual imagery. This effect is very slight in the model, but still warrants a coherent component according to the (unsupervised) PCA.

- **The third component of the PCA** is basically visual imagery, integrating part of the interaction with spatial imagery in the four-dimensional space.

Now that we have three decorrelated variables explaining meaningful aspects of the data, we can run a cluster analysis on them.

## Cluster analysis

### Method

We will choose a method of clustering based on Gaussian mixture models [^kmeans] (GMM). A Gaussian mixture model is a linear combination of several Gaussian components: it is used when the data cannot be modelled by a simple Gaussian (i.e. a normal distribution). In other words, if the data structure is naturally composed of several clusters, they should be represented by an GMM rather than a simple Gaussian distribution: each distribution then represents a cluster.

[^kmeans]: GMMs are less common in psychology than the widely used K-Means algorithm, but have several advantages over it, mainly: ($i$) K-Means assumes sphericity and an equivalent cluster size, which is rarely the case in reality; ($ii$) GMMs are probabilistic models (*soft clustering*) that are more parsimonious than the geometric approach (*hard clustering*) of K-Means (see for instance Dalmaijer, 2022).

A GMM will therefore identify normal distributions in the data with different means and covariances corresponding to clusters, as well as probabilities for each observation (in this case, the participants) of belonging to a given cluster. As very little information is known about the possible clusters, their means and covariances for each of the variables, the GMM has to follow several steps: (i) Estimating the number of clusters; (ii) The random choice of initial parameters (mean, covariance and weight) for the clusters on each variable; (iii) The use of an "*expectation-maximisation*" algorithm (EM - see Dempster, 1977), an iterative algorithm for determining the parameters of a probabilistic model when it depends on unobservable latent variables. The EM algorithm proceeds in two stages. Step E: calculates the probability of each observation belonging to a cluster; then step M: updates the parameters to maximize these probabilities and approximate the actual distributions of the clusters; finally, returns to step E with the new parameters, and repeats the two steps until the model converges on stable clusters.[^complex]
    
[^complex]: The definitions of GMMs and the EM algorithm are extremely simplified, as they are based on complex probabilistic mathematical models, the explanation of which is not really relevant to this report. More details can be found in the article dedicated to the R package used to implement this method (i.e. `Mclust`, see Scrucca, 2016) or on the internet.

### Determining the number of clusters

There are many methods for determining the ideal number of clusters in a data set. The `n_clusters` function from the package `parameters` on R allows you to use 30 unsupervised methods, and to observe their 'consensus' on this number. The results of this analysis are shown in @fig-number_of_clusters. The choice of 2 clusters is supported by 10 methods out of 30 (Elbow, kl, CCC, Duda, Pseudot2, Beale, Mcclain, Mixture (VEE), Mixture (EEE), Mixture (VII)), but the choices of 3 and 4 clusters are also supported by 6 and 7 methods respectively. We are going to compute the 2 and 4-cluster solutions[^simplicity] and examine the results.

[^simplicity]: We did not show the analysis of the 3-solution solely for the sake of simplicity, as it wasn't very "interpretable" and only highlighted the ambiguity of the procedure, which we do not ignore.

```{r number_of_clusters}
#| label: fig-number_of_clusters
#| fig-cap: "Consensus of 30 unsupervised indices of the optimal cluster number."

(df_clustering[, 7:9] %>% 
  n_clusters %>% 
  summary %>% 
  ggplot(aes(x = n_Clusters, y = n_Methods, fill = n_Methods)) +
  geom_col(color = "black", linewidth = .2) +
  labs(x = "Number of clusters", y = "Number of methods") +
  scale_fill_viridis(guide = NULL, alpha = .6) +
  scale_x_continuous(breaks = seq(1,10, by = 1)) +
  scale_y_continuous(breaks = seq(1,10, by = 1))
  ) %>% 
  ggplotly
```


```{r computing_clusters}
#| echo: true
#| code-summary: "Computing 2 and 4 clusters"

clustering_2 <- 
  Mclust(
    df_clustering[,7:9] |> filter(!is.na(PC1)),
    G = 2
  )

clustering_4 <- 
  Mclust(
    df_clustering[,7:9] |> filter(!is.na(PC1)),
    G = 4
  )

df_clustering <-
  df_clustering %>%
  filter(!is.na(PC1)) |> 
  mutate(
    clusters_2 = clustering_2$classification,
    clusters_4 = clustering_4$classification
  ) %>% 
  mutate(across(starts_with("clusters"), as.factor))
```

### Clustering results

We can visualize the results in the three dimensional spaces we examined earlier : the three PCA components, or the three variables.

#### 2-Clusters solution

:::{.panel-tabset}

#### PCA components

```{r 3d_2_clusters_pca_plotly}
#| label: fig-3d_2_clusters_pca
#| fig-cap: "3D mapping of the two clusters along the three components extracted by the PCA."

df_clustering %>% 
  plot_ly(
    x = ~ PC1,
    y = ~ PC2,
    z = ~ PC3,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_2,
    colors = ~ c("#009E73", "#F0E442"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Component 1"),
      yaxis = list(title = "Component 2"),
      zaxis = list(title = "Component 3")
      )
    )
```

#### Congruence Explicit

```{r 3d_2_clusters_explicit_plotly}
#| label: fig-3d_2_clusters_explicit
#| fig-cap: "3D mapping of the two clusters along the original variables in the explicit task."

df_clustering %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_explicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_2,
    colors = ~ c("#009E73", "#F0E442"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Congruence Implicit

```{r 3d_2_clusters_implicit_plotly}
#| label: fig-3d_2_clusters_implicit
#| fig-cap: "3D mapping of the two clusters along the original variables in the implicit task."

df_clustering %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_implicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_2,
    colors = ~ c("#009E73", "#F0E442"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Cluster repartition

```{r 2_clusters_repartition_plotly}
#| label: fig-2_clusters_repartition
#| fig-cap: "Repartition of the original groups in the two clusters."

(df_clustering %>% 
  mutate(Cluster = clusters_2, .keep = "unused") %>% 
  ggplot(aes(x = Cluster, fill = aphantasia)) +
  geom_bar(
    alpha = .5, 
    position = position_dodge(), 
    color = "black", 
    linewidth = .3
    ) +
  labs(x = "Cluster", y = "Number of participants") +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL)) %>% 
  ggplotly
```

:::

As we can see from the repartition and 3D projections, the 2-clusters solution essentially captures the obvious split in visual imagery among participants, although in an unsupervised way[^mix]. This model neglects the rest of the variance but remains the one with the best fit, as the visual imagery split far outclasses the rest of the sources of variability in terms of effect size (by construction of the experiment, through selected recruitment). However, we could argue - as do the 7 indices recommending the 4-cluster solution - that a lot of this "spatial dispersion" of the observations is not accounted for by the model. Thus, we will investigate the results of the 4-clusters solution.

[^mix]: The mix of some "non-aphantasics" in the low imagery cluster (and vice-versa) comes from the fact that VVIQ was not the sole component of the Visual imagery variable. Admittedly, if we were to use additional questionnaires to "screen" aphantasia, some people in the "ambiguous" zone could move from one group to the other.

#### 4-Clusters solution

:::{.panel-tabset}

#### PCA components

```{r 3d_4_clusters_pca_plotly}
#| label: fig-3d_4_clusters_pca
#| fig-cap: "3D mapping of the four clusters along the three components extracted by the PCA. This 3D plot may look a bit crowded, so feel free to **click on the legend to see specific combinations of clusters**."

df_clustering %>% 
  plot_ly(
    x = ~ PC1,
    y = ~ PC2,
    z = ~ PC3,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_4,
    colors = ~ c("#009E73", "#F0E442", "#D55E00","#CC79A7"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Component 1"),
      yaxis = list(title = "Component 2"),
      zaxis = list(title = "Component 3")
      )
    )
```

#### Congruence Explicit

```{r 3d_4_clusters_explicit_plotly}
#| label: fig-3d_4_clusters_explicit
#| fig-cap: "3D mapping of the four clusters along the original variables in the explicit task. This 3D plot may look a bit crowded, so feel free to **click on the legend to see specific combinations of clusters**."

df_clustering %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_explicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_4,
    colors = ~ c("#009E73", "#F0E442", "#D55E00","#CC79A7"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Congruence Implicit

```{r 3d_4_clusters_implicit_plotly}
#| label: fig-3d_4_clusters_implicit
#| fig-cap: "3D mapping of the four clusters along the original variables in the implicit task. This 3D plot may look a bit crowded, so feel free to **click on the legend to see specific combinations of clusters**."

df_clustering %>% 
  plot_ly(
    x = ~ visual_imagery,
    y = ~ spatial_imagery,
    z = ~ congruence_effect_implicit,
    marker = list(size = 5)
  ) %>% 
  add_markers(
    color = ~ clusters_4,
    colors = ~ c("#009E73", "#F0E442", "#D55E00","#CC79A7"),
    opacity = .7
    ) %>%
  layout(
    legend = list(title = list(text = "Cluster")),
    scene = list(
      xaxis = list(title = "Visual imagery"),
      yaxis = list(title = "Spatial imagery"),
      zaxis = list(title = "Congruence effect")
      )
    )
```

#### Cluster repartition

```{r 4_clusters_repartition_plotly}
#| label: fig-4_clusters_repartition
#| fig-cap: "Repartition of the original groups in the four clusters."

(df_clustering %>% 
  mutate(Cluster = clusters_4, .keep = "unused") %>% 
  ggplot(aes(x = Cluster, fill = aphantasia)) +
  geom_bar(
    alpha = .5, 
    position = position_dodge(), 
    color = "black", 
    linewidth = .3
    ) +
  labs(x = "Cluster", y = "Number of participants") +
  scale_fill_okabeito(name = "Aphantasia", labels = c("Yes", "No")) +
  scale_color_okabeito(guide = NULL)) %>% 
  ggplotly
```

:::

Interestingly, the 4-clusters solution roughly split the two visual imagery groups along the line of high/low spatial imagery. This split does not seem to be associated with a consistent dynamic across both Congruence effects. The clusters have different combinations of visual imagery, spatial imagery, and congruence effects. To have an intuitive visualization of these mean differences, we will plot them in a radar chart in @fig-4_clusters_radar_chart.

```{r 4_clusters_radar_charts}
#| label: fig-4_clusters_radar_chart
#| fig-cap: "Means of the four clusters in the four variables of interest. The axis have been rescaled to the minimum and maximum values of each variable, to 'zoom' on the differences and compensate for the small effect sizes. The 4-cluster plot may look a bit crowded, so feel free to **click on the legend to see specific combinations of clusters**."

(df_clustering %>% 
  select(clusters_4, visual_imagery:congruence_effect_implicit) %>% 
  mutate(Cluster = as.character(clusters_4), .keep = "unused") %>% 
  mutate(across(visual_imagery:congruence_effect_implicit, normalize)) %>% 
  group_by(Cluster) %>% 
  summarise(across(everything(), mean)) %>% 
  ggradar(
    axis.labels = c(
      "Visual imagery",
      "Spatial imagery",
      "Congruence (Explicit)",
      "Congruence (Implicit)"
    ),
    values.radar = c("0", "Median", "Max"),
    grid.label.size = 4,
    label.gridline.min = FALSE,
    legend.title = "Cluster",
    background.circle.transparency = .1,
    group.point.size = 3,
    group.line.width = .5,
    axis.label.size = 4
  ) +
  scale_color_manual(values = c("#009E73", "#F0E442", "#D55E00","#CC79A7"))) %>% 
  ggplotly

```

This (admittedly slightly crowded) plot shows several tendencies:

- The Congruence effect in the implicit task seems badly explained by the cluster model.

- The Congruence effect in the explicit task varies a bit more between the clusters:

    - Cluster 2 has a high Congruence effect: it characterized by a **high visual imagery** and **low spatial imagery**.
    
    - Cluster 1 has an average Congruence effect: it is characterized by **low visual** and **low spatial imagery**.
    
    - Cluster 3 and 4 have below average (i.e., inverted) Congruence effect: the two clusters are on opposite sides of the visual imagery continuum, but have in common a **high spatial imagery**.
    
The means displayed in the radar chart are rescaled on the minimums and maximums of each variable in our data, therefore the effect sizes might be disproportionately exaggerated. Therefore, let's conclude this analysis with a final model to analyse the actual numerical dynamics. 

### Coming full circle: 4-cluster models and end notes

First, we will explain the rationale behind this conclusion. In the beginning, we created two groups based on the VVIQ, then tried to explain RTs in the task with this partition. Then, as we demonstrated a different congruence effect between the groups, we analysed the congruence effect as a continuum varying in light of mental imagery. We did not find statistically significant (linear) trends, but we described tendencies by examining the distributions visually. We tried to explain these trends by fitting a new unsupervised GMM clustering, that eventually found four interesting groups with differences *both* in (explicit) congruence effect and mental imageries, bringing us full circle to a new group partition, this time not based on the sole VVIQ, but that encapsulates both Visual *and* Spatial imagery. Thus, we will go back to our initial objective, i.e. examining the Congruence effect in light of a group partition. We'll fit the simplest model we've seen so far:

$$
Congruence \ effect = \beta_{1} \cdot Cluster
$$
... Which is equivalent to analysing the width and significance of the differences identified by the GMM[^lie]. The parameters of this model are shown in @tbl-model_4_clusters_explicit. The Intercept of the model is centered on 0, denoting a null Congruence effect. Therefore, we can see that the first cluster indeed shows no Congruence effect, whereas the other three have significant Congruence effects in the direction observed in @fig-4_clusters_radar_chart and described above. We'll finally fit the same models for the other variables.

[^lie]: Or, said differently, *is our radar chart a blatant lie?*

```{r final_4_clusters_model}
#| echo: true
#| output: false
#| code-summary: "Modelling the Congruence effect with clusters"

# --- fitting a simple linear model ---
linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_explicit ~ 0 + clusters_4, data = df_clustering)
```

:::{.panel-tabset}

#### Congruence explicit

```{r final_4_clusters_explicit}
#| label: tbl-model_4_clusters_explicit
#| tbl-cap: "Parameters of the model of the explicit Congruence effect with the 4-cluster solution.*Note: 'cluster 4' was just a variable name, read Cluster (1), etc.*"

linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_explicit ~ 0 + clusters_4, data = df_clustering) %>% 
  extract_fit_engine() %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display
```

#### Congruence implicit

```{r final_4_clusters_implicit}
#| label: tbl-model_4_clusters_implicit
#| tbl-cap: "Parameters of the model of the implicit Congruence effect with the 4-cluster solution. *Note: 'cluster 4' was just a variable name, read Cluster (1), etc.*"

linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(congruence_effect_implicit ~ 0 + clusters_4, data = df_clustering) %>% 
  extract_fit_engine() %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display
```

#### Visual imagery

```{r final_4_clusters_visual_im}
#| label: tbl-model_4_clusters_vmi
#| tbl-cap: "Parameters of the model of the Visual imagery with the 4-cluster solution. *Note: 'cluster 4' was just a variable name, read Cluster (1), etc.*"

linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(visual_imagery ~ 0 + clusters_4, data = df_clustering) %>% 
  extract_fit_engine() %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display
```

#### Spatial imagery

```{r final_4_clusters_spatial_im}
#| label: tbl-model_4_clusters_spatial_im
#| tbl-cap: "Parameters of the model of the Spatial imagery with the 4-cluster solution. *Note: 'cluster 4' was just a variable name, read Cluster (1), etc.*"

linear_reg() %>% 
  set_engine("lm") %>% 
  set_mode("regression") %>% 
  fit(spatial_imagery ~ 0 + clusters_4, data = df_clustering) %>% 
  extract_fit_engine() %>% 
  model_parameters %>% 
  rename("*p*" = p) %>% 
  display
```

:::

In the end, I interpret the 4-cluster model as saying that low spatial imagery participants tend to have a higher congruence effect than high spatial imagery ones, although this difference is in *relative* terms, not absolute (as the first - aphantasic - cluster has a null congruence effect with low spatial imagery). It appears, with a certain grouping on the basis of visual imagery, that non-aphantasic participants have a slightly higher congruence effect, but I interpret the last model as showing that cluster 2 - 24 non-aphantasics only, with low spatial imagery - is the sub-group driving this effect, whereas the other non-aphantasics (cluster 3 and 4, 32 non-aphantasics total) show the opposite pattern.

I suspect the Gabor orientation factor to be a potential predictor of this importance of spatial imagery. However that inquiry would mandate further analysis, and I also suspect that this influence is very small, so I'll wrap up this report with this conclusion for the time being.

:::{.callout-note collapse="true"}

#### R Session information

```{r session_information}
report_system()
```
The following packages were used:

```{r packages}
report_packages(
  session = sessionInfo(),
  include_R = FALSE
  )
```

:::

```{r exporting_processed_data}

# df_per_subject <-
#   df_questionnaires %>%
#   left_join(
#     df_clustering %>% select(1,5:11),
#     by = "subjectid"
#     )
# 
# df_per_subject <-
#   df_per_subject %>%
#   left_join(
#     df_explicit %>%
#   group_by(subjectid) %>%
#   # filtering out extreme RTs
#   filter(rt_explicit > 300 & rt_explicit < 1797) %>%
#   summarise(
#     explicit_mean_rt = mean(rt_explicit),
#     explicit_mad_rt = mad(rt_explicit),
#     explicit_min_rt = min(rt_explicit),
#     explicit_max_rt = max(rt_explicit)
#   ),
#     by = "subjectid"
#     )
# 
# df_per_subject <-
#   df_per_subject %>%
#   left_join(
#     df_implicit %>%
#   filter(rt_implicit > 300 & rt_implicit < 1386) %>%
#   group_by(subjectid) %>%
#   summarise(
#     implicit_mean_rt = mean(rt_implicit),
#     implicit_mad_rt = mad(rt_implicit),
#     implicit_min_rt = min(rt_implicit),
#     implicit_max_rt = max(rt_implicit)
#   ),
#     by = "subjectid"
#     )
# 
# df_per_subject <- df_per_subject %>% filter(!is.na(congruence_effect_explicit))
# 
# # saving in the xlsx
# write.xlsx(
#   list("data_per_subject" = df_per_subject),
#   "data/aphantasia_per_subject_data.xlsx",
#   asTable = TRUE,
#   colNames = TRUE,
#   colWidths = "auto",
#   borders = "all",
#   tableStyle = "TableStyleMedium16"
# )
```








































